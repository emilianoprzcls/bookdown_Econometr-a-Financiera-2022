[{"path":"index.html","id":"mínimos-cuadrados-ordinarios","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1 Mínimos Cuadrados Ordinarios","text":"","code":""},{"path":"index.html","id":"el-problema","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1.1 El problema","text":"Recordando que el método de MCO resulta en encontrar la combinación de valores de los estimadores de los parámetros \\(\\hat{\\boldsymbol{\\beta}}\\) que permita minimizar la suma de los residuales (estimadores de los términos de erro \\(\\boldsymbol{\\varepsilon}\\)) al cuadrado dada por:\\[\n    \\sum^{N}_{=1}{e^2_i} = \\sum^{N}_{= 1}{(y_i - \\mathbf{X}'_i \\hat{\\boldsymbol{\\beta}})^2}\n\\]Donde \\(\\hat{\\boldsymbol{\\beta}}\\) denota el vector de estimadores \\(\\hat{\\beta}_1, \\ldots, \\hat{\\beta}_K\\) y dado que \\((e_1, e_2, \\ldots, e_n)'(e_1, e_2, \\ldots, e_n) = {\\mathbf{e'e}}\\), el problema del método de MCO consiste en resolver el problema de óptimización:\\[\\begin{eqnarray*}\nMinimizar_{\\hat{\\boldsymbol \\beta}} S(\\hat{\\boldsymbol \\beta})  =  Minimizar_{\\hat{\\boldsymbol \\beta}} \\mathbf{e'e} \\\\\n    =  Minimizar_{\\hat{\\boldsymbol \\beta}} (\\mathbf{Y}-\\mathbf{X}\\hat{\\boldsymbol \\beta})'(\\mathbf{Y}-\\mathbf{X}\\hat{\\boldsymbol \\beta})\n\\end{eqnarray*}\\]Expandiendo la expresión \\(\\mathbf{e'e}\\) obtenemos:\n\\[\n    \\mathbf{e'e} = \\mathbf{Y'Y} - 2 \\mathbf{Y'X} \\hat{\\boldsymbol \\beta} + \\hat{\\boldsymbol \\beta}' \\mathbf{X'X}\\hat{\\boldsymbol \\beta}\n\\]De esta forma obtenemos que las condiciones necesarias de un mínimo son:\\[\n    \\frac{\\partial S(\\hat{\\boldsymbol \\beta})}{\\partial \\hat{\\boldsymbol \\beta}} = -2{\\mathbf{X'Y}} + 2{\\mathbf{X'X}} \\hat{\\boldsymbol{\\beta}} = \\mathbf{0}\n\\]\nY se pueden despejar las dadas por:Debido que el objetivo es encontrar la matriz \\(\\hat{\\boldsymbol\\beta}\\) despejamos:\\[\\hat{\\boldsymbol \\beta} = (\\mathbf{X'X})^{-1}\\mathbf{X'Y}\n\\]\n\\[\n    \\mathbf{X'X}\\hat{\\boldsymbol \\beta} = \\mathbf{X'Y}\n\\]","code":""},{"path":"index.html","id":"estimación-r","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1.2 Estimación R","text":"Para la estimación utilizaremos el paquete “BatchGetSymbols”. Este paquete nos permitirá descargar información acerca de la bolsa de valores internacional.","code":""},{"path":"index.html","id":"dependencias","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1.2.1 Dependencias","text":"","code":"\n#install.packages(\"pacman\")\n#pacman nos permite cargar varias librerias en una sola línea\nlibrary(pacman)\npacman::p_load(tidyverse,BatchGetSymbols,ggplot2, lubridate)"},{"path":"index.html","id":"descarga-de-los-valores","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1.2.2 Descarga de los valores","text":"","code":"\n#Primero determinamos el lapso de tiempo\npd<-Sys.Date()-365 #primer fecha\npd\n#> [1] \"2021-10-02\"\nld<-Sys.Date() #última fecha\nld\n#> [1] \"2022-10-02\"\n#Intervalos de tiempo\nint<-\"monthly\"\n#Datos a elegir\ndt<-c(\"AMZN\")\n\n#Descargando los valores\n?BatchGetSymbols()\ndata<- BatchGetSymbols(tickers = dt,\n                       first.date = pd,\n                       last.date = ld,\n                       freq.data = int,\n                       do.cache = FALSE,\n                       thresh.bad.data = 0)\n\n#Generando data frame con los valores\ndata_precio<-data$df.tickers\ncolnames(data_precio)\n#>  [1] \"ticker\"              \"ref.date\"           \n#>  [3] \"volume\"              \"price.open\"         \n#>  [5] \"price.high\"          \"price.low\"          \n#>  [7] \"price.close\"         \"price.adjusted\"     \n#>  [9] \"ret.adjusted.prices\" \"ret.closing.prices\""},{"path":"index.html","id":"gráficas","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1.2.3 Gráficas","text":"","code":"\nsp_precio<-ggplot(data_precio, aes(x=ref.date, y=price.open))+geom_point(size =2, colour = \"black\")+labs(x=\"Fecha\", y=\"Precio de apertura (USD)\", title=\"Precio de apertura de AMZN en el ultimo año\")+ theme_light()+ geom_smooth(method = lm, se = TRUE)\nsp_precio\n\nsp_volumen<-ggplot(data_precio, aes(x=ref.date, y=volume))+geom_point(size =2, colour = \"black\")+labs(x=\"Fecha\", y=\"Volumen\", title=\"Volumenes de AMZN en el ultimo año\")+ theme_light()+ geom_smooth(method = lm, se = TRUE)\nsp_volumen"},{"path":"index.html","id":"regresión-lineal-que-optiene-los-coeficientes-hatboldsymbol-beta","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1.2.4 Regresión lineal que optiene los coeficientes \\(\\hat{\\boldsymbol \\beta}\\)","text":"","code":"\n#datos estadísticos\nsummary(data_precio[c(\"price.open\",\"volume\")])\n#>    price.open        volume         \n#>  Min.   :106.3   Min.   :1.170e+09  \n#>  1st Qu.:125.1   1st Qu.:1.270e+09  \n#>  Median :151.4   Median :1.490e+09  \n#>  Mean   :146.3   Mean   :1.506e+09  \n#>  3rd Qu.:165.0   3rd Qu.:1.644e+09  \n#>  Max.   :177.2   Max.   :2.258e+09\n#análisis de regresión lineal lm() y=precio,x=fecha\nreg_tiempo_precio<-lm(price.open~ref.date, data=data_precio) \n#¡Siempre se pone dentro de lm() la variable dependiente primero y luego la independiete!\nsummary(reg_tiempo_precio)\n#> \n#> Call:\n#> lm(formula = price.open ~ ref.date, data = data_precio)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -21.6054 -10.9524   0.6833   9.7802  20.3378 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 3481.50781  721.70171   4.824 0.000698 ***\n#> ref.date      -0.17490    0.03785  -4.621 0.000949 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 13.67 on 10 degrees of freedom\n#> Multiple R-squared:  0.6811, Adjusted R-squared:  0.6492 \n#> F-statistic: 21.36 on 1 and 10 DF,  p-value: 0.0009486\n\n#análisis de regresión lineal lm() y=volumen,x=fecha\nreg_tiempo_volumen<-lm(volume~ref.date, data=data_precio)\nsummary(reg_tiempo_volumen)\n#> \n#> Call:\n#> lm(formula = volume ~ ref.date, data = data_precio)\n#> \n#> Residuals:\n#>        Min         1Q     Median         3Q        Max \n#> -333764299 -237797962  -16397739  137378274  753026280 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)  1.765e+09  1.707e+10   0.103    0.920\n#> ref.date    -1.359e+04  8.950e+05  -0.015    0.988\n#> \n#> Residual standard error: 323300000 on 10 degrees of freedom\n#> Multiple R-squared:  2.305e-05,  Adjusted R-squared:  -0.09997 \n#> F-statistic: 0.0002305 on 1 and 10 DF,  p-value: 0.9882"},{"path":"index.html","id":"ejercicio","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1.3 Ejercicio","text":"El objetivo de este ejrcicio es simplemente que indiquen y modifiquen los errores en el código. Así pues, deberán descomentar -quitar las #antes del código- para empezar el ejercicio.","code":""},{"path":"index.html","id":"section","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1.3.1 1","text":"El objetivo de este código es explicar la variable “volume” con la variable “price.high”.","code":"\n#reg_tiempo_ej1<-lm(price.high~volume, data=data_precio)\n#sumary(reg_tiempo_ej1)"},{"path":"index.html","id":"section-1","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1.3.2 2","text":"El objetivo de este código es explicar la variable “volume” con la variable “price.low”.","code":"\n#reg_tiempo_ej2<-lm(price.low~volume, data=data_precio)\n#summary(reg_tiempo_ej1)"},{"path":"index.html","id":"opcional","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1.3.3 3 (opcional)","text":"El objetivo de este ejercicio es descargar los valores del stock de Tesla BMV: TSLA en los últimos dos años.","code":"\n#dt_ej3<-(\"TSLA\")\n#pdej<-Sys.Date()-(365*3) #primer fecha\n#pdej\n#Descargando los valores\n#dataej3<- BatchgetSymbols(tickers = dt_ej3,\n                       #first.date = pdej,\n                       #last.date = ld,\n                       #freq.data = int,\n                       #do.cache = FALSE,\n                       #thresh.bad.data = 0)\n\n#Generando data frame con los valores\n#data_precio_ej2<-dataej3$df.tickers\n#1colnames(data_precio_ej2)"},{"path":"máxima-verosimilitud.html","id":"máxima-verosimilitud","chapter":"2 Máxima Verosimilitud","heading":"2 Máxima Verosimilitud","text":"","code":""},{"path":"máxima-verosimilitud.html","id":"el-problema-1","chapter":"2 Máxima Verosimilitud","heading":"2.1 El problema","text":"Recordemos que dado \\(f(y_i | \\mathbf{x}_i)\\) la función de densidad condicional de \\(y_i\\) dado \\(\\mathbf{x}_i\\). Sea \\(\\boldsymbol{\\theta}\\) un conjunto de parámetros de la función. Entonces la función de densidad conjunta de variables aleatorias independientes \\(\\{ y_i : y_i \\\\mathbb{R} \\}\\) dados los valores \\(\\{ \\mathbf{x}_i : \\mathbf{x}_i \\\\mathbb{R}^K \\}\\) estará dada por:\\[\\begin{equation}\n    \\Pi_{= 1}^{n} f(y_i | \\mathbf{x}_i; \\boldsymbol{\\theta}) = f(y_1, y_2, \\ldots, y_n | \\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n; \\boldsymbol{\\theta}) = L(\\boldsymbol{\\theta})\n    \\tag{2.1}\n\\end{equation}\\]la ecuación (2.1) se le conoce como ecuación de verosimilitud. El problema de máxima verosimilitud entonces será:\n\\[\\begin{equation}\n    \\max_{\\boldsymbol{\\theta} \\\\boldsymbol{\\Theta}} \\Pi_{= 1}^{n} f(y_i | \\mathbf{x}_i; \\boldsymbol{\\theta}) = \\max_{\\boldsymbol{\\theta} \\\\boldsymbol{\\Theta}} L(\\boldsymbol{\\theta})\n        \\tag{2.2}\n\\end{equation}\\]Dado que el logaritmo natural es una transformación monotona, podemos decir que el problema de la ecuación (2.2) es equivalente :\\[\\begin{equation}\n     \\max_{\\boldsymbol{\\theta} \\\\boldsymbol{\\Theta}} ln L(\\boldsymbol{\\theta}) = \\max_{\\boldsymbol{\\theta} \\\\boldsymbol{\\Theta}} ln \\Pi_{= 1}^{n} f(y_i | \\mathbf{x}_i; \\boldsymbol{\\theta}) = \\max_{\\boldsymbol{\\theta} \\\\boldsymbol{\\Theta}} \\sum_{= 1}^{n} ln f(y_i | \\mathbf{x}_i; \\boldsymbol{\\theta})\n            \\tag{2.3}\n\\end{equation}\\]Para solucionnar el problema se tiene que determinar las condicones de primer y segundo orden, las cuales serán:\n\\[\\begin{equation}\n    \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} ln L(\\boldsymbol{\\theta}) = \\nabla ln L(\\boldsymbol{\\theta})\n          \\tag{2.4}\n\\end{equation}\\]\\[\\begin{equation}\n    \\frac{\\partial^2}{\\partial^2 \\boldsymbol{\\theta}} ln L(\\boldsymbol{\\theta}) = \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} ln L(\\boldsymbol{\\theta}) \\cdot  \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} ln L(\\boldsymbol{\\theta}') = H(\\boldsymbol{\\theta})\n             \\tag{2.5}\n\\end{equation}\\]La solución estará dada por aquel valor de \\(\\hat{\\boldsymbol{\\theta}}\\) que hace:\n\\[\\begin{equation*}\n    \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} ln L(\\hat{\\boldsymbol{\\theta}}) = 0\n\\end{equation*}\\]su vez, la varianza será aquella que resulta de:\n\\[\\begin{equation*}\n    Var[\\hat{\\boldsymbol{\\theta}} | \\mathbf{X}] = \\left( - \\mathbb{E}_{\\hat{\\boldsymbol{\\theta}}}[H(\\boldsymbol{\\theta})] \\right)^{-1}\n\\end{equation*}\\]","code":""},{"path":"máxima-verosimilitud.html","id":"estimación-y-simunlación","chapter":"2 Máxima Verosimilitud","heading":"2.2 Estimación y simunlación","text":"","code":""},{"path":"máxima-verosimilitud.html","id":"lanzar-una-moneda","chapter":"2 Máxima Verosimilitud","heading":"2.2.1 Lanzar una moneda","text":"Si bien el ejercicio anterior es un tanto repetitivo debido que sabemos que hay un 50% de que caiga una moneda de un lado o otro. Esto ejemplifica la manera en la que se utiliza el metodo de maximización de máxima verosimilitud.","code":"\nset.seed(1234)#esto sirve para siempre generar los mismos numeros aleatorios\n#rbinom(numero observaciones,numero de ensayos,probabilidad de exito en cada ensayo)\ncara<-rbinom(1,100,0.5)\ncara#esto nos dice de los 100 ensayos cuantos fueron cara\n#> [1] 47\nsol<-100-cara\nsol\n#> [1] 53\n\n\n#Ahora definiremos la función que encontrará la función de verosimilutud para determinado valor p\n#\nverosimilitud <- function(p){\n  dbinom(cara, 100, p)\n}\n\n#si suponemos que la probabilidad sesgada de que caiga cara es 40%\nprob_sesgada<-0.4\n#es posible calcular la función de que salga cara\nverosimilitud(prob_sesgada)\n#> [1] 0.02919091\n#ahora es posible generar una función de verimilitud negativa \n#para maximizar el valor de la verosimilitud\nneg_verosimilitud <- function(p){\n  dbinom(cara, 100, p)*-1\n}\nneg_verosimilitud(prob_sesgada)\n#> [1] -0.02919091\n# unamos la función nlm() para maximizar esta función no linear\n#?nlm()\nnlm(neg_verosimilitud,0.5,stepmax=0.5)#se pone un parametro porque sabemos que hay un 0.5 de probabilidad de que caiga cara\n#> $minimum\n#> [1] -0.07973193\n#> \n#> $estimate\n#> [1] 0.47\n#> \n#> $gradient\n#> [1] 1.589701e-10\n#> \n#> $code\n#> [1] 1\n#> \n#> $iterations\n#> [1] 4"},{"path":"método-generalizado-de-momentos-mgm.html","id":"método-generalizado-de-momentos-mgm","chapter":"3 Método Generalizado de Momentos (MGM)","heading":"3 Método Generalizado de Momentos (MGM)","text":"","code":""},{"path":"método-generalizado-de-momentos-mgm.html","id":"el-problema-2","chapter":"3 Método Generalizado de Momentos (MGM)","heading":"3.1 El problema","text":"Retomemos el modelo de regresión lineal tal que:\\[\\begin{equation}\ny_i=X_i\\beta+u_i\n    \\label{Eq_reglin}\n\\end{equation}\\]Tomando en cuenta los principios de ortogonalidad (\\(E(Z_iu_i)=0\\)) y (\\(rankE(Z_i^{'}X_i)=0\\)) sabemos que \\(\\beta\\) es el único vector de \\(N\\times1\\) que resuelve las condiciones de momento de determinada población. En otras palabras, \\(E[z_i^{'}(y_i-x_i\\beta)]=0\\) es una solución y \\(E[z_i^{'}(y_i-x_i\\beta)]\\neq0\\) es una solución. Debido que la media muestral son estimadores consistentes de momentos de una población, se puede:\\[\\begin{equation}\nN^{-1}\\sum_{=1}^{N}z_i^{'}(y_i-x_i\\beta)=0\n\\tag{3.1}\n\\end{equation}\\]Asumiendo que la ecuación (3.1) tiene L ecuaciones lineales y K coeficientes \\(\\beta\\) desconocidos y \\(K=L\\), entonces la matriz \\(\\sum_{=1}^{N}z_i^{'}x_i\\) debe ser singular para encontrar los coeficientes de la siguiente manera.\\[\\begin{equation}\n\\hat{\\beta}=N^{-1}\\left[\\sum_{=1}^{N}z_i^{'}x_i\\right]^{-1}\\left[\\sum_{=1}^{N}z_i^{'}y_i\\right]\n\\tag{3.2}\n\\end{equation}\\]Para simplificar (3.2) se puede nombrar Z juntando \\(z_i\\) N veces para crear una matriz de tamaño \\(NG\\times L\\). Lo mismo hacemos con X juntando \\(x_i\\) para obtener una de \\(NG\\times K\\) y Y obteniendo una \\(NG\\times 1\\). Obteniendo:\\[\\begin{equation}\n\\hat{\\beta}=[Z^{'}X]^{-1}[Z^{'}Y]\n\\end{equation}\\]Es importante tomar en cuenta cuando el caso en el que hay más ecuaciones lineales que coeficientes \\(\\beta\\); es decir, \\(L\\geq K\\). En estos casos es muy probrable que haya solución, por lo que mejor que se puede estimar es pones la ecuación (3.1), tan pequeña como sea posible. Por lo mismo el paso que nos lleva la ecuación (3.2), debe eliminarse \\(N^{-1}\\). El objetivo:\\[\\begin{equation}\n\\min_{\\beta} \\left[\\sum_{=1}^{N}z_i^{'}x_i\\beta\\right]^{-1}\\left[\\sum_{=1}^{N}z_i^{'}y_i\\beta\\right]\n\\tag{3.3}\n\\end{equation}\\]Así pues nombramos W como una matriz simétrica de \\(W\\times W\\) donde se genera la variable \\(b\\) que debemos minimizar que sustituye \\(\\beta\\) creando una función cuadrática en la ecuación (3.2).\n\\[\\begin{equation}\n\\min_{b}\\left[\\sum_{=1}^{N}z_i^{'}x_ib\\right]^{-1}\\left[\\sum_{=1}^{N}z_i^{'}y_ib\\right]\n\\tag{3.4}\n\\end{equation}\\]\\[\\begin{equation}\n\\therefore\\hat{\\beta}=[X^{'}Z\\hat{W}Z^{'}X]^{-1}[X^{'}Z\\hat{W}Z^{'}Y]\n\\end{equation}\\]Sin embargo, \\(X^{'}Z\\hat{W}Z^{'}X\\) debe ser singular para que haya una solución. Para esto se asume que \\(\\hat{W}\\) tiene un limite de probabilidad singular. Esto se describe como \\(\\hat{W}\\xrightarrow[]{p}W\\) y \\(N\\xrightarrow[]{}W\\infty\\) donde \\(W\\) es aleatorio, es una matriz positiva definida simétrica de \\(L\\times L\\).","code":""},{"path":"capital-asset-pricing-model-capm.html","id":"capital-asset-pricing-model-capm","chapter":"4 CAPITAL ASSET PRICING MODEL (CAPM)","heading":"4 CAPITAL ASSET PRICING MODEL (CAPM)","text":"","code":""},{"path":"capital-asset-pricing-model-capm.html","id":"el-problema-3","chapter":"4 CAPITAL ASSET PRICING MODEL (CAPM)","heading":"4.1 El problema","text":"Una vez que hemos establecido la manera en la que se pueden estimar algunos valores –como las regresiones lineales y el método de máxima verosimilitud–, además de la naturaleza de los retornos de algunos activos en el capítulo 4, es posible comenzar hablar de maneras en la que se pueden estimar los valores futuros de los rendimientos de activos y –de esta manera– poder tomar mejores decisiones de inversiones. Por ello, hablaremos del modelo de Capital Asset Pricing Model. El modelo es muy sencillo y pretende estimar su rentabilidad esperada en función del riesgo sistemático. Por lo mismo, en este modelo se utilizan los valores de los precios de los activos lo largo del tiempo y utiliza la intuición con la que derivamos la ecuación lineal con los Mínimos cuadrados ordinarios (MCO).En la ecuación (4.1)\\(R_{jt}\\) es el retorno del portafolio \\(j\\) en el tiempo \\(t\\)\\(R_{jt}\\) es el retorno del portafolio \\(j\\) en el tiempo \\(t\\)\\(R_{ft}\\) es el retorno de un bono sin riesgo gubernamental en un año. Parecido los CETES.\\(R_{ft}\\) es el retorno de un bono sin riesgo gubernamental en un año. Parecido los CETES.\\(R_{mt}\\) es el retorno en un portafolio de mercado.\\(R_{mt}\\) es el retorno en un portafolio de mercado.\\(u_{jt}\\) es el retorno en un portafolio de mercado.\\(u_{jt}\\) es el retorno en un portafolio de mercado.\\(\\alpha_{j},\\beta_j\\) son los coeficientes que queremos obtener.\\(\\alpha_{j},\\beta_j\\) son los coeficientes que queremos obtener.De esta manera, \\(\\alpha_j\\) es el coeficiente que más nos interesa debido que queremos ver si el activo supera o el index del mercado con base en el activo fijo.Si \\(\\alpha_j\\) es positivo entonces sabemos que el retorno tiene buenos rendimiendtos y uno negativo significa que . Por tanto \\(H_0:\\alpha_j=0\\)","code":""},{"path":"capital-asset-pricing-model-capm.html","id":"estimación-r-1","chapter":"4 CAPITAL ASSET PRICING MODEL (CAPM)","heading":"4.2 Estimación R","text":"Para la estimación utilizaremos el paquete “BatchGetSymbols”. Este paquete nos permitirá descargar información acerca de la bolsa de valores internacional.","code":""},{"path":"capital-asset-pricing-model-capm.html","id":"estimación","chapter":"4 CAPITAL ASSET PRICING MODEL (CAPM)","heading":"4.3 ESTIMACIÓN","text":"","code":""},{"path":"capital-asset-pricing-model-capm.html","id":"dependencias-1","chapter":"4 CAPITAL ASSET PRICING MODEL (CAPM)","heading":"4.3.1 Dependencias","text":"","code":"\n#install.packages(\"pacman\")\n#pacman nos permite cargar varias librerias en una sola línea\nlibrary(pacman)\npacman::p_load(tidyverse,BatchGetSymbols,ggplot2,lubridate,readxl,tidyquant)"},{"path":"capital-asset-pricing-model-capm.html","id":"descarga-de-los-valores-1","chapter":"4 CAPITAL ASSET PRICING MODEL (CAPM)","heading":"4.3.2 Descarga de los valores","text":"\nFigure 4.1: Relación de excesos de retornos entre AMZN y SP500\n\nFigure 4.2: Relación de excesos de retornos entre TSLA y SP500\nDe esta manera sabemos que el rendimiento de TSLA es mayor debido que el coeficiente \\(\\alpha=-2.9534\\), lo cual indica peores rendimientos al resto del SP500.","code":"\n#Primero determinamos el lapso de tiempo\npd<-as.Date(\"2021/09/18\") #primer fecha\npd\n#> [1] \"2021-09-18\"\nld<-as.Date(\"2022/09/18\") #última fecha\nld\n#> [1] \"2022-09-18\"\n#Intervalos de tiempo\nint<-\"monthly\"\n#Datos a elegir\ndt<-c(\"AMZN\")\ndt2<-c(\"TSLA\")\n#Descargando los valores\n?BatchGetSymbols()\ndata1<- BatchGetSymbols(tickers = dt,\n                       first.date = pd,\n                       last.date = ld,\n                       freq.data = int,\n                       do.cache = FALSE,\n                       thresh.bad.data = 0)\ndata2<- BatchGetSymbols(tickers = dt2,\n                       first.date = pd,\n                       last.date = ld,\n                       freq.data = int,\n                       do.cache = FALSE,\n                       thresh.bad.data = 0)\n\n#Generando data frame con los valores\ndata_precio_amzn<-data1$df.tickers\ncolnames(data_precio_amzn)\n#>  [1] \"ticker\"              \"ref.date\"           \n#>  [3] \"volume\"              \"price.open\"         \n#>  [5] \"price.high\"          \"price.low\"          \n#>  [7] \"price.close\"         \"price.adjusted\"     \n#>  [9] \"ret.adjusted.prices\" \"ret.closing.prices\"\ndata_precio_tls<-data2$df.tickers\ncolnames(data_precio_tls)\n#>  [1] \"ticker\"              \"ref.date\"           \n#>  [3] \"volume\"              \"price.open\"         \n#>  [5] \"price.high\"          \"price.low\"          \n#>  [7] \"price.close\"         \"price.adjusted\"     \n#>  [9] \"ret.adjusted.prices\" \"ret.closing.prices\"\n#necesitamos convertir la serie de tiempo de precios en retornos continuos compuestos de los precios de apertura\ndata_precio_amzn$ccrAMZN<-c(NA ,100*diff(log(data_precio_amzn$price.open)))#agregamos un valor NA al principio\ndata_precio_amzn$ccrAMZN#estos son los retornos\n#>  [1]          NA  -3.2011678   2.1889913   5.3061639\n#>  [5]  -5.6279333 -11.0646538   1.8052718   7.2089622\n#>  [9] -29.3475086  -0.1185366 -13.9945965  23.8807273\n#> [13]  -6.8696583\n\ndata_precio_tls$ccrTSLA<-c(NA ,100*diff(log(data_precio_tls$price.open)))#agregamos un valor NA al principio\ndata_precio_tls$ccrTSLA\n#>  [1]         NA   5.796888  38.591933   1.361865  -1.121972\n#>  [6] -20.478772  -7.264574  21.765521 -22.795320 -13.089771\n#> [11] -10.336735  28.307900 -10.009692\n#formateando por año y mes\ndata_precio_tls$ref.date=format(as.Date(data_precio_tls$ref.date), \"%m/%Y\")\ndata_precio_amzn$ref.date=format(as.Date(data_precio_amzn$ref.date), \"%m/%Y\")\n#Compararemos con los CETES\nCETES_sep2021_2022<-read_excel(\"BD/CETES-sep2021-2022.xlsx\", skip=17)\nhead(CETES_sep2021_2022)\n#> # A tibble: 6 × 2\n#>   Fecha               SF43936\n#>   <dttm>                <dbl>\n#> 1 2021-09-15 00:00:00    4.6 \n#> 2 2021-09-23 00:00:00    4.58\n#> 3 2021-09-30 00:00:00    4.69\n#> 4 2021-10-07 00:00:00    4.81\n#> 5 2021-10-14 00:00:00    4.79\n#> 6 2021-10-21 00:00:00    4.83\n#indice sp500\nSP500 <- read_csv(\"BD/Download Data - INDEX_US_S&P US_SPX.csv\")\nSP500$ccrSP500<-c(NA ,100*diff(log(SP500$Open)))\nnames(SP500)[1]<-paste('ref.date')\n#formateando por año y mes\n\n#cetes\ncete_1_año<-10.10#esto es el rendimiento a un año de un cete gubernamental seguro\n\n#Juntamos el df\nCAPM_2<-merge(data_precio_amzn, data_precio_tls, by = c('ref.date'))\nCAPM_4<-merge(SP500, CAPM_2, by = c('ref.date'))\nCAPM<-data.frame(CAPM_4)\n\n#exceso de retorno\nCAPM$excess_ret_AMZN<-CAPM$ccrAMZN-cete_1_año\nCAPM$excess_ret_SP500<-CAPM$ccrSP500-cete_1_año\nCAPM$excess_ret_TSLA<-CAPM$ccrTSLA-cete_1_año\n#relacion entre los excesos de demanda\nggplot(CAPM, aes(x=excess_ret_AMZN, y=excess_ret_SP500))+geom_point()+labs(title=\"Relación de excesos de retornos entre TSLA y AMZN\",y=\"Exceso de demanda de SP500\", x=\"Exceso de demanda de AMZN\")+theme_light()\n#> Warning: Removed 2 rows containing missing values\n#> (geom_point).\n#relacion entre los excesos de demanda\nggplot(CAPM, aes(x=excess_ret_TSLA, y=excess_ret_SP500))+geom_point()+labs(title=\"Relación de excesos de retornos entre TSLA y AMZN\",y=\"Exceso de demanda de SP500\", x=\"Exceso de demanda de TSLA\")+theme_light()\n#> Warning: Removed 2 rows containing missing values\n#> (geom_point).\n#veamos la regresion lineal\nCAPM_lr<-lm(excess_ret_TSLA~excess_ret_SP500,data = CAPM)\nsummary(CAPM_lr)\n#> \n#> Call:\n#> lm(formula = excess_ret_TSLA ~ excess_ret_SP500, data = CAPM)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -23.980 -13.391  -5.548  11.572  37.067 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)       -3.2334    11.8097  -0.274     0.79\n#> excess_ret_SP500   0.5379     1.0789   0.499     0.63\n#> \n#> Residual standard error: 20.88 on 9 degrees of freedom\n#>   (2 observations deleted due to missingness)\n#> Multiple R-squared:  0.02687,    Adjusted R-squared:  -0.08125 \n#> F-statistic: 0.2486 on 1 and 9 DF,  p-value: 0.6301\nalpha1<-coefficients(CAPM_lr)[1]\nalpha1<0\n#> (Intercept) \n#>        TRUE"},{"path":"capital-asset-pricing-model-capm.html","id":"ejercicio-compara-con-tsla-con-el-apple","chapter":"4 CAPITAL ASSET PRICING MODEL (CAPM)","heading":"4.4 Ejercicio Compara con TSLA con el APPLE","text":"","code":"\ndt3<-\"AAPL\"\ndata3<-BatchGetSymbols(tickers = dt3,\n                       first.date = pd,\n                       last.date = ld,\n                       freq.data = int,\n                       do.cache = FALSE,\n                       thresh.bad.data = 0)\n#> Warning: `BatchGetSymbols()` was deprecated in BatchGetSymbols 2.6.4.\n#> Please use `yfR::yf_get()` instead.\n#> 2022-05-01: Package BatchGetSymbols will soon be replaced by yfR. \n#> More details about the change is available at github <<www.github.com/msperlin/yfR>\n#> You can install yfR by executing:\n#> \n#> remotes::install_github('msperlin/yfR')\n#> \n#> Running BatchGetSymbols for:\n#>    tickers =AAPL\n#>    Downloading data for benchmark ticker\n#> ^GSPC | yahoo (1|1)\n#> AAPL | yahoo (1|1) - Got 100% of valid prices | Nice!\ndata_precio_AAPL<-data3$df.tickers\ncolnames(data_precio_AAPL)\n#>  [1] \"ticker\"              \"ref.date\"           \n#>  [3] \"volume\"              \"price.open\"         \n#>  [5] \"price.high\"          \"price.low\"          \n#>  [7] \"price.close\"         \"price.adjusted\"     \n#>  [9] \"ret.adjusted.prices\" \"ret.closing.prices\"\ndata_precio_AAPL$ccrAAPL<-c(NA ,100*diff(log(data_precio_AAPL$price.open)))#agregamos un valor NA al principio\ndata_precio_AAPL$ccrAAPL\n#>  [1]         NA  -1.330092   4.875668  11.698469   5.996413\n#>  [6]  -2.171531  -5.498712   5.510207 -10.483068  -4.442864\n#> [11]  -9.701946  16.851754  -2.751627\ndata_precio_AAPL$ref.date=format(as.Date(data_precio_AAPL$ref.date), \"%m/%Y\")\nCAPM_3<-merge(data_precio_AAPL, CAPM, by = c('ref.date'))\nCAPM_3$excess_ret_AAPL<-CAPM_3$ccrAAPL-cete_1_año\n#veamos la regresion lineal\nCAPM3_lr<-lm(excess_ret_AAPL~excess_ret_SP500,data = CAPM_3)\nsummary(CAPM3_lr)\n#> \n#> Call:\n#> lm(formula = excess_ret_AAPL ~ excess_ret_SP500, data = CAPM_3)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -10.9038  -5.4365   0.4641   3.4629  14.1798 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)       -4.7542     4.9105  -0.968    0.358\n#> excess_ret_SP500   0.4663     0.4486   1.039    0.326\n#> \n#> Residual standard error: 8.682 on 9 degrees of freedom\n#>   (2 observations deleted due to missingness)\n#> Multiple R-squared:  0.1072, Adjusted R-squared:  0.007964 \n#> F-statistic:  1.08 on 1 and 9 DF,  p-value: 0.3258\nalpha2<-coefficients(CAPM3_lr)[1]\nalpha2<0\n#> (Intercept) \n#>        TRUE"},{"path":"estacionariedad.html","id":"estacionariedad","chapter":"5 Estacionariedad","heading":"5 Estacionariedad","text":"","code":""},{"path":"estacionariedad.html","id":"el-problema-4","chapter":"5 Estacionariedad","heading":"5.1 El problema","text":"Los fundamentos de las series de tiempo están basados en la\nestacionalidad. Una serie de tiempo \\({r_t}\\) que estudia los retornos de\nun activo lo largo de tiempo es estrictamente estacionaria si la\ndistribución conjunta de los retornos \\((r_{t1},\\dots,r_{t1})\\) es\nexactamente idéntica en \\((r_{t1+T},\\dots,r_{t1+T})\\), es decir cuando\npasa \\(T\\) años, por ejemplo. En otras palabras, definiremos una serie\nde tiempo como un vector de variables \\({X_t}\\) aleatorias de dimensión\n\\(T\\), dado como:\\[\\begin{equation}\n    X_1, X_2, X_3, \\ldots ,X_T\n\\end{equation}\\]Es decir, definiremos una serie de tiempo como una\nrealización de un proceso estocástico –o un Proceso Generador de Datos\n(PGD). Consideremos una muestra de los múltiples posibles resultados de\nmuestras de tamaño \\(T\\), la colección dada por:\\[\\begin{equation}\n    \\{X^{(1)}_1, X^{(1)}_2, \\ldots, X^{(1)}_T\\}\n    \\tag{5.1}\n\\end{equation}\\]Eventualmente podríamos estar dispuestos observar este proceso\nindefinidamente, de forma tal que estemos interesados en observar la\nsecuencia dada por \\(\\{ X^{(1)}_t \\}^{\\infty}_{t = 1}\\), lo cual \ndejaría se ser sólo una de las tantas realizaciones o secuencias del\nproceso estocástico original de la ecuación (5.1).Por lo mismo, cada cambio que se hace al vector \\(\\{ X^{(1)}_t \\}\\) es\nparte del mismo proceso estocástico, por lo que la serie de tiempo es:El proceso estocástico de dimensión \\(T\\) puede ser completamente descrito\npor su función de distribución multivariada de dimensión \\(T\\). \nobstante, sólo nos enfocaremos en sus primer y segundo momentos, es\ndecir, en sus medias o valores esperados \\(\\mathbb{E} (X_t)\\)Para \\(t = 1, 2, \\ldots, T\\):De sus variazas:\\[\\begin{equation*}\n    Var[X_t] = \\mathbb{E}[(X_t - \\mathbb{E}[X_t])^2]\n\\end{equation*}\\] Para \\(t = 1, 2, \\ldots, T\\), y de sus \\(T(T-1)/2\\)\ncovarianzas: \\[\\begin{equation*}\n    Cov[X_t,X_s] = \\mathbb{E}[(X_t - \\mathbb{E}[X_t])(X_s - \\mathbb{E}[X_s])]\n\\end{equation*}\\]Para \\(t < s\\). Por lo tanto, en la forma matricial podemos escribir lo siguiente:\n\\[\\begin{equation*}\n\\left[\n    \\begin{array}{c c c c}\n    Var[X_1] & Cov[X_1,X_2] & \\cdots & Cov[X_1,X_T] \\\\\n    Cov[X_2,X_1] & Var[X_2] & \\cdots & Cov[X_2,X_T] \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    Cov[X_T,X_1] & Cov[X_T,X_2] & \\cdots & Var[X_T] \\\\\n    \\end{array}\n\\right]\n\\end{equation*}\\]\\[\\begin{equation}\n= \\left[\n    \\begin{array}{c c c c}\n    \\sigma_1^2 & \\rho_{12} & \\cdots & \\rho_{1T} \\\\\n    \\rho_{21} & \\sigma_2^2 & \\cdots & \\rho_{2T} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\rho_{T1} & \\rho_{T2} & \\cdots & \\sigma_T^2 \\\\\n    \\end{array}\n\\right]\n\\tag{5.2}\n\\end{equation}\\]Donde es claro que en la matriz de la ecuación (5.2) existen \\(T(T-1)/2\\) covarianzas distintas, ya que se cumple que \\(Cov[X_t,X_s] = Cov[X_s,X_t]\\), para \\(t \\neq s\\). menudo, esas covarianzas son denominadas como autocovarianzas puesto que ellas son covarianzas entre variables aleatorias pertenecientes al mismo proceso estocástico pero en un momento \\(t\\) diferente. Si el proceso estocástico tiene una distribución normal multivariada, su función de distribución estará totalmente descrita por sus momentos de primer y segundo orden.","code":""},{"path":"estacionariedad.html","id":"ergocidad","chapter":"5 Estacionariedad","heading":"5.1.1 Ergocidad","text":"Esto implica que los momentos muestrales, los cuales son calculados en la base de una serie de tiempo con un número finito de observaciones, conforme el tiempo \\(T \\rightarrow \\infty\\) sus correspondientes momentos muestrales, tienden los verdaderos valores poblacionales, los cuales definiremos como \\(\\mu\\), para la media, y \\(\\sigma^2_X\\) para la varianza. En pocas palabras, conforme los momentos muestrales aumenten tanto que tiendan al infinito, entonces nos acercamos valores poblacionales de la media y la varianza.\nEste concepto sólo es cierto si asumimos que\\[\\begin{eqnarray*}\n    \\mathbb{E}[X_t] = \\mu_t = \\mu \\\\\n    Var[X_t] = \\sigma^2_X\n\\end{eqnarray*}\\]\nMás formalmente, se dice que el PGD o el proceso estocástico es ergódico en la media si:\n\\[\\begin{equation}\n    \\displaystyle\\lim_{T \\\\infty}{\\mathbb{E} \\left[ \\left( \\frac{1}{T} \\sum^{T}_{t = 1} (X_t - \\mu) \\right) ^2 \\right]} = 0\n\\end{equation}\\]y ergódico en la varianza si:\n\\[\\begin{equation}\n    \\displaystyle\\lim_{T \\\\infty}{\\mathbb{E} \\left[ \\left( \\frac{1}{T} \\sum^{T}_{t = 1} (X_t - \\mu) ^2 - \\sigma^2_X \\right) ^2 \\right]} = 0\n\\end{equation}\\]\n+\nEstas condiciones se les conoce como propiedades de consistencia para las variables aleatorias. Sin embargo, éstas pueden ser probadas. Por ello se les denomina como un supuesto que pueden cumplir algunas de las series. Más importante aún: un proceso estocástico que tiende estar en equilibrio estadístico en un orden ergódico, es estacionario.","code":""},{"path":"estacionariedad.html","id":"tipos-de-estacionariedad","chapter":"5 Estacionariedad","heading":"5.1.2 Tipos de Estacionariedad","text":"Definiremos la estacionariedad por sus momentos del correspondiente proceso estocástico dado por \\(\\{X_t\\}\\):Estacionariedad en media: Un proceso estocástico es estacionario en media si \\(E[X_t] = \\mu_t = \\mu\\) es constante para todo \\(t\\).Estacionariedad en media: Un proceso estocástico es estacionario en media si \\(E[X_t] = \\mu_t = \\mu\\) es constante para todo \\(t\\).Estacionariedad en varianza: Un proceso estocástico es estacionario en varianza si \\(Var[X_t] = \\mathbb{E}[(X_t - \\mu_t)^2] = \\sigma^2_X = \\gamma(0)\\) es constante y finita para todo \\(t\\).Estacionariedad en varianza: Un proceso estocástico es estacionario en varianza si \\(Var[X_t] = \\mathbb{E}[(X_t - \\mu_t)^2] = \\sigma^2_X = \\gamma(0)\\) es constante y finita para todo \\(t\\).Estacionariedad en covarianza: Un proceso estocástico es estacionario en covarianza si \\(Cov[X_t,X_s] = \\mathbb{E}[(X_t - \\mu_t)(X_s - \\mu_s)] = \\gamma(|s-t|)\\) es sólo una función del tiempo y de la distancia entre las dos variables aleatorias. Por lo que depende del tiempo denotado por \\(t\\) (depende de la información contemporánea).Estacionariedad en covarianza: Un proceso estocástico es estacionario en covarianza si \\(Cov[X_t,X_s] = \\mathbb{E}[(X_t - \\mu_t)(X_s - \\mu_s)] = \\gamma(|s-t|)\\) es sólo una función del tiempo y de la distancia entre las dos variables aleatorias. Por lo que depende del tiempo denotado por \\(t\\) (depende de la información contemporánea).Estacionariedad débil: Como la estacionariedad en varianza resulta de forma inmediata de la estacionariedad en covarianza cuando se asume que \\(s = t\\), un proceso estocástico es débilmente estacionario cuando es estacionario en media y covarianza. ESTE ES EL MÁS COMÚN Y POSIBLE, por lo que es el que estudiaremos.Estacionariedad débil: Como la estacionariedad en varianza resulta de forma inmediata de la estacionariedad en covarianza cuando se asume que \\(s = t\\), un proceso estocástico es débilmente estacionario cuando es estacionario en media y covarianza. ESTE ES EL MÁS COMÚN Y POSIBLE, por lo que es el que estudiaremos.","code":""},{"path":"estacionariedad.html","id":"función-de-autocorrelación-acf","chapter":"5 Estacionariedad","heading":"5.1.3 Función de Autocorrelación (ACF)","text":"Para ampliar la discusión, es posible calcular la fuerza o intensidad de la dependencia de las variables aleatorias dentro de un proceso estocástico, ello mediante el uso de las autocovarianzas. Cuando las covarianzas son normalizadas respecto de la varianza, el resultado es un término que es independiente de las unidad de medida aplicada, y se conoce como la función de autocorrelación.Por su parte, un estimador consistente de la función de autocorrelación estará dado por:\n\\[\\begin{equation}\n    \\hat{\\rho}(\\tau) = \\frac{\\sum^{T - \\tau}_{t=1} (X_t - \\hat{\\mu})(X_{t+\\tau} - \\hat{\\mu})}{\\sum^T_{t=1} (X_t - \\hat{\\mu})^2} = \\frac{\\hat{\\gamma}(\\tau)}{\\hat{\\gamma}(0)} \\mbox{, para } \\tau = 1, 2, \\ldots, T-1\n    \\tag{5.3}\n\\end{equation}\\]El estimador de la ecuación (5.3) es asintóticamente insesgado y es relevante puesto que nos dice si una serie de tiempo con estacionariedad débil esta serialmente correlacionada si y solo si \\(\\hat{\\rho}(\\tau)\\neq0\\).","code":""},{"path":"estacionariedad.html","id":"ruido-blanco","chapter":"5 Estacionariedad","heading":"5.1.4 Ruido Blanco","text":"Supongamos una serie de tiempo denotada por: \\(\\{U_t\\}^T_{t = 0}\\). Decimos que el proceso estocástico \\(\\{U_t\\}\\) es un proceso estocástico puramente aleatorio o es un proceso estocástico de ruido blanco o caminata aleatoria, si éste tiene las siguientes propiedades:\\(\\mathbb{E}[U_t] = 0\\), \\(\\forall t\\)\\(\\mathbb{E}[U_t] = 0\\), \\(\\forall t\\)\\(Var[U_t] = \\mathbb{E}[(U_t - \\mu_t)^2] = \\mathbb{E}[(U_t - \\mu)^2] = \\mathbb{E}[(U_t)^2] = \\sigma^2\\), \\(\\forall t\\)\\(Var[U_t] = \\mathbb{E}[(U_t - \\mu_t)^2] = \\mathbb{E}[(U_t - \\mu)^2] = \\mathbb{E}[(U_t)^2] = \\sigma^2\\), \\(\\forall t\\)\\(Cov[U_t,U_s] = \\mathbb{E}[(U_t - \\mu_t)(U_s - \\mu_s)] = \\mathbb{E}[(U_t - \\mu)(U_s - \\mu)] = \\mathbb{E}[U_t U_s] = 0\\), \\(\\forall t \\neq s\\).\\(Cov[U_t,U_s] = \\mathbb{E}[(U_t - \\mu_t)(U_s - \\mu_s)] = \\mathbb{E}[(U_t - \\mu)(U_s - \\mu)] = \\mathbb{E}[U_t U_s] = 0\\), \\(\\forall t \\neq s\\).\\(\\hat{\\rho}(\\tau)=0\\)\\(\\hat{\\rho}(\\tau)=0\\)En palabras. Un proceso \\(U_t\\) es un ruido blanco si su valor promedio es cero (0), tiene una varianza finita y constante, y además le importa la historia pasada, así su valor presente se ve influenciado por sus valores pasados importando respecto de que periodo se tome referencia.Para procesos estacionarios, dicha función de autocorrelación esta dada por:\n\\[\\begin{equation}\n    \\rho(\\tau) = \\frac{\\mathbb{E}[(X_t - \\mu)(X_{t+\\tau} - \\mu)]}{\\mathbb{E}[(X_t - \\mu)^2]} = \\frac{\\gamma(\\tau)}{\\gamma(0)}\n\\end{equation}\\]","code":""},{"path":"estacionariedad.html","id":"estimación-1","chapter":"5 Estacionariedad","heading":"5.2 Estimación","text":"","code":""},{"path":"estacionariedad.html","id":"dependencias-2","chapter":"5 Estacionariedad","heading":"5.2.1 Dependencias","text":"","code":"\n#install.packages(\"pacman\")\n#pacman nos permite cargar varias librerias en una sola línea\nlibrary(pacman)\npacman::p_load(tidyverse,BatchGetSymbols,ggplot2,lubridate,readxl,forecast,stats)"},{"path":"estacionariedad.html","id":"caminata","chapter":"5 Estacionariedad","heading":"5.3 Caminata","text":"\nFigure 5.1: Ejemplo de 10 trayectorias de la caminata aleatoria, cuando sólo es posible cambios de +1 y -1\nAsí, el proceso estocástico dado por la caminata alaeatoria sin un\ntérmino de ajuste es estacionario en media, pero en varianza o en\ncovarianza, y consecuentemente, en general estacionario, condición\nque contraria al caso del proceso simple descrito en \\(U_t\\).Es facil ver que muchas de las posibilidades de realización de este\nproceso estocástico (series de tiempo) pueden tomar cualquiera de las\nrutas consideradas en el Figura 5.1. Ahora analicemos\nun solo camino.","code":"\n\nset.seed(1234)\n# Utilizaremos una función guardada en un archivo a parte\n# Llamamos a la función:\nsource(\"funciones/Caminata.R\")\n\n# Definimos argumentos de la función\nOpciones <- c(-1, 1)\n#\nSoporte <- 10000\n\n# Vamos a réplicar el proceso con estos parámetros\nRango <- 200\n#\nCaminos <- 10\n\n#\n\nfor(i in 1:Caminos){\n  TT <- data.matrix(data.frame(Caminata(Opciones, Soporte)[1]))\n  #\n  G_t <- data.matrix(data.frame(Caminata(Opciones, Soporte)[2]))\n  #\n  plot(TT, G_t, col = \"blue\", type = \"l\", ylab = \"Ganancias\", xlab = \"Tiempo\", ylim = c(-Rango,Rango))\n  #\n  par(new = TRUE)\n  #\n  i <- i +1\n}\n#\npar(new = FALSE)"},{"path":"estacionariedad.html","id":"un-camino","chapter":"5 Estacionariedad","heading":"5.4 Un camino","text":"\nFigure 5.2: Una Caminata aleatoria cuando sólo es posible cambios de +1 y -1\nHay que convertirlo serie de tiempo","code":"\n#Generamos datos\n  TT1 <- data.matrix(data.frame(Caminata(Opciones, Soporte)[1]))\n  G_t1 <- data.matrix(data.frame(Caminata(Opciones, Soporte)[2]))\n#Creemos un data frame\n  dt_caminata<-data.frame(TT1,G_t1)\n  colnames(dt_caminata)<-c(\"t\",\"ganancias\")\n  head(dt_caminata)\n#>   t ganancias\n#> 1 1        -1\n#> 2 2        -2\n#> 3 3        -1\n#> 4 4        -2\n#> 5 5        -3\n#> 6 6        -4\n#plot\n  plot(TT1, G_t1, col = \"blue\", type = \"l\", ylab = \"Ganancias\", xlab = \"Tiempo\", ylim = c(-Rango,Rango))\n#serie de tiempo\ncaminata_ts<-ts(G_t1,start=1,end=Soporte)"},{"path":"estacionariedad.html","id":"estacionariedad-caminata","chapter":"5 Estacionariedad","heading":"5.4.1 Estacionariedad Caminata","text":"\nFigure 5.3: Función de Autocorrelación de una Caminata\nComo se comentó con anterioridad en la Figura 5.3 es\nevidente que la Caminata si tiene autocorrelacion, por lo que nuestro\nplot de autocorrelacion tiene valores muy altos en todos los lags.\nVeamos los lags.\nFigure 5.4: Lags de una sola caminata\nDe nuevo, esto al ser creado de manera estandarizada estamos seguros de\nque va ser estacionario en la medio, por lo mismo los lags de la\nFigura 5.4 se ven tan correlacionados.","code":"\nACF_caminata_ts<-acf(caminata_ts,na.action = na.pass, main = \"Función de Autocorrelación de una Caminata\")\ngglagplot(caminata_ts,lags=10,do.lines=FALSE,colour=FALSE)+theme_light()"},{"path":"estacionariedad.html","id":"precios-de-un-activo","chapter":"5 Estacionariedad","heading":"5.5 Precios de un activo","text":"Veamos la serie de tiempo\nFigure 5.5: Serie de tiempo de los retornos de año en los últimos 20 años\n","code":"\n#Primero determinamos el lapso de tiempo\npd<-Sys.Date()-(365*20) #primer fecha\npd\n#> [1] \"2002-10-07\"\nld<-Sys.Date() #última fecha\nld\n#> [1] \"2022-10-02\"\n#Intervalos de tiempo\nint<-\"monthly\"\n#Datos a elegir\ndt<-c(\"AMZN\")\ndt2<-c(\"TSLA\")\n#Descargando los valores\ndata1<- BatchGetSymbols(tickers = dt,\n                       first.date = pd,\n                       last.date = ld,\n                       freq.data = int,\n                       do.cache = FALSE,\n                       thresh.bad.data = 0)\n#Generando data frame con los valores\ndata_precio_amzn<-data1$df.tickers\ncolnames(data_precio_amzn)\n#>  [1] \"ticker\"              \"ref.date\"           \n#>  [3] \"volume\"              \"price.open\"         \n#>  [5] \"price.high\"          \"price.low\"          \n#>  [7] \"price.close\"         \"price.adjusted\"     \n#>  [9] \"ret.adjusted.prices\" \"ret.closing.prices\"\n\n#necesitamos convertir la serie de tiempo de precios en retornos continuos compuestos de los precios de apertura\ndata_precio_amzn$ccrAMZN<-c(NA ,100*diff(log(data_precio_amzn$price.open)))#agregamos un valor NA al principio\ndata_precio_amzn$ccrAMZN#estos son los retornos\n#>   [1]           NA  14.95653017  22.83329766 -22.98950701\n#>   [5]  13.39221448   0.95260416  14.27998202  11.55626991\n#>   [9]  24.11122449  -0.46684144  13.08785513  11.63599301\n#>  [13]   3.89974592  12.48104071  -0.73260401  -3.06108260\n#>  [17]  -4.08140972 -16.32741069   0.99457279   0.06902105\n#>  [21]   9.61879412  11.67844638 -33.59147712  -0.57381482\n#>  [25]   7.69997922 -18.78100714  15.60691856  11.66713068\n#>  [29]  -4.43506452 -20.41392362  -1.23405211  -6.96531282\n#>  [33]   9.64353557  -6.77486160  30.02382912  -5.40177077\n#>  [37]   6.39945115 -12.58398922  20.12391421  -2.92703823\n#>  [41]  -7.77281354 -15.93630872  -2.10477279  -4.11970307\n#>  [45]  -1.60415929  10.64572285 -37.21478392  15.01070027\n#>  [49]   3.59739571  17.58906665   5.43570463  -4.00357496\n#>  [53]  -1.90531667   3.54637914   1.33891100  42.77167396\n#>  [57]  11.98170332  -0.13070948  12.66409736   2.27857959\n#>  [61]  15.63296023  -6.26135927   2.56510858   5.74113839\n#>  [65] -18.78533471 -21.72447597  13.78662202   7.15014819\n#>  [69]   3.44753666 -11.63053849   5.54650897   8.53074641\n#>  [73] -14.71605773 -24.20236452 -29.39126222  20.09953181\n#>  [77]  13.15576837   8.77225235  13.27882326   9.60320128\n#>  [81]  -2.73678724   7.64068237   2.50334747  -6.96036997\n#>  [85]  13.59745291  24.90536163  14.32806129  -0.50514401\n#>  [89] -10.08447333  -3.70473986  13.45839135   1.02565002\n#>  [93]  -9.33660068 -13.76436786   8.99531736   5.87517724\n#>  [97]  21.76202538   4.58513429   8.56726887   1.22598823\n#> [101]  -6.16865517   1.74979032   4.53458328   7.93222740\n#> [105]  -0.25978671   4.72685785   9.04110889  -4.41608958\n#> [109]   0.80039290  -4.18766494  -8.13529694  -8.68550170\n#> [113]  -1.18960511   3.43828044   9.60224827  14.70991690\n#> [117]  -9.58159747   9.53799598   2.08880372   5.85976366\n#> [121]   2.83140800  -8.65274050   7.52661134   1.39202437\n#> [125]   4.89612270  -2.12710012   1.39936277  -5.02332605\n#> [129]   5.76221809   3.66491122   8.27850152  -6.24554343\n#> [133]   9.85520147  15.15285156   8.73395739  -0.05013788\n#> [137] -10.51934673  -0.06687288  -5.92858190 -10.58568315\n#> [141]   2.74371861   4.15753522  -3.80625428   8.04816141\n#> [145]  -5.42111518  -5.03065911   9.90317538  -7.85404256\n#> [149]  11.32156221   8.43295383  -2.32429615  13.01462014\n#> [153]   1.54061721   2.05813986  20.15392877  -7.39490376\n#> [157]   2.34828935  20.47843365   7.17052347  -2.62563883\n#> [161] -12.67694180  -3.85435390   5.96629237  11.72089295\n#> [165]   8.23387458  -0.49783030   5.76252931   1.44112456\n#> [169]   8.10699776  -4.52676184  -6.00796092   0.72964776\n#> [173]   8.98955770   2.83447462   4.01536256   4.38443827\n#> [177]   7.35281333  -2.61760725   2.36894617  -1.20285851\n#> [181]  -2.07377928  13.68712240   5.85471090  -0.00427124\n#> [185]  20.93976645   4.63815978  -6.55115525   9.77684681\n#> [189]   4.61358018   2.75160333   5.84583306  12.74521370\n#> [193]  -0.22279328 -21.94794383   8.60716489 -18.86826361\n#> [197]  11.20213025   0.98664739   8.39682297   7.12720047\n#> [201]  -9.38002536   8.85565506  -2.70183034  -5.58782369\n#> [205]  -1.36520548   2.37757442   0.91249016   3.83805218\n#> [209]   6.98245156  -5.31693095   1.37938043  18.97247680\n#> [213]   4.64889431  11.92308161  14.25393454   9.27398656\n#> [217]  -8.41337555  -4.66642316   4.05671846   2.52393793\n#> [221]  -0.84885499  -3.59427728  -0.31861156  11.12179968\n#> [225]  -7.17375312   5.72503641  -2.40180912   4.18486227\n#> [229]  -6.11473001   2.18899134   5.30616390  -5.62793333\n#> [233] -11.06465380   1.80527180   7.20896224 -29.34750864\n#> [237]  -0.11853658 -13.99459654  23.88072732  -6.86965832\n#tenemos 20 retornos a lo largo de 20 años\nret_20_amazn<-ggplot(data=data_precio_amzn, aes(x=ref.date))+geom_line(aes(y=ccrAMZN))+labs(title=\"Retornos de AMZN en los últimos 20 años\",y=\"Retornos\", x=\"Años\")+theme_light()\nret_20_amazn"},{"path":"estacionariedad.html","id":"serie-de-tiempo","chapter":"5 Estacionariedad","heading":"5.5.1 Serie de tiempo","text":"Primero que nada es importante cargar los datos un objeto series de\ntiempo. Esto nos lo permite la función ts(). Además debemos serciorarnos\nde que los datos esten en orden cronológico.","code":"\ndata_precio_amzn<-data_precio_amzn[order(data_precio_amzn$ref.date),]\nhead(data_precio_amzn)#dado que ya estaba en orden cronológico nuestro df no cambia\n#> # A tibble: 6 × 11\n#>   ticker ref.date     volume price…¹ price…² price…³ price…⁴\n#>   <chr>  <date>        <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n#> 1 AMZN   2002-10-07   3.44e9   0.828    1.01   0.818   0.968\n#> 2 AMZN   2002-11-01   4.13e9   0.961    1.23   0.91    1.17 \n#> 3 AMZN   2002-12-02   3.11e9   1.21     1.25   0.922   0.944\n#> 4 AMZN   2003-01-02   3.38e9   0.960    1.16   0.928   1.09 \n#> 5 AMZN   2003-02-03   2.32e9   1.10     1.12   0.980   1.10 \n#> 6 AMZN   2003-03-03   3.28e9   1.11     1.40   1.07    1.30 \n#> # … with 4 more variables: price.adjusted <dbl>,\n#> #   ret.adjusted.prices <dbl>, ret.closing.prices <dbl>,\n#> #   ccrAMZN <dbl>, and abbreviated variable names\n#> #   ¹​price.open, ²​price.high, ³​price.low, ⁴​price.close\n#hagamos el objeto ts\nret_amazn_ts<-ts(data_precio_amzn$ccrAMZN)\nplot(ret_amazn_ts)#de esta manera podemos ver que se cargo bien debido a que es igual al ggplot"},{"path":"estacionariedad.html","id":"estacionariedad-1","chapter":"5 Estacionariedad","heading":"5.5.2 Estacionariedad","text":"\nFigure 5.6: Lag Plot que nos muestra la correlación entre 20 lags\n\nFigure 5.7: Función de Autocorrelación de los retornos de AMZN en los ultimos 20 años\nLa Figura 5.6 nos idica la manera en la que se\ncorrelacionan los lags, evidentemente se puede ver ningún tipo de\ncorrelacioo1ón visible. Similarmente la Figura 5.7 en\ndonde se muestra la función de autocorrelación. Expecto al primer lag\n–que muestra correlacion debido que se esta comparando consigo\nmismo– es evidente que hay correlacioo1ón fuerte entre ninguno de\nlos lags. Por lo mismo, sería difícil poder encontrar y estimar valores\nfuturos debido que la Figura 5.6 y la Figura\n5.7 indican que la serie de tiempo de los retornos de\nAMZN de la Figura 5.5 es completamente aleatorio y \nhay estacionariedad.","code":"\n#MA_m5<-forecast::ma(ret_amazn_ts,order=11,centre=TRUE)\n#plot(ret_amazn_ts)+lines(MA_m5, col=\"red\", lwd=2)\ngglagplot(ret_amazn_ts,lags=20,do.lines=FALSE,colour=FALSE)+theme_light()\nACF_ret_amazn_ts<-acf(ret_amazn_ts,na.action = na.pass)"},{"path":"procesos-estacionarios-univariados.html","id":"procesos-estacionarios-univariados","chapter":"6 Procesos estacionarios univariados","heading":"6 Procesos estacionarios univariados","text":"En este capítulo analizaremos el método o metodología de análisis de\nseries de tiempo propuesto por Box y Jenkins (1970). Los modelos\npropuestos dentro de está metodología o conjunto de métodos se han\nvuelto indispensables para efectos de realizar pronósticos de corto\nplazo.En este sentido, se analizarán los métodos más importantes en series de\ntiempo: Autoregresivos (AR) y de Medias Móviles (MA). Asimismo, se\nrealizará un análisis de los procesos que resultan de la combinación de\nambos, conocida como ARMA, los cuales son más comúnmente usados para\nrealizar pronósticos.","code":""},{"path":"procesos-estacionarios-univariados.html","id":"procesos-autoregresivos-ar","chapter":"6 Procesos estacionarios univariados","heading":"6.1 Procesos Autoregresivos (AR)","text":"Los procesos autoregresivos tienen su origen en el trabajo de Cochrane y\nOrcutt de 1949, mediante el cual analizaron los residuales de una\nregresión clásica como un proceso autoregresivo. Puede consultarse el\napéndice para la discusión del modelo de regresión clásica.","code":""},{"path":"procesos-estacionarios-univariados.html","id":"ar1","chapter":"6 Procesos estacionarios univariados","heading":"6.1.1 AR(1)","text":"Como primer caso analizaremos al proceso autoregresivo de primer orden,\n\\(AR(1)\\), el cual podemos definir como una Ecuación Lineal en Diferencia\nEstocástica de Primer Orden. Diremos que una Ecuación Lineal en\nDiferencia de Primer Orden es estocástica si en su representación\nanalítica considera un componente estocástico como en la ecuación\n(6.1) descrita continuación:\\[\\begin{equation}\n    X_t = a_0 + a_1 X_{t-1} + U_t\n\\tag{6.1}\n\\end{equation}\\]Donde \\(a_0\\) es un término constante, \\(U_t\\) es un proceso estacionario,\ncon media cero (0), una varianza finita y constante (\\(\\sigma^2\\)) y una\ncovarianza que depende de la distancia entre \\(t\\) y cualquier \\(t-s\\)\n(\\(\\gamma_s\\))–que depende de los valores pasados o futuros de la\nvariable–, \\(X_0\\) es el valor inicial de \\(X_t\\). obstante, en general\nvamos asumir que la covarianza será cero (0), por lo que tendremos un\nproceso puramente aleatorio. Considerando la ecuación (6.1) y\nun proceso de sustitución sucesivo podemos establecer lo siguiente,\nempezando con \\(X_1\\): \\[\\begin{eqnarray*}\n    X_{1} & = & a_0 + a_1 X_{0} + U_{1}\n\\end{eqnarray*}\\]Para \\(X_2\\): \\[\\begin{eqnarray*}\nX_{2} & = & a_0 + a_1 X_{1} + U_{2} \\\\\n    & = & a_0 + a_1 (a_0 + a_1 X_{0} + U_{1}) + U_{2} \\\\\n    & = & a_0 + a_1 a_0 + a_1^2 X_{0} + a_1 U_{1} + U_{2}\n\\end{eqnarray*}\\]Para \\(X_3\\): \\[\\begin{eqnarray*}\nX_{3} & = & a_0 + \\alpha X_{2} + U_{3} \\\\\n    & = & a_0 + a_1 (a_0 + a_1 a_0 + a_1^2 X_{0} + a_1 U_{1} + U_{2}) + U_{3} \\\\\n    & = & a_0 + a_1 a_0 + a_1^2 a_0 + a_1^3 X_{0} + a_1^2 U_{1} + a_1 U_{2} + U_{3}\n\\end{eqnarray*}\\]Así, para cualquier \\(X_t\\), \\(t = 1, 2, 3, \\ldots\\), obtendríamos:\n\\[\\begin{eqnarray}\nX_{t} & = & a_0 + a_1 X_{t - 1} + U_{t} \\nonumber \\\\\n    & = & a_0 + a_1 (a_0 + a_1 a_0 + a_1^2 a_0 + \\ldots + a_1^{t-2} a_0 + a_1^{t-1} X_{0} \\nonumber \\\\\n    &   & + a_1^{t-2} U_{1} + \\ldots + a_1 U_{t - 2} + U_{t - 1}) + U_{t} \\nonumber \\\\\n    & = & a_0 + a_1 a_0 + a_1^2 a_0 + a_1^3 a_0 + \\ldots + a_1^{t-1} a_0 + a_1^{t} X_{0} \\nonumber \\\\\n    &   & + a_1^{t-1} U_{1} + \\ldots a_1^2 U_{t - 2} + a_1 U_{t - 1} + U_{t} \\nonumber \\\\\n    & = & (1 + a_1 + a_1^2 + a_1^3 + \\ldots + a_1^{t-1}) a_0 + a_1^{t} X_{0} \\nonumber \\\\\n    &   & + a_1^{t-1} U_{1} + \\ldots + a_1^2 U_{t - 2} + a_1 U_{t - 1} + U_{t}  \\nonumber\\\\\n    & = & \\frac{1 - a_1^t}{1 - a_1} a_0 + a_1^{t} X_{0} + \\sum^{t-1}_{j = 0} a_1^{j} U_{t - j}\n    \\tag{6.2}\n\\end{eqnarray}\\]De esta forma en la ecuación (6.2) observamos un proceso que\nes explicado por dos partes: una que depende del tiempo y otra que\ndepende de un proceso estocástico. Asimismo, debe notarse que la\ncondición de convergencia es idéntica que en el caso de ecuaciones en\ndiferencia estudiadas al inicio del curso: \\(\\lvert a_1 \\lvert < 1\\), por\nlo que cuando \\(t \\\\infty\\), la expresión (6.2) será la\nsiguiente:\\[\\begin{equation}\n    X_t = \\frac{1}{1 - a_1} a_0 + \\sum^{\\infty}_{j = 0} a_1^{j} U_{t - j}\n    \\tag{6.3}\n\\end{equation}\\]Así, desaparece la parte dependiente del tiempo y únicamente prevalece\nla parte que es dependiente del proceso estocástico. Esta es la solución\nde largo plazo del proceso \\(AR(1)\\), la cual depende del proceso\nestocástico. Notemos, además, que esta solución implica que la variable\no la serie de tiempo \\(X_t\\) es tambien un proceso estocástico que hereda\nlas propiedades de \\(U_t\\). Así, \\(X_t\\) es también un proceso estocástico\nestacionario, como demostraremos más adelante.Observemos que la ecuación (6.3) se puede reescribir si\nconsideramos la formulación que en la literatura se denomina como la\ndescomposición de Wold, en la cual se define que es posible asumir que\n\\(\\psi_j = a_1^j\\) y se considera el caso en el cual\n\\(\\lvert a_1 \\lvert< 1\\), de esta forma tendremos que por ejemplo cuando:\n\\[\\begin{equation*}\n    \\sum^{\\infty}_{j = 0} \\psi^2_j = \\sum^{\\infty}_{j = 0} a_1^{2j} = \\frac{1}{1 - a_1^2}\n\\end{equation*}\\]Alternativamente y de forma similar las ecuaciones en diferencia\nestudiadas previamente podemos escribir el proceso \\(AR(1)\\) mediante el\nuso del operador rezago como:\\[\\begin{eqnarray}\n    X_t & = & a_0 + a_1 L X_t + U_t \\nonumber \\\\\n    X_t - a_1 L X_t & = & a_0 + U_t \\nonumber \\\\\n    (1 - a_1 L) X_t & = & a_0 + U_t \\nonumber \\\\\n    X_t & = & \\frac{a_0}{1 - a_1 L} + \\frac{1}{1 - a_1 L} U_t\n    \\tag{6.4}\n\\end{eqnarray}\\]En esta última ecuación retomamos el siguiente término para reescribirlo\ncomo:\\[\\begin{equation}\n    \\frac{1}{1 - a_1 L} = 1 + a_1 L + a_1^2 L^2 + a_1^3 L^3 + \\ldots\n    \\tag{6.5}\n\\end{equation}\\]Tomando este resultado para sustituirlo en ecuación (6.4),\nobtenemos la siguiente expresión:\\[\\begin{eqnarray}\nX_t & = & (1 + a_1 L + a_1^2 L^2 + a_1^3 L^3 + \\ldots) a_0 + (1 + a_1 L + a_1^2 L^2 + a_1^3 L^3 + \\ldots) U_t \\nonumber \\\\\n    & = & (1 + a_1 + a_1^2 + a_1^3 + \\ldots) a_0 + U_t + a_1 U_{t-1} + a_1^2 U_{t-2} + a_1^3 U_{t-3} + \\ldots \\nonumber \\\\\n    & = & \\frac{a_0}{1 - a_1} + \\sum^{\\infty}_{j = 0} a_1^j U_{t-j}\n    \\tag{6.6}\n\\end{eqnarray}\\]Donde la condición de convergencia y estabilidad del proceso descrito en\nesta ecuación es que \\(\\lvert a_1 \\lvert < 1\\). Por lo que hemos\ndemostrado que mediante el uso del operador de rezago es posible llegar\nal mismo resultado que obtuvimos mediante el procedimiento de\nsustituciones iterativas.La ecuación (6.6) se puede interpretar como sigue. La\nsolución o trayectoria de equilibrio de un AR(1) se divide en dos\npartes. La primera es una constante que depende de los valores de \\(a_0\\)\ny \\(a_1\\). La segunda parte es la suma ponderada de las desviaciones o\nerrores observados y acumulados en el tiempo hasta el momento \\(t\\).Ahora obtendremos los momentos que describen la serie de tiempo cuando\nse trata de un porceso \\(AR(1)\\). Para ello debemos obtener la media, la\nvarianza y las covarianzas de \\(X_t\\). Para los siguientes resultados\ndebemos recordar y tener en mente que si \\(U_t\\) es un proceso puramente\naleatorio, entonces:\\(\\mathbb{E}[U_t] = 0\\) para todo \\(t\\)\\(Var[U_t] = \\sigma^2\\) para todo \\(t\\)\\(Cov[U_t, U_s] = 0\\) para todo \\(t \\neq s\\)Dicho lo anterior y partiendo de la ecuación (6.6), el primer\nmomento o valor esperado de la serie de tiempo será el siguiente:\n\\[\\begin{eqnarray}\n\\mathbb{E}[X_t] & = & \\mathbb{E} \\left[ \\frac{a_0}{1 - a_1} + \\sum^{\\infty}_{j = 0} a_1^j U_{t-j} \\right] \\nonumber \\\\\n    & = & \\frac{a_0}{1 - a_1} + \\sum^{\\infty}_{j = 0} a_1^j \\mathbb{E}[U_{t-j}] \\nonumber \\\\\n    & = & \\frac{a_0}{1 - a_1} = \\mu\n    \\tag{6.7}\n\\end{eqnarray}\\]Respecto de la varianza podemos escribir la siguiente expresión partir\nde la ecuación (6.6):\\[\\begin{eqnarray}\nVar[X_t] & = & \\mathbb{E}[(X_t - \\mu)^2] \\nonumber \\\\\n    & = & \\mathbb{E} \\left[ \\left( \\frac{a_0}{1 - a_1} + \\sum^{\\infty}_{j = 0} a_1^j U_{t-j} - \\frac{a_0}{1 - a_1} \\right)^2 \\right] \\nonumber \\\\\n    & = & \\mathbb{E}[(U_{t} + a_1 U_{t-1} + a_1^2 U_{t-2} + a_1^3 U_{t-3} + \\ldots)^2] \\nonumber \\\\\n    & = & \\mathbb{E}[U^2_{t} + a_1^2 U^2_{t-1} + a_1^4 U^2_{t-2} + a_1^6 U^2_{t-3} + \\ldots \\nonumber \\\\\n    &   & + 2 a_1 U_t U_{t-1} + 2 a_1^2 U_t U_{t-2} + \\ldots] \\nonumber \\\\\n    & = & \\mathbb{E}[U^2_{t}] + a_1^2 \\mathbb{E}[U^2_{t-1}] + a_1^4 \\mathbb{E}[U^2_{t-2}] + a_1^6 \\mathbb{E}[U^2_{t-3}] + \\ldots \\nonumber \\\\\n    & = & \\sigma^2 + a_1^2 \\sigma^2 + a_1^4 \\sigma^2 + a_1^6 \\sigma^2 + \\ldots \\nonumber \\\\\n    & = & \\sigma^2 (1 + a_1^2 + a_1^4 + a_1^6 + \\ldots) \\nonumber \\\\\n    & = & \\sigma^2 \\frac{1}{1 - a_1^2} = \\gamma(0)\n    \\tag{6.8}\n\\end{eqnarray}\\]Previo analizar la covarianza de la serie recordemos que para el\nproceso puramente aleatorio \\(U_t\\) su varianza y covarianza puede verse\ncomo \\(\\mathbb{E}[U_t, U_s] = \\sigma^2\\), para \\(t = s\\), y\n\\(\\mathbb{E}[U_t, U_s] = 0\\), para cualquier otro caso, respectivamente.Dicho lo anterior, partiendo de la ecuación (6.6) la\ncovarianza de la serie estará dada por:\\[\\begin{eqnarray}\nCov(X_t, X_{t-\\tau}) & = & \\mathbb{E}[(X_t - \\mu)(X_{t-\\tau} - \\mu)] \\nonumber \\\\\n    & = & \\mathbb{E} \\left[ \\left( \\frac{a_0}{1 - a_1} + \\sum^{\\infty}_{j = 0} a_1^j U_{t-j} - \\frac{a_0}{1 - a_1} \\right) \\right. \\nonumber \\\\\n    &   & \\left. \\times \\left( \\frac{a_0}{1 - a_1} + \\sum^{\\infty}_{j = 0} a_1^j U_{t-\\tau-j} - \\frac{a_0}{1 - a_1} \\right) \\right] \\nonumber \\\\\n    & = & a_1^{\\tau} \\mathbb{E}[U^2_{t-\\tau} + a_1 U^2_{t-\\tau-1} + a_1^2 U^2_{t-\\tau-2} + a_1^3 U^2_{t-\\tau-3} + \\ldots] \\nonumber \\\\\n    & = & a_1^{\\tau} \\sigma^2 \\frac{1}{1 - a_1^2} = \\gamma(\\tau)\n    \\tag{6.9}\n\\end{eqnarray}\\]Notése que con estos resultados en las ecuaciones (6.8) y\n(6.9) podemos construir la función de autocorrelación teórica\ncomo sigue:\\[\\begin{eqnarray}\n\\rho(\\tau) & = & \\frac{\\gamma(\\tau)}{\\gamma(0)} \\nonumber \\\\\n    & = & a_1^\\tau\n    \\tag{6.10}\n\\end{eqnarray}\\]Donde \\(\\tau = 1, 2, 3, \\ldots\\) y \\(\\lvert a_1 \\lvert < 1\\). Este último\nresultado significa que cuando el proceso autoregresivo es de orden 1\n(es decir, AR(1)) la función de autocorrelación teóricamente es igual al\nparámetro \\(a_1\\) elevado al número de rezagos considerados. obstante,\nnote que esto significa que la autocorrelación observada sea como lo\nexpresa en planteamiento anterior. Por el contrario, una observación\nsencilla mostraría que la autocorrelación observada sería ligeramente\ndistinta la autocorrelación teórica.Ahora veámos algunos ejemplos. En el primer ejemplo simularemos una\nserie y mostraremos el analísis de un proceso construído considerando un\nproceso puramente aleatorio como componente \\(U_t\\).1\nPor su parte, en un segundo ejemplo aplicaremos el análisis una serie\nde tiempo de una variable económica observada.2Para el primer ejemplo consideremos un proceso dado por la forma de un\n\\(AR(1)\\) como en la ecuación (6.4) cuya solución esta dada por la\necuación (6.6). En especifico, supongamos que el término o\ncomponente estocástico \\(U_t\\) es una serie generada partir de numeros\naleatorios de una función normal con media \\(0\\) y desviación estándar\n\\(4\\). Los detalles del proceso simulado se muestra en las siguientes\ngráficas.","code":""},{"path":"procesos-estacionarios-univariados.html","id":"ejemplo-ar1","chapter":"6 Procesos estacionarios univariados","heading":"6.1.1.1 EJEMPLO AR(1)","text":"Por lo tanto tenemos la serie de tiempo \\(AR(1)\\):\n\\[ X_t= 5+0.9X_{t-1}+U_t\\]En este caso el termino estocastico tiene una media de \\(0\\) y una\ndesviación estándar constante \\(\\sigma^2=4\\)\nFigure 6.1: AR(1) considerando \\(X_t=5+0.9X_{t-1}+U_t\\) ; \\(X_0=50\\) y que \\(U_t\\)~\\(N(0, 4)\\) y que \\(U_t \\sim \\mathcal{N}(0, 4)\\)\n\nFigure 6.2: \\(X_t = \\frac{5}{1 - 0.9} + \\sum_{j = 0}^{t-1} 0.9^j U_{t-j}\\), y que \\(U_t \\sim \\mathcal{N}(0, 4)\\)}\n\nFigure 6.3: Función de autocorrelación de un AR(1): \\(\\rho(\\tau)=\\frac{\\gamma( au)}{\\gamma(0)}\\)\n\nFigure 6.4: Función de autocorrelación de un AR(1): \\(\\rho(\\tau)= a_1^\\tau\\)\n\nFigure 6.5: Función de autocorrelación de un AR(1): \\(\\rho(\\tau)= a_1^\\tau\\)\n\nFigure 6.6: AR(1) considerando en conjunto \\(X_t = 5 + 0.9 X_{t-1} + U_t\\); \\(X_0 = 50\\) y \\(X_t = \\frac{5}{1 - 0.9} + \\sum_{j = 0}^{t-1} 0.9^j U_{t-j}\\), y que \\(U_t \\sim \\mathcal{N}(0, 4)\\)\nLa Figura 6.1 ilustra el comportamiento que se debería\nobservar en una serie considerando el procedimiento iterativo de\nconstrucción. Por su parte, la Figura 6.2 ilustra el\nproceso o trayectoria de la solución de la serie de tiempo. Finalmente,\nlas Figuras 6.3 y 6.3 muestran el\ncorrelograma calculado considerando una función de autocorrelación\naplicada al porceso real y una función de autocorrelación aplicada al\nproceso teórico, respectivamente.Recordemos que una trayectoria de equilibrio o solución de un \\(AR(1)\\) es\ncomo se muestra en la ecuación (6.6). Así, nuestra serie\nsimulada cumple con la característica de que los errores son más\nrelevantes cuando la serie es corta. Por el contrario, los errores son\nmenos relevantes, cuando la serie es muy larga. La Figura\n6.6 ilustra esta observación de la trayectoria de\nequilibrio.","code":"\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(latex2exp)\nsource(\"funciones/arroots.R\")\nsource(\"funciones/plot.armaroots.R\")\nsource(\"funciones/maroots.R\")\n\n# Parametros:\na0 <- 5; a1 <- 0.9; X_0 <- (a0/(1 - a1)); T <- 1000\n# Definimos un data frame para almacenar el proceso, agregamos una columna para el tiempo\nX_t <- data.frame(Tiempo = c(0:T))\n\n#  Parte estocastica de la serie de tiempo:\nset.seed(12345)\n\n# Agregamos un término estocástico al data frame\nX_t$U_t <- rnorm(T+1, mean = 0, sd = 4)\n# Agregamos columnas con NA's para un proceso teorico y uno real\nX_t$X_t <- NA\nX_t$XR_t <- NA\n\n# La serie teórica inicia en un valor inicial X_0\nX_t$X_t[1] <- X_0\n\n# La serie real inicia en un valor inicial X_0\nX_t$XR_t[1] <- X_0\n\n# Agregamos una columna para la función de Autocorrelación teórica:\nX_t$rho <-NA\n\nfor (i in 2:(T + 1)) {\n  # Real:\n  X_t$XR_t[i] = a0 + a1*X_t$XR_t[i-1] + X_t$U_t[i-1]\n  \n  # Teórico:\n  X_t$X_t[i] = X_t$X_t[i-1] + (a1^(i-1))*X_t$U_t[i-1]\n  \n  # Autocorrelación:\n  X_t$rho[i-1] = a1^(i-1)\n}\n\nhead(X_t)\n#>   Tiempo        U_t      X_t     XR_t      rho\n#> 1      0  2.3421153 50.00000 50.00000 0.900000\n#> 2      1  2.8378641 52.10790 52.34212 0.810000\n#> 3      2 -0.4372133 54.40657 54.94577 0.729000\n#> 4      3 -1.8139887 54.08785 54.01398 0.656100\n#> 5      4  2.4235498 52.89769 51.79859 0.590490\n#> 6      5 -7.2718239 54.32877 54.04228 0.531441\n\nggplot(data = X_t, aes(x = Tiempo, y = X_t)) + \n  geom_line(size = 0.5, color = \"#0F531C\") +\n  theme_light() + \n  xlab(\"Tiempo\") + \n  ylab(TeX(\"$X_t$\")) + \n  theme(plot.title = element_text(size = 11, face = \"bold\", hjust = 0)) + \n  theme(plot.subtitle = element_text(size = 10, hjust = 0)) + \n  theme(plot.caption = element_text(size = 10, hjust = 0)) +\n  theme(plot.margin = unit(c(1,1,1,1), \"cm\")) +\n  labs(\n    title = \"Comportamiento del Proceso Teórico\",\n    subtitle = \"Con un error con Distribución Normal (media = 0, desviación estándar = 4)\",\n    caption = \"Fuente: Elaboración propia.\"\n  )\n\nacf(X_t$XR_t, lag.max = 30, col = \"blue\", \n    ylab = \"Autocorrelacion\",\n    xlab=\"Rezagos\", \n    main=\"Funcion de Autocorrelacion Real\")\n\nbarplot(X_t$rho[1:30], names.arg = c(1:30), col = \"blue\", border=\"blue\", density = c(10,20), \n        ylab = \"Autocorrelacion\", \n        xlab=\"Rezagos\", \n        main=\"Funcion de Autocorrelacion Teórica\")\n\nacf(X_t$XR_t, lag.max = 30, col = \"blue\", \n    ylab = \"Autocorrelacion\",\n    xlab=\"Rezagos\", \n    main=\"Funcion de Autocorrelacion Real\")\n\nggplot(data = X_t, aes(x = Tiempo)) +\n  geom_line(aes(y = XR_t), size = 0.5, color = \"darkred\") +\n  geom_line(aes(y = X_t), size = 1, color = \"#0F531C\") +\n  theme_bw() + \n  xlab(\"Tiempo\") + \n  ylab(TeX(\"$X_t$\")) + \n  theme(plot.title = element_text(size = 11, face = \"bold\", hjust = 0)) + \n  theme(plot.subtitle = element_text(size = 10, hjust = 0)) + \n  theme(plot.caption = element_text(size = 10, hjust = 0)) +\n  theme(plot.margin = unit(c(1,1,1,1), \"cm\")) +\n  labs(\n    title = \"Comportamiento de los Procesos Real y Teórico\",\n    subtitle = \"Con un error con Distribución Normal (media = 0, desviación estándar = 4)\",\n    caption = \"Fuente: Elaboración propia.\"\n  )"},{"path":"procesos-estacionarios-univariados.html","id":"ar2","chapter":"6 Procesos estacionarios univariados","heading":"6.1.2 AR(2)","text":"Una vez analizado el caso de \\(AR(1)\\) analizaremos el caso del \\(AR(2)\\).\nLa ecuación generalizada del proceso autoregresivo de orden 2 (denotado\ncomo \\(AR(2)\\)) puede ser escrito como:\\[\\begin{equation}\n    X_t = a_0 + a_1 X_{t-1} + a_2 X_{t-2} + U_t\n    \\tag{6.11}\n\\end{equation}\\]Donde \\(U_t\\) denota un proceso puramente aleatorio con media cero (\\(0\\)), varianza constante (\\(\\sigma^2\\)) y autocovarianza cero (\\(Cov(U_t, U_s) = 0\\), con \\(t \\neq s\\)), y un parametro \\(a_2 \\neq 0\\). Así, utilizando el operador rezago podemos reescribir la ecuación (6.11) como:\n\\[\\begin{eqnarray*}\n    X_t - a_1 X_{t-1} - a_2 X_{t-2} & = & a_0 + U_t \\\\\n    (1 - a_1 L^1 - a_2 L^2) X_t & = & a_0 + U_t\n\\end{eqnarray*}\\]Donde, vamos denotar \\(\\alpha (L) = (1 - a_1 L^1 - a_2 L^2)\\), y lo llamaremos como un polinomio que depende del operador rezago y que es distinto de cero. De esta forma podemos reescribir la ecuación (6.11) como:\n\\[\\begin{equation}\n    \\alpha(L) X_t = a_0 + U_t\n    \\tag{6.12}\n\\end{equation}\\]Ahora, supongamos que existe el inverso multiplicativo del polinomio \\(\\alpha(L)\\), el cual será denotado como: \\(\\alpha^{-1}(L)\\) y cumple con que:\n\\[\\begin{equation}\n    \\alpha^{-1}(L) \\alpha(L) = 1   \n      \\tag{6.13}\n\\end{equation}\\]Así, podemos escribir la solución la ecuación (6.11) como:\n\\[\\begin{equation}\n    X_t = \\alpha^{-1}(L) \\delta + \\alpha^{-1}(L) U_t\n\\end{equation}\\]Si utilizamos el hecho que \\(\\alpha^{-1}(L)\\) se puede descomponer través del procedimiento de Wold en un polinomio de forma similar el caso de \\(AR(1)\\), tenemos que:\n\\[\\begin{equation}\n    \\alpha^{-1}(L) = \\psi_0 + \\psi_1 L + \\psi_2 L^2 + \\ldots\n      \\tag{6.14}\n\\end{equation}\\]Por lo tanto, el inverso multiplicativo \\(\\alpha^{-1}(L)\\) se puede ver como:\n\\[\\begin{equation}\n    1 = (1 - a_1 L^1 - a_2 L^2) (\\psi_0 + \\psi_1 L + \\psi_2 L^2 + \\ldots)\n      \\tag{6.15}\n\\end{equation}\\]Desarrollando la ecuación (6.15) tenemos la sigueinte expresión:\\[\\begin{eqnarray}\n    1 & = & \\psi_0 & + & \\psi_1 L & + & \\psi_2 L^2 & + & \\psi_3 L^3 & + & \\ldots \\\\\n     &  &  & - & a_1 \\psi_0 L & - & a_1 \\psi_1 L^2 & - & a_1 \\psi_2 L^3 & - & \\ldots \\\\\n    & &  &  &  & - & a_2 \\psi_0 L^2  & - & a_2 \\psi_1 L^3 & - & \\ldots\n\\end{eqnarray}\\]Ahora, podemos agrupar todos los términos en función del exponente asociado al operador rezago \\(L\\). La siguiente es una solución partícular y es una de las múltiples que podrían existir que cumpla con la ecuación (6.15). Sin embargo, para efectos del análisis sólo necesitamos una de esas soluciones. Utilizaremos las siguientes condiciones que deben cumplirse en una de las posibles soluciones:\\[\\begin{eqnarray}\n    L^0 : &   & \\Rightarrow & \\psi_0 = 1 \\\\\n    L : & \\psi_1 - a_1 \\psi_0 = 0 & \\Rightarrow & \\psi_1 = a_1$ \\\\\n    L^2 : & \\psi_2 - a_1 \\psi_1 - a_2 \\psi_0 = 0 & \\Rightarrow & \\psi_2 = ^2_1 + a_2 \\\\\n    L^3 : & \\psi_3 - a_1 \\psi_2 - a_2 \\psi_1 = 0 & \\Rightarrow & \\psi_3 = ^3_1 + 2 a_1 a_2$ \\\\\n    \\vdots & \\vdots & \\vdots & \\vdots\n\\end{eqnarray}\\]De esta forma podemos observar que en el límite siempre obtendremos una ecuación del tipo \\(\\psi_j - a_1 \\psi_{j-1} - a_2 \\psi_{j-2} = 0\\) asociada cada uno de los casos en que exista un \\(L^j\\), donde \\(j \\neq 0, 1\\), y la cual siempre podremos resolver conociendo que las condiciones iniciales son: \\(\\psi_0 = 1\\) y \\(\\psi_1 = a_1\\).Así, de las relaciones antes mencionadas y considerando que \\(\\alpha^{-1} (L)\\) aplicada una constante como \\(a_0\\), tendrá como resultado otra constante. De esta forma podemos escribir que la solución del proceso AR(2) en la ecuación (6.11) será dada por una expresión como sigue:\n\\[\\begin{equation}\n    X_t = \\frac{\\delta}{1 - a_1 - a_2} + \\sum^{\\infty}_{j = 0} \\psi_{t - j} U_{t - j}\n    \\tag{6.16}\n\\end{equation}\\]Donde todos los parametros \\(\\psi_i\\) está determinado por los parámtros \\(a_0\\), \\(a_1\\) y \\(a_2\\). En particular, \\(\\psi_0 = 1\\) y \\(\\psi_1 = a_1\\) como describimos anteriormente. Al igual que en el caso del \\(AR(1)\\), en la ecuación (6.16) las condiciones de estabilidad estarán dadas por las soluciones del siguiente polinomio característico:3\n\\[\\begin{equation}\n    \\lambda^2 - \\lambda a_1 - a_2 = 0\n    \\tag{6.17}\n\\end{equation}\\]Así, la condición de estabilidad de la trayectoria es que \\(\\lvert\\lambda_i\\lvert < 1\\), para \\(= 1, 2\\). Es decir, es necesario que cada una de las raíces sea, en valor absoluto, siempre menor que la unidad. Estas son las condiciones de estabilidad para el proceso \\(AR(2)\\).Finalmente, al igual que en un \\(AR(1)\\), continuación determinamos los momentos de una serie que sigue un proceso \\(AR(2)\\). Iniciamos con la determinación de la media de la serie:\n\\[\\begin{equation}\n    \\mathbb{E}[X_t] = \\mu = \\frac{a_0}{1 - a_1 - a_2}\n    \\tag{6.18}\n\\end{equation}\\]Lo anterior es cierto puesto que \\(\\mathbb{E}[U_{t - }] = 0\\), para todo \\(= 0, 1, 2, \\ldots\\). Para determinar la varianza utilizaremos las siguientes relaciones basadas en el uso del valor esperado, varianza y covarianza de la serie. Adicionalmente, para simplificar el trabajo asumamos que \\(a_0 = 0\\), lo cual implica que \\(\\mu = 0\\). Dicho lo anterior, partamos de:\n\\[\\begin{eqnarray}\n    \\mathbb{E}[X_t X_{t - \\tau}] & = & \\mathbb{E}[(a_1 X_{t-1} + a_2 X_{t-2} + U_t) X_{t - \\tau}]\\\\\n    & = & a_1 \\mathbb{E}[X_{t - 1} X_{t - \\tau}] + a_2 \\mathbb{E}[X_{t - 2} X_{t - \\tau}] + \\mathbb{E}[U_{t} X_{t - \\tau}]\n    \\tag{6.19}\n\\end{eqnarray}\\]Donde \\(\\tau = 0, 1, 2, 3, \\ldots\\) y que \\(\\mathbb{E}[U_{t} X_{t - \\tau}] = 0\\) para todo \\(\\tau \\neq 0\\).4 Dicho esto, podemos derivar el valor del valor esperado para diferentes valores de \\(\\tau\\):\\[\\begin{eqnarray}\n    \\tau = 0: & \\gamma(0) & = & \\alpha_1 \\gamma(1) + \\alpha_2 \\gamma(2) + \\sigma^2 \\\\\n    \\tau = 1: & \\gamma(1) & = & \\alpha_1 \\gamma(0) + \\alpha_2 \\gamma(1) \\\\\n    \\tau = 2: & \\gamma(2) & = & \\alpha_1 \\gamma(1) + \\alpha_2 \\gamma(0) \\\\\n    \\vdots & \\vdots & \\vdots & \\vdots\n\\end{eqnarray}\\]Donde debe ser claro que \\(\\mathbb{E}[(X_{t} - \\mu)(X_{t - \\tau} - \\mu)] = \\mathbb{E}[X_{t} X_{t - \\tau}] = \\gamma(\\tau)\\). Así, en general cuando \\(\\tau \\neq 0\\):\n\\[\\begin{equation}\n    \\gamma(\\tau) = a_1 \\gamma(\\tau - 1) + a_2 \\gamma(\\tau - 2)\n\\end{equation}\\]Realizando la sustitución recursiva y solucionando el sistema respectivo obtenemos que las varianza y covarianzas estaran determinadas por:\n\\[\\begin{equation}\n    Var[X_t] = \\gamma(0) = \\frac{1 - a_2}{(1 + a_2)[(1 - a_2)^2 - ^2_1]} \\sigma^2\n    \\tag{6.20}\n\\end{equation}\\]\\[\\begin{equation}\n    \\gamma(1) = \\frac{a_1}{(1 + a_2)[(1 - a_2)^2 - ^2_1]} \\sigma^2\n    \\tag{6.21}\n\\end{equation}\\]\\[\\begin{equation}\n    \\gamma(2) = \\frac{^2_1 + a_2 - ^2_2}{(1 + a_2)[(1 - a_2)^2 - ^2_1]} \\sigma^2\n    \\tag{6.22}\n\\end{equation}\\]Recordemos que las funciones de autocorrelación se obtienen de la división de cada unas de las funciones de covarianza (\\(\\gamma(\\tau)\\)) por la varianza (\\(\\gamma(0)\\)). Así, podemos construir la siguiente expresión:\n\\[\\begin{equation}\n    \\rho(\\tau) - a_1 \\rho(\\tau - 1) - a_2 \\rho(\\tau - 2) = 0\n    \\tag{6.23}\n\\end{equation}\\]Para el segundo ejemplo consideremos una aplicación una serie de\ntiempo en especifico:","code":""},{"path":"procesos-estacionarios-univariados.html","id":"ejemplo-ar2","chapter":"6 Procesos estacionarios univariados","heading":"6.1.2.1 EJEMPLO AR(2)","text":"Recordando el tema pasado y la serie en la que evaluamos los cambios de\nprecio del ACTIVO AMZN como si fueran retornos:\nFigure 5.5: Serie de tiempo de los precios de apertura de año en los últimos 20 años\nPrimero que nada es importante cargar los datos un objeto series de\ntiempo. Esto nos lo permite la función ts(). Además debemos serciorarnos\nde que los datos esten en orden cronológico.Dado que queremos saber si existe un proceso \\(AR(2)\\) en estos cambio\ndebemos calcularlo. Para ello utilizamos la función \\(lm()\\) que realizará una regresión lineal y veremos la relación de los valores con sus valores pasados en \\(t-2\\):Veamos la tabla de la regresión lineal:\nAR(2) de los precios de apertura de AMZN\nLa tabla anterior claramente indica que hay una relación entre el valor del precio y sus valores anteriores en un proceso AR(2).Así pues es importante modificar la serie de tiempo para ilustrar como se puede controlar los efectos de los autoregresores \\(AR(2)\\). Para ello, utilizaremos la función \\(arima()\\). En “order” tenemos un vector \\(c(p,d,q)\\) que corresponde \\(p\\) el grado de AR, \\(d\\) el grado de diferención y \\(q\\) el grado de MA que utilizaremos. El valor \\(q\\) quedaráá en \\(0\\) por ahora pero será analizado más adelante.Veamos si las raices inversas mantienen la estabilidad al ser menores 1.\nFigure 6.7: Raices AR(2) Inversas de la serie de tiempo\nClaramente se puede en la Figura 6.7 ver que los valores de las raices inversas están dentro del circulo unitario y, por consiguiente son menores 1. Ahora veamos como se ve la estimación ajustada AR(2) con el plot original.\nFigure 6.8: Diferencia entre la serie de tiempo original de precios de AMZN y su AR(2)\nConsecuentemente en la Figura 6.8 es posible ver la manera en la que se suaviza un poco la línea lo cual debe ayudarnos hacer una mejor estimación. Ahora veamos \\(AR(p)\\).","code":"\n#install.packages(\"pacman\")\n#pacman nos permite cargar varias librerias en una sola línea\nlibrary(pacman)\npacman::p_load(tidyverse,BatchGetSymbols,ggplot2,lubridate,readxl,forecast,stats,stargazer)\n#Primero determinamos el lapso de tiempo\npd<-as.Date(\"2002/9/30\") #primer fecha\npd\n#> [1] \"2002-09-30\"\n#> [1] \"2021-09-18\"\nld<- as.Date(\"2021/09/30\")#última fecha\nld\n#> [1] \"2021-09-30\"\n#Intervalos de tiempo\nint<-\"monthly\"\n\n#Datos a elegir\ndt<-c(\"AMZN\")\n\n#Descargando los valores\ndata1<- BatchGetSymbols(tickers = dt,\n                       first.date = pd,\n                       last.date = ld,\n                       freq.data = int,\n                       do.cache = FALSE,\n                       thresh.bad.data = 0)\n\n#Generando data frame con los valores\ndata_precio_amzn<-data1$df.tickers\ncolnames(data_precio_amzn)\n#>  [1] \"ticker\"              \"ref.date\"           \n#>  [3] \"volume\"              \"price.open\"         \n#>  [5] \"price.high\"          \"price.low\"          \n#>  [7] \"price.close\"         \"price.adjusted\"     \n#>  [9] \"ret.adjusted.prices\" \"ret.closing.prices\"\nret_20_amazn<-ggplot(data=data_precio_amzn, aes(x=ref.date))+\n  geom_line(aes(y=price.open))+\n  labs(title=\"Precios de apertura de AMZN en los últimos 20 años\",y=\"Retornos\", x=\"Año\")+\n  theme_light()\nret_20_amazn\ndata_precio_amzn<-data_precio_amzn[order(data_precio_amzn$ref.date),]\nhead(data_precio_amzn)#dado que ya estaba en orden cronológico nuestro df no cambia\n#> # A tibble: 6 × 10\n#>   ticker ref.date     volume price…¹ price…² price…³ price…⁴\n#>   <chr>  <date>        <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n#> 1 AMZN   2002-09-30   1.85e8   0.832   0.842   0.792   0.796\n#> 2 AMZN   2002-10-01   4.07e9   0.812   1.01    0.800   0.968\n#> 3 AMZN   2002-11-01   4.13e9   0.961   1.23    0.91    1.17 \n#> 4 AMZN   2002-12-02   3.11e9   1.21    1.25    0.922   0.944\n#> 5 AMZN   2003-01-02   3.38e9   0.960   1.16    0.928   1.09 \n#> 6 AMZN   2003-02-03   2.32e9   1.10    1.12    0.980   1.10 \n#> # … with 3 more variables: price.adjusted <dbl>,\n#> #   ret.adjusted.prices <dbl>, ret.closing.prices <dbl>,\n#> #   and abbreviated variable names ¹​price.open,\n#> #   ²​price.high, ³​price.low, ⁴​price.close\n#hagamos el objeto ts\nprice_amazn_ts<-ts(data_precio_amzn$price.open, frequency = 12)\nplot(price_amazn_ts)#de esta manera podemos ver que se cargo bien debido a que es igual al ggplot\npriceopen<-data_precio_amzn$price.open\npriceopen_amazn<-data.matrix(priceopen)\nlags<-function(mat,p){\n  for(i in 1:p){\n    uno<-mat[,1]\n    mat<-cbind(mat,lag(uno,i))}\n  as.data.frame(mat)\n}\n\ndf.lags<-lags(priceopen_amazn,2)\nar2_amazn<-lm(V1~., data=df.lags)\nAR_price_amazn_ts<-arima(price_amazn_ts,order=c(2,0,0),method = \"ML\")\nAR_price_amazn_pl<-Arima(price_amazn_ts,order=c(2,0,0),method = \"ML\")\nautoplot(AR_price_amazn_ts)+theme_light()\nplot(AR_price_amazn_pl$x,col=\"black\", main = \"Diferencia entre la serie de tiempo original y AR(2)\",xlab=\"Tiempo\",ylab=\"Precio\")+lines(fitted(AR_price_amazn_pl),col=\"blue\")#> integer(0)"},{"path":"procesos-estacionarios-univariados.html","id":"arp","chapter":"6 Procesos estacionarios univariados","heading":"6.1.3 AR(p)","text":"Veremos ahora una generalización de los procesos autoregresivos (AR). Esta generalización es conocida como un proceso \\(AR(p)\\) y que puede ser descrito por la siguiente ecuación en diferencia estocástica:\n\\[\\begin{equation}\n    X_t = a_0 + a_1 X_{t-1} + a_2 X_{t-2} + a_3 X_{t-3} + \\ldots + a_p X_{t-p} + U_t\n    \\tag{6.24}\n\\end{equation}\\]Donde \\(a_p \\neq 0\\), y \\(U_t\\) es un proceso puramente aleatorio con media cero (0), varianza constante (\\(\\sigma^2\\)) y covarianza cero (0). Usando el operador rezago, \\(L^k\\), para \\(k = 0, 1, 2, \\ldots, p\\), obtenemos la siguiente expresión de la ecuación (6.24):\n\\[\\begin{equation}\n    (1 - a_1 L - a_2 L^2 - a_3 L^3 - \\ldots - a_p L^p) X_t = a_0 + U_t\n    \\tag{6.25}\n\\end{equation}\\]Definamos el polinomio \\(\\alpha(L)\\) como:\n\\[\\begin{equation}\n    \\alpha(L) = 1 - a_1 L - a_2 L^2 - a_3 L^3 - \\ldots - a_p L^p\n    \\tag{6.26}\n\\end{equation}\\]De forma similar que en los procesos \\(AR(1)\\) y \\(AR(2)\\), las condiciones de estabilidad del proceso \\(AR(p)\\) estarán dadas por la solución de la ecuación característica:\n\\[\\begin{equation}\n    \\lambda^p - a_1 \\lambda^{p-1} - a_2 \\lambda^{p-2} - a_3 \\lambda^{p-3} - \\ldots - a_p = 0\n        \\tag{6.27}\n\\end{equation}\\]Así, solo si el polinomio anterior tiene raíces cuyo valor absoluto sea menor uno (\\(\\lvert\\lambda_i\\lvert < 1\\)) y si \\(1 - a_1 L - a_2 L^2 - a_3 L^3 - \\ldots - a_p L^p < 1\\) podremos decir que el proceso es convergente y estable. Lo anterior significa que la ecuación (6.26) puede expresarse en términos de la descomposición de Wold o como la suma infinita de términos como:\n\\[\\begin{equation}\n    \\frac{1}{1 - a_1 L  - a_2 L^2 - a_3 L^3  - \\ldots - a_p L^p} = \\psi_0 + \\psi_1 L + \\psi_2 L^2 + \\psi_3 L^3 + \\ldots\n\\tag{6.28}\n\\end{equation}\\]Donde, por construcción de \\(\\alpha(L) \\alpha^{-1}(L) = 1\\) implica que \\(\\psi_0 = 1\\). De forma similar los proceso AR(1) y AR(2), es posible determinar el valor de los coefieentes \\(\\psi_j\\) en términos de los coefientes \\(a_i\\). Así, la solución del proceso \\(AR(p)\\) estará dada por:\n\\[\\begin{equation}\n    X_t = \\frac{a_0}{1 - a_1  - a_2 - a_3  - \\ldots - a_p} + \\sum^{\\infty}_{j = 0} \\psi_j U_{t-j}\n    \\tag{6.29}\n\\end{equation}\\]Considerando la solución de la ecuación (6.24) expresada en la ecuación (6.29) podemos determinar los momentos del proceso y que estarán dados por una media como:\n\\[\\begin{equation}\n    \\mathbb{E}[X_t] = \\mu = \\frac{a_o}{1 - a_1  - a_2 - a_3  - \\ldots - a_p}\n        \\tag{6.30}\n\\end{equation}\\]Lo anterior, considerado que \\(\\mathbb{E}[U_t] = 0\\), para todo \\(t\\). Para determinar la varianza del proceso, sin pérdida de generalidad, podemos definir una ecuación: \\(\\gamma(\\tau) = \\mathbb{E}[X_{t - \\tau} X_t]\\), la cual (omitiendo la constante, ya que la correlación de una constante con cuaquier variable aleatoria que depende del tiempo es cero (0)) puede ser escrita como:\n\\[\\begin{equation}\n    \\gamma(\\tau) = \\mathbb{E}[(X_{t - \\tau}) \\cdot (a_1 X_{t-1} + a_2 X_{t-2} + a_3 X_{t-3} + \\ldots + + a_p X_{t-p} + U_t)]\n        \\tag{6.31}\n\\end{equation}\\]Donde \\(\\tau = 0, 1, 2, \\ldots, p\\) y \\(a_0 = 0\\), lo que implica que \\(\\mu = 0\\). De lo anterior obtenemos el siguiente conjunto de ecuaciones mediante sustituciones de los valores de \\(\\tau\\):\n\\[\\begin{eqnarray}\n    \\gamma(0) & = & a_1 \\gamma(1) + a_2 \\gamma(2) + \\ldots + a_p \\gamma(p) + \\sigma^2 \\nonumber \\\\\n    \\gamma(1) & = & a_1 \\gamma(0) + a_2 \\gamma(1) + \\ldots + a_p \\gamma(p-1) \\nonumber \\\\\n    \\vdots \\nonumber \\\\\n    \\gamma(p) & = & a_1 \\gamma(p-1) + a_2 \\gamma(p-2) + \\ldots + a_p \\gamma(0) \\nonumber\n\\end{eqnarray}\\]De esta forma, es fácil observar que la ecuación general para \\(p > 0\\) estará dada por:\n\\[\\begin{equation}\n    \\gamma(p) - a_1 \\gamma(\\tau - 1) - a_2 \\gamma(\\tau - 2) - \\ldots - a_p \\gamma(\\tau - p) = 0\n    \\tag{6.32}\n\\end{equation}\\]Dividiendo la ecuación (6.32) por \\(\\gamma(0)\\), se obtiene la siguiente ecuación:\n\\[\\begin{equation}\n    \\rho(p) - a_1 \\rho(\\tau - 1) + a_2 \\rho(\\tau - 2) + \\ldots + a_p \\rho(\\tau - p) = 0\n        \\tag{6.33}\n\\end{equation}\\]Así, podemos escribir el siguiente sistema de ecuaciones:\n\\[\\begin{eqnarray}\n    \\rho(1) & = & a_1 + a_2 \\rho(1) + a_3 \\rho(2) + \\ldots + a_p \\rho(p-1) \\nonumber \\\\\n    \\rho(2) & = & a_1 \\rho(1) + a_2 + a_3 \\rho(1) + \\ldots + a_p \\rho(p-2) \\nonumber \\\\\n    & \\vdots & \\nonumber \\\\\n    \\rho(p) & = & a_1 \\rho(p-1) + a_2 \\rho(p-2) + \\ldots + a_p \\nonumber\n\\end{eqnarray}\\]Lo anterior se puede expresar como un conjunto de vectores y matrices de la siguiente forma:\n\\[\\begin{equation}\n    \\left[\n    \\begin{array}{c}\n        \\rho(1) \\\\\n        \\rho(2) \\\\\n        \\vdots \\\\\n        \\rho(p)\n    \\end{array}\n    \\right]\n    =\n    \\left[\n    \\begin{array}{c c c c}\n        1 & \\rho(1) & \\ldots & \\rho(p - 1) \\\\\n        \\rho(1) & 1 & \\ldots & \\rho(p - 2) \\\\\n        \\rho(2) & \\rho(1) & \\ldots & \\rho(p - 3) \\\\\n        \\vdots & \\vdots & \\ldots & \\vdots \\\\\n        \\rho(p - 1) & \\rho(p - 2) & \\ldots & 1 \\\\\n    \\end{array}\n    \\right]\n    \\left[\n    \\begin{array}{c}\n        a_1 \\\\\n        a_2 \\\\\n        a_3 \\\\\n        \\vdots \\\\\n        a_p \\\\\n    \\end{array}\n    \\right]\n\\tag{6.34}\n\\end{equation}\\]De lo anterior podemos escribir la siguiente ecuación que es una forma alternativa para expresar los valores de los coefientes \\(a_i\\) de la la solución del proceso \\(AR(p)\\):\n\\[\\begin{equation}\n    \\mathbf{\\rho} = \\mathbf{R} \\mathbf{}\n  \\tag{6.35}\n\\end{equation}\\]Es decir, podemos obtener la siguiente expresión:\n\\[\\begin{equation}\n    \\mathbf{} = \\mathbf{R}^{-1} \\mathbf{\\rho}\n    \\tag{6.36}\n\\end{equation}\\]","code":""},{"path":"procesos-estacionarios-univariados.html","id":"ejemplo-arp","chapter":"6 Procesos estacionarios univariados","heading":"6.1.3.1 Ejemplo AR(p)","text":"Veamos primero la función de autocorrelacion y el plot de los lags.\nFigure 6.9: Función de Autocorrelación parcial de la serie de tiempo de AMZN\n\nFigure 6.10: Función de Autocorrelación de la serie de tiempo de AMZN\n\nFigure 6.11: 5 plots de correlacion de los lags de la serie de tiempo de AMZN\nDado que se puede ver en las Figuras 6.10, 6.9 y 6.11 que los valores de lag=1 tienen correlación resulta relevante hacer un AR(1), que corresponde 1 años. Así pues:AR:Regresión lineal:\nAR(1) de los precios de apertura de AMZN\nRaices:\nFigure 6.12: Raices AR(1) Inversas de la serie de tiempo\nPlot:\nFigure 6.13: Diferencia entre la serie de tiempo original de precios de AMZN y su AR(1)\n","code":"\npacf(price_amazn_ts, main=\"\", lag.max = 60)\nacf(price_amazn_ts, main=\"\", lag.max = 60)\nlag.plot(price_amazn_ts, main=\"\", lags=5)\nAR_price_amazn_ts_1<-arima(price_amazn_ts,order=c(1,0,0),method = \"ML\")\nAR_price_amazn_pl_1<-Arima(price_amazn_ts,order=c(1,0,0),method = \"ML\")\ndf.lags.1<-lags(priceopen_amazn,1)\nar20_amazn<-lm(V1~., data=df.lags.1)\nautoplot(AR_price_amazn_ts_1)+theme_light()\nplot(AR_price_amazn_pl_1$x,col=\"black\", main = \"Diferencia entre la serie de tiempo original y AR(1)\",xlab=\"Tiempo\",ylab=\"Precio\")+lines(fitted(AR_price_amazn_pl_1),col=\"red\")#> integer(0)"},{"path":"procesos-estacionarios-univariados.html","id":"función-de-autocorrelación-parcial","chapter":"6 Procesos estacionarios univariados","heading":"6.2 Función de Autocorrelación Parcial","text":"Ahora introduciremos el concepto de Función de Autocorrelación Parcial (PACF, por sus siglas en inglés). Primero, dadas las condiciones de estabilidad y de convergencia, si suponemos que un proceso AR, MA, ARMA o ARIMA tienen toda la información de los rezagos de la serie en conjunto y toda la información de los promedio móviles del término de error, resulta importante construir una métrica para distinguir el efecto de \\(X_{t - \\tau}\\) o el efecto de \\(U_{t - \\tau}\\) (para cualquier \\(\\tau\\)) sobre \\(X_t\\) de forma individual.La idea es construir una métrica de la correlación que existe entre las diferentes varibles aleatorias, si para tal efecto se ha controlado el efecto del resto de la información. Así, podemos definir la ecuación que puede responder este planteamiento como:\n\\[\\begin{equation}\n    X_t = \\phi_{k1} X_{t-1} + \\phi_{k2} X_{t-2} + \\ldots + \\phi_{kk} X_{t-k} + U_t\n    \\tag{6.37}\n\\end{equation}\\]Donde \\(\\phi_{ki}\\) es el coeficiente de la variable dada con el rezago \\(\\) si el proceso tiene un órden \\(k\\). Así, los coeficientes \\(\\phi_{kk}\\) son los coeficientes de la autocorrelación parcial (considerando un proceso AR(k)). Observemos que la autocorrelaicón parcial mide la correlación entre \\(X_t\\) y \\(X_{t-k}\\) que se mantiene cuando el efecto de las variables \\(X_{t-1}\\), \\(X_{t-2}\\), \\(\\ldots\\) y \\(X_{t-k-1}\\) en \\(X_{t}\\) y \\(X_{t-k}\\) ha sido eliminado.Dada la expresión considerada en la ecuación (6.37), podemos resolver el problema de establecer el valor de cada \\(\\phi_{ki}\\) mediante la solución del sistema que se representa en lo siguiente:Table 6.1:  Relación entre la Función de autocorrelación y la Función de autocorrelación parcial de una serie \\(X_t\\).\\[\\begin{equation}\n    \\left[\n    \\begin{array}{c}\n        \\rho(1) \\\\\n        \\rho(2) \\\\\n        \\vdots \\\\\n        \\rho(k)\n    \\end{array}\n    \\right]\n    =\n    \\left[\n    \\begin{array}{c c c c}\n        1 & \\rho(1) & \\ldots & \\rho(k - 1)\\\\\n        \\rho(1) & 1 & \\ldots & \\rho(k - 2)\\\\\n        \\rho(2) & \\rho(1) & \\ldots & \\rho(k - 3)\\\\\n        \\vdots & \\vdots & \\ldots & \\vdots\\\\\n        \\rho(k - 1) & \\rho(k - 2) & \\ldots & 1\\\\\n    \\end{array}\n    \\right]\n    \\left[\n    \\begin{array}{c}\n        \\phi_{k1} \\\\\n        \\phi_{k2} \\\\\n        \\phi_{k3} \\\\\n        \\vdots \\\\\n        \\phi_{kk} \\\\\n    \\end{array}\n    \\right]\n    \\tag{6.38}    \n\\end{equation}\\]Del cual se puede derivar una solución, resoviendo por el método de cramer, o cualquier otro método que consideremos y que permita calcular la solución de sistemas de ecuaciones.Posterior al análisis analítico platearemos un enfoque para interpretar las funciones de autocorrelación y autocorrelación parcial. Este enfoque pretende aportar al principio de parcimonia, en el cual podemos identificar el número de parámetros que posiblemente puede describir mejor la serie en un modelo ARMA(p, q).En el Cuadro 6.1 se muestra un resumen de las caranterísticas que debemos observar para determinar el número de parámetros de cada uno de los componentes AR y MA. Lo anterior por observación de las funciones de autocorrelación y autocorrelación parcial. Este enfoque es el más formal, más adelante implemtaremos uno más formal y que puede ser más claro de cómo determinar el númeto de parámetros.","code":""},{"path":"procesos-estacionarios-univariados.html","id":"procesos-de-medias-móviles-ma","chapter":"6 Procesos estacionarios univariados","heading":"6.3 Procesos de Medias Móviles (MA)","text":"","code":""},{"path":"procesos-estacionarios-univariados.html","id":"ma1","chapter":"6 Procesos estacionarios univariados","heading":"6.3.1 MA(1)","text":"Una vez planteado el proceso generalizado de \\(AR(p)\\), iniciamos el planteamiento de los proceso de medias móviles, denotados como \\(MA(q)\\). Iniciemos con el planteamiento del proceso \\(MA(1)\\), que se puede escribir como una ecuación como la siguiente:\n\\[\\begin{equation}\n    X_t = \\mu + U_t - b_1 U_{t-1}\n    \\tag{6.39}\n\\end{equation}\\]O como:\n\\[\\begin{equation}\n    X_t - \\mu = (1 - b_1 L) U_{t}\n    \\tag{6.40}\n\\end{equation}\\]Donde \\(U_t\\) es un proceso puramente aleatorio, es decir, con \\(\\mathbb{E}[U_t] = 0\\), \\(Var[U_t] = \\sigma^2\\), y \\(Cov[U_t, U_s] = 0\\). Así, un proceso \\(MA(1)\\) puede verse como un proceso AR con una descomposición de Wold en la que \\(\\psi_0 = 1\\), \\(\\psi_1 = - b_1\\) y \\(\\psi_j = 0\\) para todo \\(j > 1\\).Al igual que los procesos autoregresivos, determinaremos los momentos de un proceso \\(MA(1)\\). En el caso de la media observamos que será:\n\\[\\begin{eqnarray}\n    \\mathbb{E}[X_t] & = & \\mu + \\mathbb{E}[U_t] - b_1 \\mathbb{E}[U_{t - 1}] \\nonumber \\\\\n    & = & \\mu\n    \\tag{6.41}\n\\end{eqnarray}\\]Por su parte la varianza estará dada por:\n\\[\\begin{eqnarray}\n    Var[X_t] & = & \\mathbb{E}[(X_t - \\mu)^2] \\nonumber \\\\\n    & = & \\mathbb{E}[(U_t - b_1 U_{t-1})^2] \\nonumber \\\\\n    & = & \\mathbb{E}[U_t^2 - 2 b_1 U_t U_{t-1} + b_1^2 U_{t - 1}^2] \\nonumber \\\\\n    & = &\\mathbb{E}[U_t^2] - 2 b_1 \\mathbb{E}[U_t U_{t-1}] + b_1^2 \\mathbb{E}[U_{t - 1}^2]] \\nonumber \\\\\n    & = & \\sigma^2 + b_1^2 \\sigma^2 \\nonumber \\\\\n    & = & (1 + b_1^2) \\sigma^2 = \\gamma(0)\n    \\tag{6.42}\n\\end{eqnarray}\\]De esta forma, la varianza del proceso es constante en cualquier periodo \\(t\\). Para determinar la covarianza utilizaremos la siguiente ecuación:\n\\[\\begin{eqnarray}\n    \\mathbb{E}[(x_t - \\mu)(x_{t + \\tau} - \\mu)] & = & \\mathbb{E}[(U_t - b_1 U_{t-1})(U_{t + \\tau} - b_1 U_{t + \\tau - 1})] \\nonumber \\\\\n    & = & \\mathbb{E}[U_t U_{t + \\tau} - b_1 U_t U_{t + \\tau - 1} - b_1 U_{t - 1} U_{t + \\tau} \\nonumber \\\\\n    &   & + b_1^2 U_{t - 1} U_{t + \\tau - 1}] \\nonumber \\\\\n    & = & \\mathbb{E}[U_t U_{t + \\tau}] - b_1 \\mathbb{E}[U_t U_{t + \\tau - 1}] \\nonumber \\\\\n    &   & - b_1 \\mathbb{E}[U_{t - 1} U_{t + \\tau}] + b_1^2 \\mathbb{E}[U_{t - 1} U_{t + \\tau - 1}]\n    \\tag{6.43}\n\\end{eqnarray}\\]Si hacemos sustituciones de diferentes valores de \\(\\tau\\) en la ecuación (6.43) notaremos que la covarianza será distinta de cero únicamente para el caso de \\(\\tau = 1, -1\\). En ambos casos tendremos como resultado:\n\\[\\begin{eqnarray}\n    \\mathbb{E}[(x_t - \\mu)(x_{t + 1} - \\mu)] & = & \\mathbb{E}[(x_t - \\mu)(x_{t - 1} - \\mu)] \\nonumber \\\\\n    & = & - b_1 \\mathbb{E}[U_t U_{t}] \\nonumber \\\\\n    & = & - b_1 \\mathbb{E}[U_{t - 1} U_{t - 1}] \\nonumber \\\\\n    & = & - b_1^2 \\sigma^2 = \\gamma(1)\n        \\tag{6.44}\n\\end{eqnarray}\\]De esta forma tendremos que las funciones de autocorrelación estarán dadas por los siguientes casos:\n\\[\\begin{eqnarray}\n    \\rho(0) & = & 1 \\nonumber \\\\\n    \\rho(1) & = & \\frac{- b_1}{1 + b_1^2} \\nonumber \\\\\n    \\rho(\\tau) & = & 0 \\text{ para todo } \\tau > 1 \\nonumber\n\\end{eqnarray}\\]Ahora regresando la ecuación (6.39),e su solución la podemos expresar como:\n\\[\\begin{eqnarray}\n    U_ t & = & - \\frac{\\mu}{1 - b_1} + \\frac{1}{1 - b_1 L} X_t \\nonumber \\\\\n    & = & - \\frac{\\mu}{1 - b_1} + X_t + b_1 X_{t-1} + b_1^2 X_{t-2} + \\ldots \\nonumber\n\\end{eqnarray}\\]Donde la condición para que se cumpla esta ecuación es que \\(\\lvert b_1 \\lvert< 1\\). La manera de interpretar esta condición es como una condición de estabilidad de la solución y cómo una condición de invertibilidad. Notemos que un \\(MA(1)\\) (y en general un \\(MA(q)\\)) es equivalente un \\(AR(\\infty)\\), es decir, cuando se invierte un MA se genera un AR con infinitos rezagos.En esta sección desarrollaremos un ejemplo, primero explicaremos en qué consiste una modelación del tipo \\(MA(q)\\) y después platearemos un ejemplo en concreto.","code":""},{"path":"procesos-estacionarios-univariados.html","id":"maq","chapter":"6 Procesos estacionarios univariados","heading":"6.3.2 MA(q)","text":"En general, el proceso de medias móviles de orden \\(q\\), \\(MA(q)\\), puede ser escrito como:\n\\[\\begin{equation}\n    X_t = \\mu + U_t - b_1 U_{t-1} - b_2 U_{t-2} - \\ldots - b_q U_{t-q}\n    \\tag{6.45}\n\\end{equation}\\]Podemos reescribir la ecuación (6.45) utilizando el operador rezago, así tendrémos el proceso de \\(MA(q)\\) como:\n\\[\\begin{eqnarray}\n    X_t - \\mu & = & (1 - b_1 L - b_2 L^2 - \\ldots - b_q L^q) U_{t} \\nonumber \\\\\n    X_t - \\mu & = & \\beta(L) U_t\n    \\tag{6.46}\n\\end{eqnarray}\\]Donde \\(U_t\\) es un proceso puramente aleatorio con \\(\\mathbb{E}[U_t] = 0\\), \\(Var[U_t] = \\mathbb{E}[U_t^2] = 0\\) y \\(Cov[U_t, U_s] = \\mathbb{E}[U_t, U_s] = 0\\), y \\(\\beta(L) = 1 - b_1 L - b_2 L^2 - \\ldots - b_q L^q\\) es un polinomio del operador rezago \\(L\\). la ecuación (6.46) puede ser interpretada como un proceso \\(AR(q)\\) sobre la serie \\(U_t\\).Ahora determinemos los momentos de un proceso \\(MA(q)\\):\n\\[\\begin{eqnarray}\n    \\mathbb{E}[X_t] & = & \\mathbb{E}[\\mu + U_t - b_1 U_{t-1} - b_2 U_{t-2} - \\ldots - b_q U_{t-q}] \\nonumber \\\\\n    & = & \\mu + \\mathbb{E}[U_t] - b_1 \\mathbb{E}[U_{t-1}] - b_2 \\mathbb{E}[U_{t-2}] - \\ldots - b_q \\mathbb{E}[U_{t-q}] \\nonumber \\\\\n    & = & \\mu\n    \\tag{6.47}\n\\end{eqnarray}\\]En el caso de la varianza tenemos que se puede expresar como:\n\\[\\begin{eqnarray}\n    Var[X_t] & = & \\mathbb{E}[(X_t - \\mu)^2] \\nonumber \\\\\n    & = & \\mathbb{E}[(U_t - b_1 U_{t-1} - b_2 U_{t-2} - \\ldots - b_q U_{t-q})^2] \\nonumber \\\\\n    & = & \\mathbb{E}[U_t^2 + b_1^2 U_{t-1}^2 + b_2^2 U_{t-2}^2 + \\ldots + b_q^2 U_{t-q}^2 \\nonumber \\\\\n    &   & - 2 b_1 U_t U_{t - 1} - \\ldots - 2 b_{q - 1} b_q U_{t - q + 1} U_{t - q}] \\nonumber \\\\\n    & = & \\mathbb{E}[U_t^2] + b_1^2 \\mathbb{E}[U_{t-1}^2] + b_2^2 \\mathbb{E}[U_{t-2}^2] + \\ldots + b_q^2 \\mathbb{E}[U_{t-q}^2] \\nonumber \\\\\n    &   & - 2 b_1 \\mathbb{E}[U_t U_{t - 1}] - \\ldots - 2 b_{q - 1} b_q \\mathbb{E}[U_{t - q + 1} U_{t - q}] \\nonumber \\\\\n    & = & \\sigma^2 + b^2_1 \\sigma^2 + b^2_2 \\sigma^2 + \\ldots + b^2_q \\sigma^2 \\nonumber \\\\\n    & = & (1 + b^2_1 + b^2_2 + \\ldots + b^2_q) \\sigma^2\n    \\tag{6.48}\n\\end{eqnarray}\\]En el caso de las covarianzas podemos utilizar una idea similar al caso del \\(AR(p)\\), construir una expresión general para cualquier rezago \\(\\tau\\):\n\\[\\begin{eqnarray}\n    Cov[X_t, X_{t + \\tau}] & = & \\mathbb{E}[(X_t - \\mu)(X_{t + \\tau} - \\mu)] \\nonumber \\\\\n    & = & \\mathbb{E}[(U_t - b_1 U_{t-1} - b_2 U_{t-2} - \\ldots - b_q U_{t-q}) \\nonumber \\\\\n    &   & (U_{t + \\tau} - b_1 U_{t + \\tau -1} - b_2 U_{t + \\tau -2} - \\ldots - b_q U_{t + \\tau - q})] \\nonumber\n\\end{eqnarray}\\]La expresión anterior se puede desarrollar para múltiples casos de \\(\\tau = 1, 2, \\ldots, q\\). De esta forma tenemos el siguiente sistema:\n\\[\\begin{eqnarray}\n    \\tau = 1 & : & \\gamma(1) = (- b_1 + b_1 b_2 + \\ldots + b_{q-1} b_q) \\sigma^2 \\nonumber \\\\\n    \\tau = 2 & : & \\gamma(2) = (- b_2 + b_1 b_3 + \\ldots + b_{q-2} b_q) \\sigma^2 \\nonumber \\\\\n    & \\vdots & \\nonumber \\\\\n    \\tau = q & : & \\gamma(q) = b_q \\sigma^2 \\nonumber\n\\end{eqnarray}\\]Donde \\(\\gamma(\\tau) = 0\\) para todo \\(\\tau > q\\). Es decir, todas las autocovarianzas y autocorrelaciones con ordenes superiores \\(q\\) son cero (0). De esta forma, esta caracterítica teórica permite identificar el orden de \\(MA(q)\\) visualizando la función de autocorrelación y verificando partir de cual valor de rezago la autocorrelación es significaiva.Regresando al problema original que es el de determinar una solución para la eucación @ref(eq:MAq_EQ), tenemos que dicha solución estará dada por un \\(AR(\\infty)\\) en términos de \\(U_t\\):\n\\[\\begin{eqnarray}\n    U_t & = & - \\frac{\\mu}{1 - b_1 - b_2 - \\ldots - b_q} + \\beta(L)^{-1} X_t \\nonumber \\\\\n    &   & - \\frac{\\mu}{1 - b_1 - b_2 - \\ldots - b_q} + \\sum_{j = 0}^{\\infty} c_j X_{t-j}\n    \\tag{6.49}\n\\end{eqnarray}\\]Donde se cumple que: \\(1 = (1 - b_1 L^1 - b_2 L^2 - \\ldots - b_q L^q)(1 - c_1 L - c_2 L^2 - \\ldots)\\) y los coeficientes \\(c_j\\) se pueden determinar por un método de coeficientes indeterminados y en términos de los valores \\(b_i\\). De igual forma que en el caso de la ecuación (6.24), en la ecuación (6.49) se deben cumplir condiciones de estabilidad asociadas con las raíces del polinomio carácterististico dado por:\n\\[\\begin{equation}\n    1 - b_1 x - b_2 x^2 - \\ldots b_q x^q = 0\n    \\tag{6.50}\n\\end{equation}\\]El cual debe cumplir que \\(\\lvert x_i \\lvert < 1\\) y que \\(1 - b_1 - b_2 - \\ldots b_q < 1\\).Ahora veamos el ejemplo.","code":""},{"path":"procesos-estacionarios-univariados.html","id":"ejemplo-maq","chapter":"6 Procesos estacionarios univariados","heading":"6.3.2.1 Ejemplo MA(q)","text":"Cuando nos fijamos en las figuras 6.9 y 6.10 es claro que hay un proceso MA(1). Es decir que picos de un año estan causando efectos en valores futuros. Así pues, escogemos un valor de \\(q=1\\).MA:Raices MA(1):\nFigure 6.14: Raices MA(1) Inversas de la serie de tiempo\nDado que el valor de la raiz inversa de MA(1) en la Figura 6.14 sabemos que tenemos una serie convergente y estable.Plot:\nFigure 6.15: Diferencia entre la serie de tiempo original de precios de AMZN y su AR(1)\nEn la Figura 6.15, es muy fácil ver que los efectos de los picos son muy bajos ahora, lo cual nos permitirá hacer mejores estimaciones. Evidentemente querremos hacer el proceso AR(p) y MA(q) simultaneamente para obtener los mejores resultados.","code":"\nMA_price_amazn_ts_1<-arima(price_amazn_ts,order=c(0,0,1),method = \"ML\")\nMA_price_amazn_pl_1<-Arima(price_amazn_ts,order=c(0,0,1),method = \"ML\")\nautoplot(MA_price_amazn_ts_1)+theme_light()\nplot(MA_price_amazn_pl_1$x,col=\"black\", main = \"Diferencia entre la serie de tiempo original y MA(1)\",xlab=\"Tiempo\",ylab=\"Precio\")+lines(fitted(MA_price_amazn_pl_1),col=\"red\")#> integer(0)"},{"path":"procesos-estacionarios-univariados.html","id":"procesos-armap-q-y-arimap-d-q","chapter":"6 Procesos estacionarios univariados","heading":"6.4 Procesos ARMA(p, q) y ARIMA(p, d, q)","text":"Hemos establecido algunas relaciones las de los porcesos AR y los procesos MA, es decir, cómo un \\(MA(q)\\) de la serie \\(X_t\\) puede ser reexpresada como un \\(AR(\\infty)\\) de la serie \\(U_t\\), y viceversa un \\(AR(p)\\) de la serie \\(X_t\\) puede ser reeexpresada como un \\(MA(\\infty)\\).En este sentido, para cerrar esta sección veámos el caso de la especificación que conjunta ambos modelos en un modelo general conocido como \\(ARMA(p, q)\\) o \\(ARIMA(p, d, q)\\). La diferencia entre el primero y el segundo es las veces que su tuvo que diferenciar la serie analizada, registro que se lleva en el índice \\(d\\) de los paramétros dentro del concepto \\(ARIMA(p, d, q)\\). obstante, en general nos referiremos al modelo como \\(ARMA(p, q)\\) y dependerá del analista si modela la serie en niveles (por ejemplo, en logaritmos) o en diferencias logarítmicas (o diferencias sin logaritmos).","code":""},{"path":"procesos-estacionarios-univariados.html","id":"arma1-1","chapter":"6 Procesos estacionarios univariados","heading":"6.4.1 ARMA(1, 1)","text":"Dicho lo anterior vamos empezar con el análisis de un \\(ARMA(1, 1)\\). Un proceso \\(ARMA(1, 1)\\) puede verse como:\n\\[\\begin{equation}\n    X_t = \\delta + a_1 X_{t - 1} + U_t - b_1 U_{t - 1}\n    \\tag{6.51}\n\\end{equation}\\]Aplicando el operado rezago podemos rescribir la ecuación (6.51) como:\n\\[\\begin{equation}\n    (1 - a_1 L) X_t = \\delta + (1 - b_1 L) U_t\n     \\tag{6.52}\n\\end{equation}\\]Donde \\(U_t\\) es un proceso pueramente aleatorio como en los casos de \\(AR(p)\\) y \\(MA(q)\\), y \\(X_t\\) puede ser una serie en niveles o en diferencias (ambas, en términos logarítmicos).Así, el modelo \\(ARIMA (p, q)\\) también tiene una representación de Wold que estará dada por las siguientes expresiones:\n\\[\\begin{equation}\n    X_t = \\frac{\\delta}{1 - a_1} + \\frac{1 - b_1 L}{1 - a_1 L} U_t\n    \\tag{6.53}\n\\end{equation}\\]Donde \\(a_1 \\neq b_1\\), puesto que en caso contrario \\(X_t\\) sería un proceso puramente aleatorio con una media \\(\\mu = \\frac{\\delta}{1 - a_1}\\). Así, podemos reescribir la descomposición de Wold partir del componente de la ecuación (6.53)\n\\[\\begin{equation}\n    \\frac{1 - b_1 L}{1 - a_1 L} = \\psi_0 + \\psi_1 L + \\psi_2 L^2 + \\psi_3 L^3 + \\ldots\n    \\tag{6.54}\n\\end{equation}\\]Está ecuación es equivalente la expresión:\n\\[\\begin{eqnarray}\n    (1 - b_1 L) & = & (1 - a_1 L)(\\psi_0 + \\psi_1 L + \\psi_2 L^2 + \\psi_3 L^3 + \\ldots) \\nonumber \\\\\n    & = & \\psi_0 + \\psi_1 L + \\psi_2 L^2 + \\psi_3 L^3 + \\ldots \\nonumber \\\\\n    &   & - a_1 \\psi_0 L - a_1 \\psi_1 L^2 - a_2 \\psi_2 L^3 - a_1 \\psi_3 L^4 - \\ldots \\nonumber\n\\end{eqnarray}\\]De esta forma podemos establecer el siguiente sistema de coeficientes indeterminados:\n\\[\\begin{eqnarray}\n    L^0 : &   & \\Rightarrow  &  \\psi_0 = 1  \\\\\n     L^1 :  &  \\psi_1 - a_1 \\psi_0 = - b_1  &  \\Rightarrow  &  \\psi_1 = a_1 - b_1  \\\\\n     L^2 :  &  \\psi_2 - a_1 \\psi_1 = 0  &  \\Rightarrow  &  \\psi_2 = a_1(a_1 - b_1)  \\\\\n     L^3 :  &  \\psi_3 - a_1 \\psi_2 = 0  &  \\Rightarrow  &  \\psi_3 = ^2_1(a_1 - b_1)  \\\\\n     \\vdots  &  \\vdots  &  \\vdots  &  \\vdots  \\\\\n     L^j :  &  \\psi_j - a_1 \\psi_{j - 1} = 0  &  \\Rightarrow  &  \\psi_j = ^{j - 1}_1(a_1 - b_1)\n\\end{eqnarray}\\]Así, la solución la ecuación (6.51) estará dada por la siguiente generalización:\n\\[\\begin{equation}\n    X_t = \\frac{\\delta}{1 - a_1} + U_t + (a_1 - b_1) U_{t - 1} + a_1(a_1 - b_1) U_{t - 2} + a_1^2(a_1 - b_1) U_{t - 3} + \\ldots\n\\tag{6.55}\n\\end{equation}\\]En la ecuación (6.55) las condiciones de estabilidad y de invertibilidad del sistema (de un MA un AR, y viceversa) estarán dadas por: \\(\\lvert a_1 \\lvert < 1\\) y \\(\\lvert b_1 \\lvert< 1\\). Adicionalmente, la ecuación (6.55) expresa cómo una serie que tiene un comportamiento \\(ARMA(1, 1)\\) es equivalente una serie modelada bajo un \\(MA(\\infty)\\).Al igual que en los demás modelos, ahora determinaremos los momentos del proceso \\(ARMA(1, 1)\\). La media estará dada por:\n\\[\\begin{eqnarray}\n    \\mathbb{E}[X_t] & = & \\mathbb{E}[\\delta + a_1 X_{t-1} + U_t - b_1 U_{t-1}] \\nonumber \\\\\n    & = & \\delta + a_1 \\mathbb{E}[X_{t-1}] \\nonumber \\\\\n    & = & \\frac{\\delta}{1 - a_1} \\nonumber \\\\\n    & = & \\mu\n    \\tag{6.56}\n\\end{eqnarray}\\]Donde hemos utilizado que \\(\\mathbb{E}[X_t] = \\mathbb{E}[X_{t-1}] = \\mu\\). Es decir, la media de un \\(ARMA(1, 1)\\) es idéntica la de un \\(AR(1)\\).Para determinar la varianza tomaremos una estrategía similar los casos de \\(AR(p)\\) y \\(MA(q)\\). Por lo que para todo \\(\\tau \\geq 0\\), y suponiendo por simplicidad que \\(\\delta = 0\\) (lo que implica que \\(\\mu = 0\\)) tendremos:\n\\[\\begin{eqnarray}\n    \\mathbb{E}[X_{t-\\tau} X_t] & = & \\mathbb{E}[(X_{t-\\tau}) \\cdot (a_1 X_{t-1} + U_t - b_1 U_{t-1})] \\nonumber \\\\\n    & = & a_1 \\mathbb{E}[X_{t-\\tau} X_{t-1}] + \\mathbb{E}[X_{t-\\tau} U_t] - b_1 \\mathbb{E}[X_{t-\\tau} U_{t-1}]\n    \\tag{6.57}\n\\end{eqnarray}\\]De la ecuación (6.57) podemos determinar una expresión para el caso de \\(\\tau = 0\\):\n\\[\\begin{eqnarray}\n    \\mathbb{E}[X_{t} X_t] & = & \\gamma(0) \\nonumber \\\\\n    & = & a_1 \\gamma(1) + \\mathbb{E}[U_t X_t] - b_1 \\mathbb{E}[X_t U_{t-1}] \\nonumber \\\\\n    & = & a_1 \\gamma(1) + \\sigma^2 + b_1 \\mathbb{E}[U_{t-1} (a_1 X_{t-1} + U_t - b_1 U_{t-1})] \\nonumber \\\\\n    & = & a_1 \\gamma(1) + \\sigma^2 - b_1 a_1 \\sigma^2 + b_1 \\sigma^2 \\nonumber \\\\\n    & = & a_1 \\gamma(1) + (1 - b_1 (a_1 - b_1)) \\sigma^2\n        \\tag{6.58}\n\\end{eqnarray}\\]Para el caso en que \\(\\tau = 1\\):\n\\[\\begin{eqnarray}\n    \\mathbb{E}[X_{t-1} X_t] & = & \\gamma(1) \\nonumber \\\\\n    & = & a_1 \\gamma(0) + \\mathbb{E}[X_{t-1} U_t] - b_1 \\mathbb{E}[X_{t-1} U_{t-1}] \\nonumber \\\\\n    & = & a_1 \\gamma(0) - b_1 \\sigma^2\n        \\tag{6.59}\n\\end{eqnarray}\\]Estas últimas expresiones podemos resolverlas como sistema para determinar los siguientes valores:\n\\[\\begin{eqnarray}\n    \\gamma(0) & = & \\frac{1 + b_1^2 - 2 a_1 b_1}{1 - a_1^2} \\sigma^2\n    \\tag{6.60}\n\\end{eqnarray}\\]\\[\\begin{eqnarray}\n    \\gamma(1) & = & \\frac{(a_1 - b_1)(1 - a_1 b_1)}{1 - a_1^2} \\sigma^2\n    \\tag{6.61}\n\\end{eqnarray}\\]En general para cualquier valor \\(\\tau \\geq 2\\) tenemos que la autocovarianza y la función de autocorrelación serán:\n\\[\\begin{eqnarray}\n    \\gamma(\\tau) = a_1 \\gamma(\\tau - 1) \\\\\n    \\tag{6.62}\n    \\end{eqnarray}\\]\\[\\begin{eqnarray}    \n    \\rho(\\tau) = a_1 \\rho(\\tau - 1)\n    \\tag{6.63}\n\\end{eqnarray}\\]Por ejemplo, para el caso de \\(\\tau = 1\\) tendríamos:\n\\[\\begin{equation}\n    \\rho(1) = \\frac{(a_1 - b_1)(1 - a_1 b_1)}{1 + b_1^2 - 2 a_1 b_1}\n    \\tag{6.64}\n\\end{equation}\\]De esta forma, la función de autocorrelación oscilará en razón de los valores que tome \\(a_1\\) y \\(b_1\\).","code":""},{"path":"procesos-estacionarios-univariados.html","id":"armap-q","chapter":"6 Procesos estacionarios univariados","heading":"6.4.2 ARMA(p, q)","text":"La especificación general de un \\(ARMA(p, q)\\) (donde \\(p, q \\\\mathbb{N}\\)) puede ser descrita por la siguiente ecuación:\n\\[\\begin{eqnarray}\n    X_t & = & \\delta + a_1 X_{t - 1} + a_2 X_{t - 2} + \\ldots + a_p X_{t - p} \\nonumber \\\\\n    &   & + U_t - b_1 U_{t - 1} - b_2  U_{t - 2} - \\ldots - b_q  U_{t - q}\n    \\tag{6.65}\n\\end{eqnarray}\\]Donde \\(U_t\\) es un proceso puramente aleatorio, y \\(X_t\\) puede ser modelada en niveles o en diferencias (ya sea en logaritmos o sin transformación logarítmica).Mediante el uso del operador rezago se puede escribir la ecuación (6.65) como:\n\\[\\begin{equation}\n    (1 - a_1 L - a_2 L^2 - \\ldots - a_p L^p) X_t = \\delta + (1 - b_1 L - b_2 L^2 - \\ldots - b_q L^q) U_t\n    \\tag{6.66}\n\\end{equation}\\]En la ecuación (6.66) definamos dos polinomios: \\(\\alpha(L) = (1 - a_1 L - a_2 L^2 - \\ldots - a_p L^p)\\) y \\(\\beta(L) = (1 - b_1 L - b_2 L^2 - \\ldots - b_q L^q)\\). Así, podemos reescribir la ecuación (6.66) como:\n\\[\\begin{equation}\n    \\alpha(L) X_t = \\delta + \\beta(L) U_t\n    \\tag{6.67}\n\\end{equation}\\]Asumiendo que existe el polinomio inverso tal que: \\(\\alpha(L)^{-1}\\alpha(L) = 1\\).La solución entonces puede ser escrita como:\n\\[\\begin{eqnarray}\n    X_t & = & \\alpha(L)^{-1} \\delta + \\alpha(L)^{-1} \\beta(L) U_t \\nonumber \\\\\n    & = & \\frac{\\delta}{1 - a_1 - a_2 - \\ldots - a_p} + \\frac{\\beta(L)}{\\alpha(L)} U_t \\nonumber \\\\\n    & = & \\frac{\\delta}{1 - a_1 - a_2 - \\ldots - a_p} + U_t + \\psi_1 L U_t + \\psi_2 L^2 U_t + \\ldots\n    \\tag{6.68}\n\\end{eqnarray}\\]Donde la ecuación (6.68) nos permite interpretar que un ARMA(p, q) se puede reexpresar e interpreetar como un \\(MA(\\infty)\\) y donde las condiciones para la estabilidad de la solución y la invertibilidad es que las ráices de los polinomios característicos \\(\\alpha(L)\\) y \\(\\beta(L)\\) son en valor absoluto menores 1.Adicionalmente, la fracción en la ecuación (6.68) se puede descomponer como en la forma de Wold:\n\\[\\begin{equation}\n    \\frac{\\beta(L)}{\\alpha(L)} = 1 + \\psi_1 L + \\psi_2 L^2 + \\ldots\n    \\tag{6.69}\n\\end{equation}\\]Bajo los supuestos de estacionariedad del componente \\(U_t\\), los valores de la media y varianza de un proceso \\(ARMA(p, q)\\) serán como describimos ahora. Para el caso de la media podemos partir de la ecuación (6.68) para generar:\n\\[\\begin{eqnarray}\n    \\mathbb{E}[X_t] & = & \\mathbb{E}\\left[ \\frac{\\delta}{1 - a_1 - a_2 - \\ldots - a_p} + U_t + \\psi_1 U_{t-1} + \\psi_2 U_{t-2} + \\ldots \\right] \\nonumber \\\\\n    & = & \\frac{\\delta}{1 - a_1 - a_2 - \\ldots - a_p} \\nonumber \\\\\n    & = & \\mu\n    \\tag{6.70}\n\\end{eqnarray}\\]Esta expresión indica que en general un proceso \\(ARMA(p, q)\\) converge una media idéntica la de un porceso \\(AR(p)\\). Para determinar la varianza utilizaremos la misma estratégia que hemos utilizado para otros modelos \\(AR(p)\\) y \\(MA(q)\\).Sin pérdida de generalidad podemos asumir que \\(\\delta = 0\\), lo que implica que \\(\\mu = 0\\), de lo que podemos establecer una expresión de autocovarianzas para cualquier valor \\(\\tau = 0, 1, 2, \\ldots\\):\n\\[\\begin{eqnarray}\n    \\gamma(\\tau) & = & \\mathbb{E}[X_{t-\\tau} X_t] \\nonumber \\\\\n    & = & \\mathbb{E}[X_{t-\\tau} (\\delta + a_1 X_{t - 1} + a_2 X_{t - 2} + \\ldots + a_p X_{t - p} \\nonumber \\\\\n    &   & + U_t - b_1 U_{t - 1} - b_2  U_{t - 2} - \\ldots - b_q  U_{t - q})] \\nonumber \\\\\n    & = & a_1 \\gamma(\\tau - 1) + a_2 \\gamma(\\tau - 2) + \\ldots + a_p \\gamma(\\tau - p) \\nonumber \\\\\n    &   & + \\mathbb{E}[X_{t-\\tau} U_{t}] - b_1  \\mathbb{E}[X_{t-\\tau} U_{t-1}] - \\ldots  - b_q  \\mathbb{E}[X_{t-\\tau} U_{t-q}]\n    \\tag{6.71}\n\\end{eqnarray}\\]","code":""},{"path":"procesos-estacionarios-univariados.html","id":"selección-de-las-constantes-p-q-d-en-un-arp-un-maq-un-armap-q-o-un-arimap-d-q","chapter":"6 Procesos estacionarios univariados","heading":"6.5 Selección de las constantes p, q, d en un AR(p), un MA(q), un ARMA(p, q) o un ARIMA(p, d, q)","text":"Respecto de cómo estimar un proceso ARMA(p, q) –en general utilizaremos este modelo para discutir, pero lo planteado en esta sección es igualmente aplicable en cualquier otro caso como aquellos modelos que incluyen variables exogénas– existen diversas formas de estimar los paramétros \\(a_i\\) y \\(b_i\\): ) por máxima verosimilitd y ii) por mínimos cuadrados órdinarios. El primer caso requiere que conozcamos la distribución del proceso aleatorio \\(U_t\\). El segundo, por el contrario, requiere el mismo supuesto. obstante, para el curso utilizaremos el método de máxima verosimilitud.Otra duda que debe quedar hasta el momento es ¿cómo determinar el orden \\(p\\) y \\(q\\) del proceso ARMA(p, q)? La manera más convencional y formal que existe para tal efecto es utilizar los criterios de información. Así, el orden se elije de acuerdo aquel críterio de información que resulta ser el mínimo. En el caso de \\(d\\) se selecciona revisando la gráfica que parezca más estacionaria–más adelante mostraremos un proceso más formal para su selección.Los criterios de información más comunes son los siguientes:FPE (Final Prediction Error):\n\\[\\begin{equation}\nFPE = \\frac{T+m}{T-m}\\frac{1}{T}\\sum_{t=1}^{T} \\left( \\hat{U}_t^{(p)} \\right) ^2\n\\tag{6.72}\n\\end{equation}\\]FPE (Final Prediction Error):\n\\[\\begin{equation}\nFPE = \\frac{T+m}{T-m}\\frac{1}{T}\\sum_{t=1}^{T} \\left( \\hat{U}_t^{(p)} \\right) ^2\n\\tag{6.72}\n\\end{equation}\\]Akaike:\n\\[\\begin{equation}\nAIC = ln \\left[ \\frac{1}{T} \\sum_{t=1}^{T} \\left( \\hat{U}_t^{(p)} \\right) ^2 \\right] + m \\frac{2}{T}\n     \\tag{6.73}\n\\end{equation}\\]Akaike:\n\\[\\begin{equation}\nAIC = ln \\left[ \\frac{1}{T} \\sum_{t=1}^{T} \\left( \\hat{U}_t^{(p)} \\right) ^2 \\right] + m \\frac{2}{T}\n     \\tag{6.73}\n\\end{equation}\\]Schwarz:\n\\[\\begin{equation}\nSC = ln \\left[ \\frac{1}{T} \\sum_{t=1}^{T} \\left( \\hat{U}_t^{(p)} \\right) ^2 \\right] + m \\frac{ln(T)}{T}\n   \\tag{6.74}\n\\end{equation}\\]Schwarz:\n\\[\\begin{equation}\nSC = ln \\left[ \\frac{1}{T} \\sum_{t=1}^{T} \\left( \\hat{U}_t^{(p)} \\right) ^2 \\right] + m \\frac{ln(T)}{T}\n   \\tag{6.74}\n\\end{equation}\\]Hannan - Quinn:\n\\[\\begin{equation}\nHQ = ln \\left[ \\frac{1}{T} \\sum_{t=1}^{T} \\left( \\hat{U}_t^{(p)} \\right) ^2 \\right] + m \\frac{2 ln(ln(T))}{T}\n   \\tag{6.75}\n\\end{equation}\\]Hannan - Quinn:\n\\[\\begin{equation}\nHQ = ln \\left[ \\frac{1}{T} \\sum_{t=1}^{T} \\left( \\hat{U}_t^{(p)} \\right) ^2 \\right] + m \\frac{2 ln(ln(T))}{T}\n   \\tag{6.75}\n\\end{equation}\\]Donde \\(\\hat{U}_t^{(p)}\\) son los residuales estimados mediante un proceso ARIMA y \\(m\\) es el número de parametros estimados: \\(m = p + q + 0 + 1\\) (ya que asumimos que \\(d = 0\\)). Una propiedad que se debe perder de vista es que los criterios de información cumplen la siguiente relación:\n\\[\\begin{equation}\n    orden(SC) \\leq orden(HQ) \\leq orden(AIC)\n    \\tag{6.76}\n\\end{equation}\\]Por esta razón, durante el curso solo utilizaremos el criterio se Akaike para determinar el orden óptimo del proceso ARMA, ya que ello garantiza el orden más grande posible.Veamos el ejemplo.","code":""},{"path":"procesos-estacionarios-univariados.html","id":"ejemplo-arma","chapter":"6 Procesos estacionarios univariados","heading":"6.5.1 Ejemplo ARMA","text":"Para este caso coemzaremos por agregar dos series de tiempo. Una correspone una transformación logarítmica de los valores de los precios y, otra, corresponde la diferentcia logaritmica. Esto dado que:\n\\[log(X_t)-log(X_{t-k})\\sim\\frac{X_t-X_{t-k}}{X_t}\\].Tranformacion de la serie originalLas tres seriesEs claro que este pico se da en 2020 desde junio hasta julio, probablemente causado por el aumento del uso en amzn durante la pandemia. Esos son outlier que debemos considerar. Por ello hay que marcarlos con una variable dummy.Como lo vimos en las figuras 6.9 y 6.10, es claro que debemos usar un valor \\(q\\) de \\(1\\) y \\(p\\) de \\(1\\). Pero tambien necesitamos diferenciar, lo cual corresponde al valor \\(d\\). Así pues, dado que en las funciones de autocorrelacion solo vemos un pico arriba de la linea punteada azul, podemos asumir que el valor de diferenciación debe ser uno. Esto se debe seguir como regla de dedo, pero en general se debe usar el valor que minimice la desviacion estandar. Por tanto nuestro modelo debe ser \\(arima(1,1,1)\\).Utilizamos la funcion “auto.arima” para confirmar que nuestra serie de tiempo deba ser \\(arima(1,1,1)\\), el arima que debos usar es aquel que minimize el indice AIC.\nARMA(1,1,1) de los precios de apertura de AMZN\nRaices MA():\nFigure 6.16: Raices ARMA(1) Inversas de la serie de tiempo\nDado esto, sabemos claramente que podremos analizar de mejor manera estos valores y, en consecuencia, hacer mejores estimaciones.","code":"\n#original\nprice_amazn_ts<-ts(data_precio_amzn$price.open, frequency = 12, start=c(2002,10))\n#logartimo\nlprice_amazn_ts<-ts(log(data_precio_amzn$price.open), frequency = 12)\n#diferencias logaritmicas(cambio porcential)\ndlprice_amazn_ts<-ts(log(data_precio_amzn$price.open)-lag(log(data_precio_amzn$price.open),1), frequency = 12)\npar(mfrow = c(3,1))\nplot(price_amazn_ts, xlab = \"Tiempo\", \n     main = \"Precios de apertura\",\n     col = \"darkgreen\")\n\nplot(lprice_amazn_ts, xlab = \"Tiempo\", \n     main = \"LN Precios de apertura\",\n     col = \"darkblue\")\n\nplot(dlprice_amazn_ts, xlab = \"Tiempo\", \n     main = \"Diff LN de precios de apertura\", \n     col = \"darkred\")\n#Generamos el rango de tiempo\njunio2020 <- seq.Date(\n  from=as.Date(\"2020-06-01\"),\n  to=as.Date(\"2020-6-29\"),\n  by=\"day\")\njulio2020 <- seq.Date(\n  from=as.Date(\"2020-07-01\"),\n  to=as.Date(\"2020-07-29\"),\n  by=\"day\")\nagosto2020 <- seq.Date(\n  from=as.Date(\"2020-08-01\"),\n  to=as.Date(\"2020-08-29\"),\n  by=\"day\")\nsept2020 <- seq.Date(\n  from=as.Date(\"2020-09-01\"),\n  to=as.Date(\"2020-09-29\"),\n  by=\"day\")\noct2020 <- seq.Date(\n  from=as.Date(\"2020-10-01\"),\n  to=as.Date(\"2020-10-29\"),\n  by=\"day\")\nnov2020 <- seq.Date(\n  from=as.Date(\"2020-11-01\"),\n  to=as.Date(\"2020-11-29\"),\n  by=\"day\")\ndiciembre2020 <- seq.Date(\n  from=as.Date(\"2020-12-01\"),\n  to=as.Date(\"2020-12-29\"),\n  by=\"day\")\n#Añadimos valores 1 y 0 dependiento si estan dentro(1) o no\ndata_precio_amzn$junio2020<-ifelse(data_precio_amzn$ref.date%in%junio2020,1,0)\ndata_precio_amzn$julio2020<-ifelse(data_precio_amzn$ref.date%in%julio2020,1,0)\ndata_precio_amzn$agosto2020<-ifelse(data_precio_amzn$ref.date%in%agosto2020,1,0)\ndata_precio_amzn$sept2020<-ifelse(data_precio_amzn$ref.date%in%sept2020,1,0)\ndata_precio_amzn$oct2020<-ifelse(data_precio_amzn$ref.date%in%oct2020,1,0)\ndata_precio_amzn$nov2020<-ifelse(data_precio_amzn$ref.date%in%nov2020,1,0)\ndata_precio_amzn$diciembre2020<-ifelse(data_precio_amzn$ref.date%in%diciembre2020,1,0)\n#ts\njunio2020ts<-ts(data_precio_amzn$junio2020, frequency = 12, start=c(2002,10))\njulio2020ts<-ts(data_precio_amzn$julio2020, frequency = 12, start=c(2002,10))\nagosto2020ts<-ts(data_precio_amzn$agosto2020, frequency = 12, start=c(2002,10))\nsep2020ts<-ts(data_precio_amzn$sept2020, frequency = 12, start=c(2002,10))\noct2020ts<-ts(data_precio_amzn$oct2020, frequency = 12, start=c(2002,10))\nnov2020ts<-ts(data_precio_amzn$nov2020, frequency = 12, start=c(2002,10))\ndiciembre2020ts<-ts(data_precio_amzn$diciembre2020, frequency = 12, start=c(2002,10))\n#espacio de prediccion\n#dummies\ntimepred <- seq.Date(\n  from=as.Date(\"2022-10-01\"),\n  to=as.Date(\"2023-10-10\"),\n  by=\"month\")\njunio2020 <- rep(0,13)\njulio2020 <- rep(0,13)\nagosto2020 <- rep(0,13)\nsep2020 <- rep(0,13)\noct2020 <- rep(0,13)\nnov2020 <- rep(0,13)\ndiciembre2020 <- rep(0,13)\n#data frame\npred.df <- data.frame(timepred,junio2020,julio2020, agosto2020,sep2020, oct2020, nov2020, diciembre2020)\n#dummies\nd.enero <- seq.Date(\n  from=as.Date(\"2023-01-01\"),\n  to=as.Date(\"2023-01-29\"),\n  by=\"day\")\nd.junio <- seq.Date(\n  from=as.Date(\"2023-06-01\"),\n  to=as.Date(\"2023-06-29\"),\n  by=\"day\")\nd.dic <- seq.Date(\n  from=as.Date(\"2023-12-01\"),\n  to=as.Date(\"2023-12-29\"),\n  by=\"day\")\npred.df$d.enero<-ifelse(pred.df$timepred%in%d.enero,1,0)\npred.df$d.junio<-ifelse(pred.df$timepred%in%d.junio,1,0)\npred.df$d.dic<-ifelse(pred.df$timepred%in%d.dic,1,0)\n#series de tiempo predictivas\nf.junio2020 <- ts(pred.df$junio2020, frequency = 12, start=c(2022,10))\nf.agosto2020 <- ts(pred.df$junio2020, frequency = 12, start=c(2022,10))\nf.sep2020 <- ts(pred.df$junio2020, frequency = 12, start=c(2022,10))\nf.oct2020 <- ts(pred.df$junio2020, frequency = 12, start=c(2022,10))\nf.nov2020 <- ts(pred.df$junio2020, frequency = 12, start=c(2022,10))\nf.julio2020 <- ts(pred.df$junio2020, frequency = 12, start=c(2022,10))\nf.enero <- ts(pred.df$d.enero, frequency = 12, start=c(2022,10))\nf.junio <- ts(pred.df$d.junio, frequency = 12, start=c(2022,10))\nf.diciembre <- ts(pred.df$d.dic, frequency = 12, start=c(2022,10))\nauto.arima(price_amazn_ts, trace=TRUE)\n#> \n#>  Fitting models using approximations to speed things up...\n#> \n#>  ARIMA(2,2,2)(1,0,1)[12]                    : Inf\n#>  ARIMA(0,2,0)                               : 1458.057\n#>  ARIMA(1,2,0)(1,0,0)[12]                    : 1396.532\n#>  ARIMA(0,2,1)(0,0,1)[12]                    : 1296.904\n#>  ARIMA(0,2,1)                               : 1295.488\n#>  ARIMA(0,2,1)(1,0,0)[12]                    : 1308.745\n#>  ARIMA(0,2,1)(1,0,1)[12]                    : Inf\n#>  ARIMA(1,2,1)                               : 1297.615\n#>  ARIMA(0,2,2)                               : 1296.662\n#>  ARIMA(1,2,0)                               : 1382.899\n#>  ARIMA(1,2,2)                               : 1299.614\n#> \n#>  Now re-fitting the best model(s) without approximations...\n#> \n#>  ARIMA(0,2,1)                               : 1305.631\n#> \n#>  Best model: ARIMA(0,2,1)\n#> Series: price_amazn_ts \n#> ARIMA(0,2,1) \n#> \n#> Coefficients:\n#>           ma1\n#>       -0.9716\n#> s.e.   0.0140\n#> \n#> sigma^2 estimated as 17.95:  log likelihood=-650.79\n#> AIC=1305.58   AICc=1305.63   BIC=1312.43\n#sin embargo hay que controlar con nuestra variable dummy\nARIMA_price_amzn_ts_111<-arima(price_amazn_ts,order=c(1,1,1),\n                               method = \"ML\",xreg=cbind(junio2020ts,julio2020ts,agosto2020ts,sep2020ts,oct2020ts,nov2020ts,diciembre2020ts))\nARIMA_price_amzn_pl_111<-Arima(price_amazn_ts,order=c(1,1,1),\n                               method = \"ML\",xreg=cbind(junio2020ts,julio2020ts,agosto2020ts,sep2020ts,oct2020ts,nov2020ts,diciembre2020ts))\nautoplot(ARIMA_price_amzn_ts_111)+theme_light()"},{"path":"procesos-estacionarios-univariados.html","id":"pronósticos","chapter":"6 Procesos estacionarios univariados","heading":"6.5.2 Pronósticos","text":"Para pronósticar el valor de la serie es necesario determinar cuál es el valor esperado de la serie en un momento \\(t + \\tau\\) condicional en que ésta se comporta como un \\(AR(p)\\), un \\(MA(q)\\) o un \\(ARMA(p, q)\\) y que los valores antes de \\(t\\) están dados. Por lo que el pronóstico de la serie estará dado por una expresión:\\[\\begin{eqnarray}\n    \\mathbb{E}_t[X_{t+\\tau}] = \\delta + a_1 \\mathbb{E}_t[X_{t+\\tau-1}] + a_2 \\mathbb{E}_t[X_{t+\\tau-2}] + \\ldots + + a_p \\mathbb{E}_t[X_{t+\\tau-p}]\n    \\tag{6.77}\n\\end{eqnarray}\\]Valores:\nFigure 6.17: Predicción de \\(Arima(1,1,1)\\)\n","code":"\nARIMA_price_amzn_ts_111_F<-predict(ARIMA_price_amzn_ts_111,n.ahead=13,newxreg=cbind(f.junio2020,f.julio2020,f.agosto2020,f.sep2020,f.oct2020,f.nov2020,f.diciembre))\n\nforecast.Arima <- forecast(ARIMA_price_amzn_pl_111,h=13, xreg=cbind(f.junio2020,f.julio2020,f.agosto2020,f.sep2020,f.oct2020,f.nov2020,f.diciembre))\nforecast.Arima.df <- data.frame(forecast.Arima)\nstargazer(forecast.Arima.df,type = \"html\")\nplot(forecast.Arima, main =\"Forecast ARIMA(1,1,1) de los precios de AMZN\")"}]
