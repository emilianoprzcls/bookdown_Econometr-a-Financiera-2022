[{"path":"index.html","id":"mínimos-cuadrados-ordinarios","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1 Mínimos Cuadrados Ordinarios","text":"","code":""},{"path":"index.html","id":"el-problema","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1.1 El problema","text":"Recordando que el método de MCO resulta en encontrar la combinación de valores de los estimadores de los parámetros \\(\\hat{\\boldsymbol{\\beta}}\\) que permita minimizar la suma de los residuales (estimadores de los términos de erro \\(\\boldsymbol{\\varepsilon}\\)) al cuadrado dada por:\\[\n    \\sum^{N}_{=1}{e^2_i} = \\sum^{N}_{= 1}{(y_i - \\mathbf{X}'_i \\hat{\\boldsymbol{\\beta}})^2}\n\\]Donde \\(\\hat{\\boldsymbol{\\beta}}\\) denota el vector de estimadores \\(\\hat{\\beta}_1, \\ldots, \\hat{\\beta}_K\\) y dado que \\((e_1, e_2, \\ldots, e_n)'(e_1, e_2, \\ldots, e_n) = {\\mathbf{e'e}}\\), el problema del método de MCO consiste en resolver el problema de óptimización:\\[\\begin{eqnarray*}\nMinimizar_{\\hat{\\boldsymbol \\beta}} S(\\hat{\\boldsymbol \\beta})  =  Minimizar_{\\hat{\\boldsymbol \\beta}} \\mathbf{e'e} \\\\\n    =  Minimizar_{\\hat{\\boldsymbol \\beta}} (\\mathbf{Y}-\\mathbf{X}\\hat{\\boldsymbol \\beta})'(\\mathbf{Y}-\\mathbf{X}\\hat{\\boldsymbol \\beta})\n\\end{eqnarray*}\\]Expandiendo la expresión \\(\\mathbf{e'e}\\) obtenemos:\n\\[\n    \\mathbf{e'e} = \\mathbf{Y'Y} - 2 \\mathbf{Y'X} \\hat{\\boldsymbol \\beta} + \\hat{\\boldsymbol \\beta}' \\mathbf{X'X}\\hat{\\boldsymbol \\beta}\n\\]De esta forma obtenemos que las condiciones necesarias de un mínimo son:\\[\n    \\frac{\\partial S(\\hat{\\boldsymbol \\beta})}{\\partial \\hat{\\boldsymbol \\beta}} = -2{\\mathbf{X'Y}} + 2{\\mathbf{X'X}} \\hat{\\boldsymbol{\\beta}} = \\mathbf{0}\n\\]\nY se pueden despejar las dadas por:Debido que el objetivo es encontrar la matriz \\(\\hat{\\boldsymbol\\beta}\\) despejamos:\\[\\hat{\\boldsymbol \\beta} = (\\mathbf{X'X})^{-1}\\mathbf{X'Y}\n\\]\n\\[\n    \\mathbf{X'X}\\hat{\\boldsymbol \\beta} = \\mathbf{X'Y}\n\\]","code":""},{"path":"index.html","id":"estimación-r","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1.2 Estimación R","text":"Para la estimación utilizaremos el paquete “BatchGetSymbols”. Este paquete nos permitirá descargar información acerca de la bolsa de valores internacional.","code":""},{"path":"index.html","id":"dependencias","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1.2.1 Dependencias","text":"","code":"\n#install.packages(\"pacman\")\n#pacman nos permite cargar varias librerias en una sola línea\nlibrary(pacman)\npacman::p_load(tidyverse,BatchGetSymbols,ggplot2, lubridate)"},{"path":"index.html","id":"descarga-de-los-valores","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1.2.2 Descarga de los valores","text":"","code":"\n#Primero determinamos el lapso de tiempo\npd<-Sys.Date()-365 #primer fecha\npd\n#> [1] \"2021-11-22\"\nld<-Sys.Date() #última fecha\nld\n#> [1] \"2022-11-22\"\n#Intervalos de tiempo\nint<-\"monthly\"\n#Datos a elegir\ndt<-c(\"AMZN\")\n\n#Descargando los valores\n?BatchGetSymbols()\ndata<- BatchGetSymbols(tickers = dt,\n                       first.date = pd,\n                       last.date = ld,\n                       freq.data = int,\n                       do.cache = FALSE,\n                       thresh.bad.data = 0)\n\n#Generando data frame con los valores\ndata_precio<-data$df.tickers\ncolnames(data_precio)\n#>  [1] \"ticker\"              \"ref.date\"           \n#>  [3] \"volume\"              \"price.open\"         \n#>  [5] \"price.high\"          \"price.low\"          \n#>  [7] \"price.close\"         \"price.adjusted\"     \n#>  [9] \"ret.adjusted.prices\" \"ret.closing.prices\""},{"path":"index.html","id":"gráficas","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1.2.3 Gráficas","text":"","code":"\nsp_precio<-ggplot(data_precio, aes(x=ref.date, y=price.open))+geom_point(size =2, colour = \"black\")+labs(x=\"Fecha\", y=\"Precio de apertura (USD)\", title=\"Precio de apertura de AMZN en el ultimo año\")+ theme_light()+ geom_smooth(method = lm, se = TRUE)\nsp_precio\n\nsp_volumen<-ggplot(data_precio, aes(x=ref.date, y=volume))+geom_point(size =2, colour = \"black\")+labs(x=\"Fecha\", y=\"Volumen\", title=\"Volumenes de AMZN en el ultimo año\")+ theme_light()+ geom_smooth(method = lm, se = TRUE)\nsp_volumen"},{"path":"index.html","id":"regresión-lineal-que-optiene-los-coeficientes-hatboldsymbol-beta","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1.2.4 Regresión lineal que optiene los coeficientes \\(\\hat{\\boldsymbol \\beta}\\)","text":"","code":"\n#datos estadísticos\nsummary(data_precio[c(\"price.open\",\"volume\")])\n#>    price.open        volume         \n#>  Min.   :104.0   Min.   :4.224e+08  \n#>  1st Qu.:122.3   1st Qu.:1.288e+09  \n#>  Median :135.0   Median :1.465e+09  \n#>  Mean   :140.4   Mean   :1.451e+09  \n#>  3rd Qu.:164.1   3rd Qu.:1.635e+09  \n#>  Max.   :183.8   Max.   :2.258e+09\n#análisis de regresión lineal lm() y=precio,x=fecha\nreg_tiempo_precio<-lm(price.open~ref.date, data=data_precio) \n#¡Siempre se pone dentro de lm() la variable dependiente primero y luego la independiete!\nsummary(reg_tiempo_precio)\n#> \n#> Call:\n#> lm(formula = price.open ~ ref.date, data = data_precio)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -21.886  -9.705   1.750   9.191  16.846 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 4158.44364  605.82333   6.864 2.71e-05 ***\n#> ref.date      -0.21019    0.03169  -6.633 3.70e-05 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 12.71 on 11 degrees of freedom\n#> Multiple R-squared:    0.8,  Adjusted R-squared:  0.7818 \n#> F-statistic: 43.99 on 1 and 11 DF,  p-value: 3.697e-05\n\n#análisis de regresión lineal lm() y=volumen,x=fecha\nreg_tiempo_volumen<-lm(volume~ref.date, data=data_precio)\nsummary(reg_tiempo_volumen)\n#> \n#> Call:\n#> lm(formula = volume ~ ref.date, data = data_precio)\n#> \n#> Residuals:\n#>        Min         1Q     Median         3Q        Max \n#> -894976876 -160139242   36286886  230217822  809528583 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)\n#> (Intercept) -1.417e+10  2.035e+10  -0.696    0.501\n#> ref.date     8.174e+05  1.065e+06   0.768    0.459\n#> \n#> Residual standard error: 427100000 on 11 degrees of freedom\n#> Multiple R-squared:  0.05085,    Adjusted R-squared:  -0.03543 \n#> F-statistic: 0.5893 on 1 and 11 DF,  p-value: 0.4588"},{"path":"index.html","id":"ejercicio","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1.3 Ejercicio","text":"El objetivo de este ejrcicio es simplemente que indiquen y modifiquen los errores en el código. Así pues, deberán descomentar -quitar las #antes del código- para empezar el ejercicio.","code":""},{"path":"index.html","id":"section","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1.3.1 1","text":"El objetivo de este código es explicar la variable “volume” con la variable “price.high”.","code":"\n#reg_tiempo_ej1<-lm(price.high~volume, data=data_precio)\n#sumary(reg_tiempo_ej1)"},{"path":"index.html","id":"section-1","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1.3.2 2","text":"El objetivo de este código es explicar la variable “volume” con la variable “price.low”.","code":"\n#reg_tiempo_ej2<-lm(price.low~volume, data=data_precio)\n#summary(reg_tiempo_ej1)"},{"path":"index.html","id":"opcional","chapter":"1 Mínimos Cuadrados Ordinarios","heading":"1.3.3 3 (opcional)","text":"El objetivo de este ejercicio es descargar los valores del stock de Tesla BMV: TSLA en los últimos dos años.","code":"\n#dt_ej3<-(\"TSLA\")\n#pdej<-Sys.Date()-(365*3) #primer fecha\n#pdej\n#Descargando los valores\n#dataej3<- BatchgetSymbols(tickers = dt_ej3,\n                       #first.date = pdej,\n                       #last.date = ld,\n                       #freq.data = int,\n                       #do.cache = FALSE,\n                       #thresh.bad.data = 0)\n\n#Generando data frame con los valores\n#data_precio_ej2<-dataej3$df.tickers\n#1colnames(data_precio_ej2)"},{"path":"máxima-verosimilitud.html","id":"máxima-verosimilitud","chapter":"2 Máxima Verosimilitud","heading":"2 Máxima Verosimilitud","text":"","code":""},{"path":"máxima-verosimilitud.html","id":"el-problema-1","chapter":"2 Máxima Verosimilitud","heading":"2.1 El problema","text":"Recordemos que dado \\(f(y_i | \\mathbf{x}_i)\\) la función de densidad condicional de \\(y_i\\) dado \\(\\mathbf{x}_i\\). Sea \\(\\boldsymbol{\\theta}\\) un conjunto de parámetros de la función. Entonces la función de densidad conjunta de variables aleatorias independientes \\(\\{ y_i : y_i \\\\mathbb{R} \\}\\) dados los valores \\(\\{ \\mathbf{x}_i : \\mathbf{x}_i \\\\mathbb{R}^K \\}\\) estará dada por:\\[\\begin{equation}\n    \\Pi_{= 1}^{n} f(y_i | \\mathbf{x}_i; \\boldsymbol{\\theta}) = f(y_1, y_2, \\ldots, y_n | \\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n; \\boldsymbol{\\theta}) = L(\\boldsymbol{\\theta})\n    \\tag{2.1}\n\\end{equation}\\]la ecuación (2.1) se le conoce como ecuación de verosimilitud. El problema de máxima verosimilitud entonces será:\n\\[\\begin{equation}\n    \\max_{\\boldsymbol{\\theta} \\\\boldsymbol{\\Theta}} \\Pi_{= 1}^{n} f(y_i | \\mathbf{x}_i; \\boldsymbol{\\theta}) = \\max_{\\boldsymbol{\\theta} \\\\boldsymbol{\\Theta}} L(\\boldsymbol{\\theta})\n        \\tag{2.2}\n\\end{equation}\\]Dado que el logaritmo natural es una transformación monotona, podemos decir que el problema de la ecuación (2.2) es equivalente :\\[\\begin{equation}\n     \\max_{\\boldsymbol{\\theta} \\\\boldsymbol{\\Theta}} ln L(\\boldsymbol{\\theta}) = \\max_{\\boldsymbol{\\theta} \\\\boldsymbol{\\Theta}} ln \\Pi_{= 1}^{n} f(y_i | \\mathbf{x}_i; \\boldsymbol{\\theta}) = \\max_{\\boldsymbol{\\theta} \\\\boldsymbol{\\Theta}} \\sum_{= 1}^{n} ln f(y_i | \\mathbf{x}_i; \\boldsymbol{\\theta})\n            \\tag{2.3}\n\\end{equation}\\]Para solucionnar el problema se tiene que determinar las condicones de primer y segundo orden, las cuales serán:\n\\[\\begin{equation}\n    \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} ln L(\\boldsymbol{\\theta}) = \\nabla ln L(\\boldsymbol{\\theta})\n          \\tag{2.4}\n\\end{equation}\\]\\[\\begin{equation}\n    \\frac{\\partial^2}{\\partial^2 \\boldsymbol{\\theta}} ln L(\\boldsymbol{\\theta}) = \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} ln L(\\boldsymbol{\\theta}) \\cdot  \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} ln L(\\boldsymbol{\\theta}') = H(\\boldsymbol{\\theta})\n             \\tag{2.5}\n\\end{equation}\\]La solución estará dada por aquel valor de \\(\\hat{\\boldsymbol{\\theta}}\\) que hace:\n\\[\\begin{equation*}\n    \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} ln L(\\hat{\\boldsymbol{\\theta}}) = 0\n\\end{equation*}\\]su vez, la varianza será aquella que resulta de:\n\\[\\begin{equation*}\n    Var[\\hat{\\boldsymbol{\\theta}} | \\mathbf{X}] = \\left( - \\mathbb{E}_{\\hat{\\boldsymbol{\\theta}}}[H(\\boldsymbol{\\theta})] \\right)^{-1}\n\\end{equation*}\\]","code":""},{"path":"máxima-verosimilitud.html","id":"estimación-y-simunlación","chapter":"2 Máxima Verosimilitud","heading":"2.2 Estimación y simunlación","text":"","code":""},{"path":"máxima-verosimilitud.html","id":"lanzar-una-moneda","chapter":"2 Máxima Verosimilitud","heading":"2.2.1 Lanzar una moneda","text":"Si bien el ejercicio anterior es un tanto repetitivo debido que sabemos que hay un 50% de que caiga una moneda de un lado o otro. Esto ejemplifica la manera en la que se utiliza el metodo de maximización de máxima verosimilitud.","code":"\nset.seed(1234)#esto sirve para siempre generar los mismos numeros aleatorios\n#rbinom(numero observaciones,numero de ensayos,probabilidad de exito en cada ensayo)\ncara<-rbinom(1,100,0.5)\ncara#esto nos dice de los 100 ensayos cuantos fueron cara\n#> [1] 47\nsol<-100-cara\nsol\n#> [1] 53\n\n\n#Ahora definiremos la función que encontrará la función de verosimilutud para determinado valor p\n#\nverosimilitud <- function(p){\n  dbinom(cara, 100, p)\n}\n\n#si suponemos que la probabilidad sesgada de que caiga cara es 40%\nprob_sesgada<-0.4\n#es posible calcular la función de que salga cara\nverosimilitud(prob_sesgada)\n#> [1] 0.02919091\n#ahora es posible generar una función de verimilitud negativa \n#para maximizar el valor de la verosimilitud\nneg_verosimilitud <- function(p){\n  dbinom(cara, 100, p)*-1\n}\nneg_verosimilitud(prob_sesgada)\n#> [1] -0.02919091\n# unamos la función nlm() para maximizar esta función no linear\n#?nlm()\nnlm(neg_verosimilitud,0.5,stepmax=0.5)#se pone un parametro porque sabemos que hay un 0.5 de probabilidad de que caiga cara\n#> $minimum\n#> [1] -0.07973193\n#> \n#> $estimate\n#> [1] 0.47\n#> \n#> $gradient\n#> [1] 1.589701e-10\n#> \n#> $code\n#> [1] 1\n#> \n#> $iterations\n#> [1] 4"},{"path":"método-generalizado-de-momentos-mgm.html","id":"método-generalizado-de-momentos-mgm","chapter":"3 Método Generalizado de Momentos (MGM)","heading":"3 Método Generalizado de Momentos (MGM)","text":"","code":""},{"path":"método-generalizado-de-momentos-mgm.html","id":"el-problema-2","chapter":"3 Método Generalizado de Momentos (MGM)","heading":"3.1 El problema","text":"Retomemos el modelo de regresión lineal tal que:\\[\\begin{equation}\ny_i=X_i\\beta+u_i\n    \\label{Eq_reglin}\n\\end{equation}\\]Tomando en cuenta los principios de ortogonalidad (\\(E(Z_iu_i)=0\\)) y (\\(rankE(Z_i^{'}X_i)=0\\)) sabemos que \\(\\beta\\) es el único vector de \\(N\\times1\\) que resuelve las condiciones de momento de determinada población. En otras palabras, \\(E[z_i^{'}(y_i-x_i\\beta)]=0\\) es una solución y \\(E[z_i^{'}(y_i-x_i\\beta)]\\neq0\\) es una solución. Debido que la media muestral son estimadores consistentes de momentos de una población, se puede:\\[\\begin{equation}\nN^{-1}\\sum_{=1}^{N}z_i^{'}(y_i-x_i\\beta)=0\n\\tag{3.1}\n\\end{equation}\\]Asumiendo que la ecuación (3.1) tiene L ecuaciones lineales y K coeficientes \\(\\beta\\) desconocidos y \\(K=L\\), entonces la matriz \\(\\sum_{=1}^{N}z_i^{'}x_i\\) debe ser singular para encontrar los coeficientes de la siguiente manera.\\[\\begin{equation}\n\\hat{\\beta}=N^{-1}\\left[\\sum_{=1}^{N}z_i^{'}x_i\\right]^{-1}\\left[\\sum_{=1}^{N}z_i^{'}y_i\\right]\n\\tag{3.2}\n\\end{equation}\\]Para simplificar (3.2) se puede nombrar Z juntando \\(z_i\\) N veces para crear una matriz de tamaño \\(NG\\times L\\). Lo mismo hacemos con X juntando \\(x_i\\) para obtener una de \\(NG\\times K\\) y Y obteniendo una \\(NG\\times 1\\). Obteniendo:\\[\\begin{equation}\n\\hat{\\beta}=[Z^{'}X]^{-1}[Z^{'}Y]\n\\end{equation}\\]Es importante tomar en cuenta cuando el caso en el que hay más ecuaciones lineales que coeficientes \\(\\beta\\); es decir, \\(L\\geq K\\). En estos casos es muy probrable que haya solución, por lo que mejor que se puede estimar es pones la ecuación (3.1), tan pequeña como sea posible. Por lo mismo el paso que nos lleva la ecuación (3.2), debe eliminarse \\(N^{-1}\\). El objetivo:\\[\\begin{equation}\n\\min_{\\beta} \\left[\\sum_{=1}^{N}z_i^{'}x_i\\beta\\right]^{-1}\\left[\\sum_{=1}^{N}z_i^{'}y_i\\beta\\right]\n\\tag{3.3}\n\\end{equation}\\]Así pues nombramos W como una matriz simétrica de \\(W\\times W\\) donde se genera la variable \\(b\\) que debemos minimizar que sustituye \\(\\beta\\) creando una función cuadrática en la ecuación (3.2).\n\\[\\begin{equation}\n\\min_{b}\\left[\\sum_{=1}^{N}z_i^{'}x_ib\\right]^{-1}\\left[\\sum_{=1}^{N}z_i^{'}y_ib\\right]\n\\tag{3.4}\n\\end{equation}\\]\\[\\begin{equation}\n\\therefore\\hat{\\beta}=[X^{'}Z\\hat{W}Z^{'}X]^{-1}[X^{'}Z\\hat{W}Z^{'}Y]\n\\end{equation}\\]Sin embargo, \\(X^{'}Z\\hat{W}Z^{'}X\\) debe ser singular para que haya una solución. Para esto se asume que \\(\\hat{W}\\) tiene un limite de probabilidad singular. Esto se describe como \\(\\hat{W}\\xrightarrow[]{p}W\\) y \\(N\\xrightarrow[]{}W\\infty\\) donde \\(W\\) es aleatorio, es una matriz positiva definida simétrica de \\(L\\times L\\).","code":""},{"path":"capital-asset-pricing-model-capm.html","id":"capital-asset-pricing-model-capm","chapter":"4 Capital Asset Pricing Model (CAPM)","heading":"4 Capital Asset Pricing Model (CAPM)","text":"","code":""},{"path":"capital-asset-pricing-model-capm.html","id":"el-problema-3","chapter":"4 Capital Asset Pricing Model (CAPM)","heading":"4.1 El problema","text":"Una vez que hemos establecido la manera en la que se pueden estimar algunos valores –como las regresiones lineales y el método de máxima verosimilitud–, además de la naturaleza de los retornos de algunos activos en el capítulo 4, es posible comenzar hablar de maneras en la que se pueden estimar los valores futuros de los rendimientos de activos y –de esta manera– poder tomar mejores decisiones de inversiones. Por ello, hablaremos del modelo de Capital Asset Pricing Model. El modelo es muy sencillo y pretende estimar su rentabilidad esperada en función del riesgo sistemático. Por lo mismo, en este modelo se utilizan los valores de los precios de los activos lo largo del tiempo y utiliza la intuición con la que derivamos la ecuación lineal con los Mínimos cuadrados ordinarios (MCO).En la ecuación (4.1)\\(R_{jt}\\) es el retorno del portafolio \\(j\\) en el tiempo \\(t\\)\\(R_{jt}\\) es el retorno del portafolio \\(j\\) en el tiempo \\(t\\)\\(R_{ft}\\) es el retorno de un bono sin riesgo gubernamental en un año. Parecido los CETES.\\(R_{ft}\\) es el retorno de un bono sin riesgo gubernamental en un año. Parecido los CETES.\\(R_{mt}\\) es el retorno en un portafolio de mercado.\\(R_{mt}\\) es el retorno en un portafolio de mercado.\\(u_{jt}\\) es el retorno en un portafolio de mercado.\\(u_{jt}\\) es el retorno en un portafolio de mercado.\\(\\alpha_{j},\\beta_j\\) son los coeficientes que queremos obtener.\\(\\alpha_{j},\\beta_j\\) son los coeficientes que queremos obtener.De esta manera, \\(\\alpha_j\\) es el coeficiente que más nos interesa debido que queremos ver si el activo supera o el index del mercado con base en el activo fijo.Si \\(\\alpha_j\\) es positivo entonces sabemos que el retorno tiene buenos rendimiendtos y uno negativo significa que . Por tanto \\(H_0:\\alpha_j=0\\)","code":""},{"path":"capital-asset-pricing-model-capm.html","id":"estimación-r-1","chapter":"4 Capital Asset Pricing Model (CAPM)","heading":"4.2 Estimación R","text":"Para la estimación utilizaremos el paquete “BatchGetSymbols”. Este paquete nos permitirá descargar información acerca de la bolsa de valores internacional.","code":""},{"path":"capital-asset-pricing-model-capm.html","id":"estimación","chapter":"4 Capital Asset Pricing Model (CAPM)","heading":"4.3 ESTIMACIÓN","text":"","code":""},{"path":"capital-asset-pricing-model-capm.html","id":"dependencias-1","chapter":"4 Capital Asset Pricing Model (CAPM)","heading":"4.3.1 Dependencias","text":"","code":"\n#install.packages(\"pacman\")\n#pacman nos permite cargar varias librerias en una sola línea\nlibrary(pacman)\npacman::p_load(tidyverse,BatchGetSymbols,ggplot2,lubridate,readxl,tidyquant)"},{"path":"capital-asset-pricing-model-capm.html","id":"descarga-de-los-valores-1","chapter":"4 Capital Asset Pricing Model (CAPM)","heading":"4.3.2 Descarga de los valores","text":"\nFigure 4.1: Relación de excesos de retornos entre AMZN y SP500\n\nFigure 4.2: Relación de excesos de retornos entre TSLA y SP500\nDe esta manera sabemos que el rendimiento de TSLA es mayor debido que el coeficiente \\(\\alpha=-2.9534\\), lo cual indica peores rendimientos al resto del SP500.","code":"\n#Primero determinamos el lapso de tiempo\npd<-as.Date(\"2021/09/18\") #primer fecha\npd\n#> [1] \"2021-09-18\"\nld<-as.Date(\"2022/09/18\") #última fecha\nld\n#> [1] \"2022-09-18\"\n#Intervalos de tiempo\nint<-\"monthly\"\n#Datos a elegir\ndt<-c(\"AMZN\")\ndt2<-c(\"TSLA\")\n#Descargando los valores\n?BatchGetSymbols()\ndata1<- BatchGetSymbols(tickers = dt,\n                       first.date = pd,\n                       last.date = ld,\n                       freq.data = int,\n                       do.cache = FALSE,\n                       thresh.bad.data = 0)\ndata2<- BatchGetSymbols(tickers = dt2,\n                       first.date = pd,\n                       last.date = ld,\n                       freq.data = int,\n                       do.cache = FALSE,\n                       thresh.bad.data = 0)\n\n#Generando data frame con los valores\ndata_precio_amzn<-data1$df.tickers\ncolnames(data_precio_amzn)\n#>  [1] \"ticker\"              \"ref.date\"           \n#>  [3] \"volume\"              \"price.open\"         \n#>  [5] \"price.high\"          \"price.low\"          \n#>  [7] \"price.close\"         \"price.adjusted\"     \n#>  [9] \"ret.adjusted.prices\" \"ret.closing.prices\"\ndata_precio_tls<-data2$df.tickers\ncolnames(data_precio_tls)\n#>  [1] \"ticker\"              \"ref.date\"           \n#>  [3] \"volume\"              \"price.open\"         \n#>  [5] \"price.high\"          \"price.low\"          \n#>  [7] \"price.close\"         \"price.adjusted\"     \n#>  [9] \"ret.adjusted.prices\" \"ret.closing.prices\"\n#necesitamos convertir la serie de tiempo de precios en retornos continuos compuestos de los precios de apertura\ndata_precio_amzn$ccrAMZN<-c(NA ,100*diff(log(data_precio_amzn$price.open)))#agregamos un valor NA al principio\ndata_precio_amzn$ccrAMZN#estos son los retornos\n#>  [1]          NA  -3.2011678   2.1889913   5.3061639\n#>  [5]  -5.6279333 -11.0646538   1.8052718   7.2089622\n#>  [9] -29.3475086  -0.1185366 -13.9945965  23.8807273\n#> [13]  -6.8696583\n\ndata_precio_tls$ccrTSLA<-c(NA ,100*diff(log(data_precio_tls$price.open)))#agregamos un valor NA al principio\ndata_precio_tls$ccrTSLA\n#>  [1]         NA   5.796888  38.591933   1.361865  -1.121972\n#>  [6] -20.478772  -7.264574  21.765521 -22.795320 -13.089771\n#> [11] -10.336735  28.307900 -10.009692\n#formateando por año y mes\ndata_precio_tls$ref.date=format(as.Date(data_precio_tls$ref.date), \"%m/%Y\")\ndata_precio_amzn$ref.date=format(as.Date(data_precio_amzn$ref.date), \"%m/%Y\")\n#Compararemos con los CETES\nCETES_sep2021_2022<-read_excel(\"BD/CETES-sep2021-2022.xlsx\", skip=17)\nhead(CETES_sep2021_2022)\n#> # A tibble: 6 × 2\n#>   Fecha               SF43936\n#>   <dttm>                <dbl>\n#> 1 2021-09-15 00:00:00    4.6 \n#> 2 2021-09-23 00:00:00    4.58\n#> 3 2021-09-30 00:00:00    4.69\n#> 4 2021-10-07 00:00:00    4.81\n#> 5 2021-10-14 00:00:00    4.79\n#> 6 2021-10-21 00:00:00    4.83\n#indice sp500\nSP500 <- read_csv(\"BD/Download Data - INDEX_US_S&P US_SPX.csv\")\nSP500$ccrSP500<-c(NA ,100*diff(log(SP500$Open)))\nnames(SP500)[1]<-paste('ref.date')\n#formateando por año y mes\n\n#cetes\ncete_1_año<-10.10#esto es el rendimiento a un año de un cete gubernamental seguro\n\n#Juntamos el df\nCAPM_2<-merge(data_precio_amzn, data_precio_tls, by = c('ref.date'))\nCAPM_4<-merge(SP500, CAPM_2, by = c('ref.date'))\nCAPM<-data.frame(CAPM_4)\n\n#exceso de retorno\nCAPM$excess_ret_AMZN<-CAPM$ccrAMZN-cete_1_año\nCAPM$excess_ret_SP500<-CAPM$ccrSP500-cete_1_año\nCAPM$excess_ret_TSLA<-CAPM$ccrTSLA-cete_1_año\n#relacion entre los excesos de demanda\nggplot(CAPM, aes(x=excess_ret_AMZN, y=excess_ret_SP500))+geom_point()+labs(title=\"Relación de excesos de retornos entre TSLA y AMZN\",y=\"Exceso de demanda de SP500\", x=\"Exceso de demanda de AMZN\")+theme_light()\n#> Warning: Removed 2 rows containing missing values\n#> (`geom_point()`).\n#relacion entre los excesos de demanda\nggplot(CAPM, aes(x=excess_ret_TSLA, y=excess_ret_SP500))+geom_point()+labs(title=\"Relación de excesos de retornos entre TSLA y AMZN\",y=\"Exceso de demanda de SP500\", x=\"Exceso de demanda de TSLA\")+theme_light()\n#> Warning: Removed 2 rows containing missing values\n#> (`geom_point()`).\n#veamos la regresion lineal\nCAPM_lr<-lm(excess_ret_TSLA~excess_ret_SP500,data = CAPM)\nsummary(CAPM_lr)\n#> \n#> Call:\n#> lm(formula = excess_ret_TSLA ~ excess_ret_SP500, data = CAPM)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -23.980 -13.391  -5.548  11.572  37.067 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)       -3.2334    11.8097  -0.274     0.79\n#> excess_ret_SP500   0.5379     1.0789   0.499     0.63\n#> \n#> Residual standard error: 20.88 on 9 degrees of freedom\n#>   (2 observations deleted due to missingness)\n#> Multiple R-squared:  0.02687,    Adjusted R-squared:  -0.08125 \n#> F-statistic: 0.2486 on 1 and 9 DF,  p-value: 0.6301\nalpha1<-coefficients(CAPM_lr)[1]\nalpha1<0\n#> (Intercept) \n#>        TRUE"},{"path":"capital-asset-pricing-model-capm.html","id":"ejercicio-compara-con-tsla-con-el-apple","chapter":"4 Capital Asset Pricing Model (CAPM)","heading":"4.4 Ejercicio Compara con TSLA con el APPLE","text":"","code":"\ndt3<-\"AAPL\"\ndata3<-BatchGetSymbols(tickers = dt3,\n                       first.date = pd,\n                       last.date = ld,\n                       freq.data = int,\n                       do.cache = FALSE,\n                       thresh.bad.data = 0)\n#> \n#> Running BatchGetSymbols for:\n#>    tickers =AAPL\n#>    Downloading data for benchmark ticker\n#> ^GSPC | yahoo (1|1)\n#> AAPL | yahoo (1|1) - Got 100% of valid prices | Mais contente que cusco de cozinheira!\ndata_precio_AAPL<-data3$df.tickers\ncolnames(data_precio_AAPL)\n#>  [1] \"ticker\"              \"ref.date\"           \n#>  [3] \"volume\"              \"price.open\"         \n#>  [5] \"price.high\"          \"price.low\"          \n#>  [7] \"price.close\"         \"price.adjusted\"     \n#>  [9] \"ret.adjusted.prices\" \"ret.closing.prices\"\ndata_precio_AAPL$ccrAAPL<-c(NA ,100*diff(log(data_precio_AAPL$price.open)))#agregamos un valor NA al principio\ndata_precio_AAPL$ccrAAPL\n#>  [1]         NA  -1.330092   4.875668  11.698469   5.996413\n#>  [6]  -2.171531  -5.498712   5.510207 -10.483068  -4.442864\n#> [11]  -9.701946  16.851754  -2.751627\ndata_precio_AAPL$ref.date=format(as.Date(data_precio_AAPL$ref.date), \"%m/%Y\")\nCAPM_3<-merge(data_precio_AAPL, CAPM, by = c('ref.date'))\nCAPM_3$excess_ret_AAPL<-CAPM_3$ccrAAPL-cete_1_año\n#veamos la regresion lineal\nCAPM3_lr<-lm(excess_ret_AAPL~excess_ret_SP500,data = CAPM_3)\nsummary(CAPM3_lr)\n#> \n#> Call:\n#> lm(formula = excess_ret_AAPL ~ excess_ret_SP500, data = CAPM_3)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -10.9038  -5.4365   0.4641   3.4629  14.1798 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)       -4.7542     4.9105  -0.968    0.358\n#> excess_ret_SP500   0.4663     0.4486   1.039    0.326\n#> \n#> Residual standard error: 8.682 on 9 degrees of freedom\n#>   (2 observations deleted due to missingness)\n#> Multiple R-squared:  0.1072, Adjusted R-squared:  0.007964 \n#> F-statistic:  1.08 on 1 and 9 DF,  p-value: 0.3258\nalpha2<-coefficients(CAPM3_lr)[1]\nalpha2<0\n#> (Intercept) \n#>        TRUE"},{"path":"estacionariedad.html","id":"estacionariedad","chapter":"5 Estacionariedad","heading":"5 Estacionariedad","text":"","code":""},{"path":"estacionariedad.html","id":"el-problema-4","chapter":"5 Estacionariedad","heading":"5.1 El problema","text":"Los fundamentos de las series de tiempo están basados en la\nestacionalidad. Una serie de tiempo \\({r_t}\\) que estudia los retornos de\nun activo lo largo de tiempo es estrictamente estacionaria si la\ndistribución conjunta de los retornos \\((r_{t1},\\dots,r_{t1})\\) es\nexactamente idéntica en \\((r_{t1+T},\\dots,r_{t1+T})\\), es decir cuando\npasa \\(T\\) años, por ejemplo. En otras palabras, definiremos una serie\nde tiempo como un vector de variables \\({X_t}\\) aleatorias de dimensión\n\\(T\\), dado como:\\[\\begin{equation}\n    X_1, X_2, X_3, \\ldots ,X_T\n\\end{equation}\\]Es decir, definiremos una serie de tiempo como una\nrealización de un proceso estocástico –o un Proceso Generador de Datos\n(PGD). Consideremos una muestra de los múltiples posibles resultados de\nmuestras de tamaño \\(T\\), la colección dada por:\\[\\begin{equation}\n    \\{X^{(1)}_1, X^{(1)}_2, \\ldots, X^{(1)}_T\\}\n    \\tag{5.1}\n\\end{equation}\\]Eventualmente podríamos estar dispuestos observar este proceso\nindefinidamente, de forma tal que estemos interesados en observar la\nsecuencia dada por \\(\\{ X^{(1)}_t \\}^{\\infty}_{t = 1}\\), lo cual \ndejaría se ser sólo una de las tantas realizaciones o secuencias del\nproceso estocástico original de la ecuación (5.1).Por lo mismo, cada cambio que se hace al vector \\(\\{ X^{(1)}_t \\}\\) es\nparte del mismo proceso estocástico, por lo que la serie de tiempo es:El proceso estocástico de dimensión \\(T\\) puede ser completamente descrito\npor su función de distribución multivariada de dimensión \\(T\\). \nobstante, sólo nos enfocaremos en sus primer y segundo momentos, es\ndecir, en sus medias o valores esperados \\(\\mathbb{E} (X_t)\\)Para \\(t = 1, 2, \\ldots, T\\):De sus variazas:\\[\\begin{equation*}\n    Var[X_t] = \\mathbb{E}[(X_t - \\mathbb{E}[X_t])^2]\n\\end{equation*}\\] Para \\(t = 1, 2, \\ldots, T\\), y de sus \\(T(T-1)/2\\)\ncovarianzas: \\[\\begin{equation*}\n    Cov[X_t,X_s] = \\mathbb{E}[(X_t - \\mathbb{E}[X_t])(X_s - \\mathbb{E}[X_s])]\n\\end{equation*}\\]Para \\(t < s\\). Por lo tanto, en la forma matricial podemos escribir lo siguiente:\n\\[\\begin{equation*}\n\\left[\n    \\begin{array}{c c c c}\n    Var[X_1] & Cov[X_1,X_2] & \\cdots & Cov[X_1,X_T] \\\\\n    Cov[X_2,X_1] & Var[X_2] & \\cdots & Cov[X_2,X_T] \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    Cov[X_T,X_1] & Cov[X_T,X_2] & \\cdots & Var[X_T] \\\\\n    \\end{array}\n\\right]\n\\end{equation*}\\]\\[\\begin{equation}\n= \\left[\n    \\begin{array}{c c c c}\n    \\sigma_1^2 & \\rho_{12} & \\cdots & \\rho_{1T} \\\\\n    \\rho_{21} & \\sigma_2^2 & \\cdots & \\rho_{2T} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\rho_{T1} & \\rho_{T2} & \\cdots & \\sigma_T^2 \\\\\n    \\end{array}\n\\right]\n\\tag{5.2}\n\\end{equation}\\]Donde es claro que en la matriz de la ecuación (5.2) existen \\(T(T-1)/2\\) covarianzas distintas, ya que se cumple que \\(Cov[X_t,X_s] = Cov[X_s,X_t]\\), para \\(t \\neq s\\). menudo, esas covarianzas son denominadas como autocovarianzas puesto que ellas son covarianzas entre variables aleatorias pertenecientes al mismo proceso estocástico pero en un momento \\(t\\) diferente. Si el proceso estocástico tiene una distribución normal multivariada, su función de distribución estará totalmente descrita por sus momentos de primer y segundo orden.","code":""},{"path":"estacionariedad.html","id":"ergocidad","chapter":"5 Estacionariedad","heading":"5.1.1 Ergocidad","text":"Esto implica que los momentos muestrales, los cuales son calculados en la base de una serie de tiempo con un número finito de observaciones, conforme el tiempo \\(T \\rightarrow \\infty\\) sus correspondientes momentos muestrales, tienden los verdaderos valores poblacionales, los cuales definiremos como \\(\\mu\\), para la media, y \\(\\sigma^2_X\\) para la varianza. En pocas palabras, conforme los momentos muestrales aumenten tanto que tiendan al infinito, entonces nos acercamos valores poblacionales de la media y la varianza.\nEste concepto sólo es cierto si asumimos que\\[\\begin{eqnarray*}\n    \\mathbb{E}[X_t] = \\mu_t = \\mu \\\\\n    Var[X_t] = \\sigma^2_X\n\\end{eqnarray*}\\]\nMás formalmente, se dice que el PGD o el proceso estocástico es ergódico en la media si:\n\\[\\begin{equation}\n    \\displaystyle\\lim_{T \\\\infty}{\\mathbb{E} \\left[ \\left( \\frac{1}{T} \\sum^{T}_{t = 1} (X_t - \\mu) \\right) ^2 \\right]} = 0\n\\end{equation}\\]y ergódico en la varianza si:\n\\[\\begin{equation}\n    \\displaystyle\\lim_{T \\\\infty}{\\mathbb{E} \\left[ \\left( \\frac{1}{T} \\sum^{T}_{t = 1} (X_t - \\mu) ^2 - \\sigma^2_X \\right) ^2 \\right]} = 0\n\\end{equation}\\]\n+\nEstas condiciones se les conoce como propiedades de consistencia para las variables aleatorias. Sin embargo, éstas pueden ser probadas. Por ello se les denomina como un supuesto que pueden cumplir algunas de las series. Más importante aún: un proceso estocástico que tiende estar en equilibrio estadístico en un orden ergódico, es estacionario.","code":""},{"path":"estacionariedad.html","id":"tipos-de-estacionariedad","chapter":"5 Estacionariedad","heading":"5.1.2 Tipos de Estacionariedad","text":"Definiremos la estacionariedad por sus momentos del correspondiente proceso estocástico dado por \\(\\{X_t\\}\\):Estacionariedad en media: Un proceso estocástico es estacionario en media si \\(E[X_t] = \\mu_t = \\mu\\) es constante para todo \\(t\\).Estacionariedad en media: Un proceso estocástico es estacionario en media si \\(E[X_t] = \\mu_t = \\mu\\) es constante para todo \\(t\\).Estacionariedad en varianza: Un proceso estocástico es estacionario en varianza si \\(Var[X_t] = \\mathbb{E}[(X_t - \\mu_t)^2] = \\sigma^2_X = \\gamma(0)\\) es constante y finita para todo \\(t\\).Estacionariedad en varianza: Un proceso estocástico es estacionario en varianza si \\(Var[X_t] = \\mathbb{E}[(X_t - \\mu_t)^2] = \\sigma^2_X = \\gamma(0)\\) es constante y finita para todo \\(t\\).Estacionariedad en covarianza: Un proceso estocástico es estacionario en covarianza si \\(Cov[X_t,X_s] = \\mathbb{E}[(X_t - \\mu_t)(X_s - \\mu_s)] = \\gamma(|s-t|)\\) es sólo una función del tiempo y de la distancia entre las dos variables aleatorias. Por lo que depende del tiempo denotado por \\(t\\) (depende de la información contemporánea).Estacionariedad en covarianza: Un proceso estocástico es estacionario en covarianza si \\(Cov[X_t,X_s] = \\mathbb{E}[(X_t - \\mu_t)(X_s - \\mu_s)] = \\gamma(|s-t|)\\) es sólo una función del tiempo y de la distancia entre las dos variables aleatorias. Por lo que depende del tiempo denotado por \\(t\\) (depende de la información contemporánea).Estacionariedad débil: Como la estacionariedad en varianza resulta de forma inmediata de la estacionariedad en covarianza cuando se asume que \\(s = t\\), un proceso estocástico es débilmente estacionario cuando es estacionario en media y covarianza. ESTE ES EL MÁS COMÚN Y POSIBLE, por lo que es el que estudiaremos.Estacionariedad débil: Como la estacionariedad en varianza resulta de forma inmediata de la estacionariedad en covarianza cuando se asume que \\(s = t\\), un proceso estocástico es débilmente estacionario cuando es estacionario en media y covarianza. ESTE ES EL MÁS COMÚN Y POSIBLE, por lo que es el que estudiaremos.","code":""},{"path":"estacionariedad.html","id":"función-de-autocorrelación-acf","chapter":"5 Estacionariedad","heading":"5.1.3 Función de Autocorrelación (ACF)","text":"Para ampliar la discusión, es posible calcular la fuerza o intensidad de la dependencia de las variables aleatorias dentro de un proceso estocástico, ello mediante el uso de las autocovarianzas. Cuando las covarianzas son normalizadas respecto de la varianza, el resultado es un término que es independiente de las unidad de medida aplicada, y se conoce como la función de autocorrelación.Por su parte, un estimador consistente de la función de autocorrelación estará dado por:\n\\[\\begin{equation}\n    \\hat{\\rho}(\\tau) = \\frac{\\sum^{T - \\tau}_{t=1} (X_t - \\hat{\\mu})(X_{t+\\tau} - \\hat{\\mu})}{\\sum^T_{t=1} (X_t - \\hat{\\mu})^2} = \\frac{\\hat{\\gamma}(\\tau)}{\\hat{\\gamma}(0)} \\mbox{, para } \\tau = 1, 2, \\ldots, T-1\n    \\tag{5.3}\n\\end{equation}\\]El estimador de la ecuación (5.3) es asintóticamente insesgado y es relevante puesto que nos dice si una serie de tiempo con estacionariedad débil esta serialmente correlacionada si y solo si \\(\\hat{\\rho}(\\tau)\\neq0\\).","code":""},{"path":"estacionariedad.html","id":"ruido-blanco","chapter":"5 Estacionariedad","heading":"5.1.4 Ruido Blanco","text":"Supongamos una serie de tiempo denotada por: \\(\\{U_t\\}^T_{t = 0}\\). Decimos que el proceso estocástico \\(\\{U_t\\}\\) es un proceso estocástico puramente aleatorio o es un proceso estocástico de ruido blanco o caminata aleatoria, si éste tiene las siguientes propiedades:\\(\\mathbb{E}[U_t] = 0\\), \\(\\forall t\\)\\(\\mathbb{E}[U_t] = 0\\), \\(\\forall t\\)\\(Var[U_t] = \\mathbb{E}[(U_t - \\mu_t)^2] = \\mathbb{E}[(U_t - \\mu)^2] = \\mathbb{E}[(U_t)^2] = \\sigma^2\\), \\(\\forall t\\)\\(Var[U_t] = \\mathbb{E}[(U_t - \\mu_t)^2] = \\mathbb{E}[(U_t - \\mu)^2] = \\mathbb{E}[(U_t)^2] = \\sigma^2\\), \\(\\forall t\\)\\(Cov[U_t,U_s] = \\mathbb{E}[(U_t - \\mu_t)(U_s - \\mu_s)] = \\mathbb{E}[(U_t - \\mu)(U_s - \\mu)] = \\mathbb{E}[U_t U_s] = 0\\), \\(\\forall t \\neq s\\).\\(Cov[U_t,U_s] = \\mathbb{E}[(U_t - \\mu_t)(U_s - \\mu_s)] = \\mathbb{E}[(U_t - \\mu)(U_s - \\mu)] = \\mathbb{E}[U_t U_s] = 0\\), \\(\\forall t \\neq s\\).\\(\\hat{\\rho}(\\tau)=0\\)\\(\\hat{\\rho}(\\tau)=0\\)En palabras. Un proceso \\(U_t\\) es un ruido blanco si su valor promedio es cero (0), tiene una varianza finita y constante, y además le importa la historia pasada, así su valor presente se ve influenciado por sus valores pasados importando respecto de que periodo se tome referencia.Para procesos estacionarios, dicha función de autocorrelación esta dada por:\n\\[\\begin{equation}\n    \\rho(\\tau) = \\frac{\\mathbb{E}[(X_t - \\mu)(X_{t+\\tau} - \\mu)]}{\\mathbb{E}[(X_t - \\mu)^2]} = \\frac{\\gamma(\\tau)}{\\gamma(0)}\n\\end{equation}\\]","code":""},{"path":"estacionariedad.html","id":"estimación-1","chapter":"5 Estacionariedad","heading":"5.2 Estimación","text":"","code":""},{"path":"estacionariedad.html","id":"dependencias-2","chapter":"5 Estacionariedad","heading":"5.2.1 Dependencias","text":"","code":"\n#install.packages(\"pacman\")\n#pacman nos permite cargar varias librerias en una sola línea\nlibrary(pacman)\npacman::p_load(tidyverse,BatchGetSymbols,ggplot2,lubridate,readxl,forecast,stats)"},{"path":"estacionariedad.html","id":"caminata","chapter":"5 Estacionariedad","heading":"5.3 Caminata","text":"\nFigure 5.1: Ejemplo de 10 trayectorias de la caminata aleatoria, cuando sólo es posible cambios de +1 y -1\nAsí, el proceso estocástico dado por la caminata alaeatoria sin un\ntérmino de ajuste es estacionario en media, pero en varianza o en\ncovarianza, y consecuentemente, en general estacionario, condición\nque contraria al caso del proceso simple descrito en \\(U_t\\).Es facil ver que muchas de las posibilidades de realización de este\nproceso estocástico (series de tiempo) pueden tomar cualquiera de las\nrutas consideradas en el Figura 5.1. Ahora analicemos\nun solo camino.","code":"\n\nset.seed(1234)\n# Utilizaremos una función guardada en un archivo a parte\n# Llamamos a la función:\nsource(\"funciones/Caminata.R\")\n\n# Definimos argumentos de la función\nOpciones <- c(-1, 1)\n#\nSoporte <- 10000\n\n# Vamos a réplicar el proceso con estos parámetros\nRango <- 200\n#\nCaminos <- 10\n\n#\n\nfor(i in 1:Caminos){\n  TT <- data.matrix(data.frame(Caminata(Opciones, Soporte)[1]))\n  #\n  G_t <- data.matrix(data.frame(Caminata(Opciones, Soporte)[2]))\n  #\n  plot(TT, G_t, col = \"blue\", type = \"l\", ylab = \"Ganancias\", xlab = \"Tiempo\", ylim = c(-Rango,Rango))\n  #\n  par(new = TRUE)\n  #\n  i <- i +1\n}\n#\npar(new = FALSE)"},{"path":"estacionariedad.html","id":"un-camino","chapter":"5 Estacionariedad","heading":"5.4 Un camino","text":"\nFigure 5.2: Una Caminata aleatoria cuando sólo es posible cambios de +1 y -1\nHay que convertirlo serie de tiempo","code":"\n#Generamos datos\n  TT1 <- data.matrix(data.frame(Caminata(Opciones, Soporte)[1]))\n  G_t1 <- data.matrix(data.frame(Caminata(Opciones, Soporte)[2]))\n#Creemos un data frame\n  dt_caminata<-data.frame(TT1,G_t1)\n  colnames(dt_caminata)<-c(\"t\",\"ganancias\")\n  head(dt_caminata)\n#>   t ganancias\n#> 1 1        -1\n#> 2 2        -2\n#> 3 3        -1\n#> 4 4        -2\n#> 5 5        -3\n#> 6 6        -4\n#plot\n  plot(TT1, G_t1, col = \"blue\", type = \"l\", ylab = \"Ganancias\", xlab = \"Tiempo\", ylim = c(-Rango,Rango))\n#serie de tiempo\ncaminata_ts<-ts(G_t1,start=1,end=Soporte)"},{"path":"estacionariedad.html","id":"estacionariedad-caminata","chapter":"5 Estacionariedad","heading":"5.4.1 Estacionariedad Caminata","text":"\nFigure 5.3: Función de Autocorrelación de una Caminata\nComo se comentó con anterioridad en la Figura 5.3 es\nevidente que la Caminata si tiene autocorrelacion, por lo que nuestro\nplot de autocorrelacion tiene valores muy altos en todos los lags.\nVeamos los lags.\nFigure 5.4: Lags de una sola caminata\nDe nuevo, esto al ser creado de manera estandarizada estamos seguros de\nque va ser estacionario en la medio, por lo mismo los lags de la\nFigura 5.4 se ven tan correlacionados.","code":"\nACF_caminata_ts<-acf(caminata_ts,na.action = na.pass, main = \"Función de Autocorrelación de una Caminata\")\ngglagplot(caminata_ts,lags=10,do.lines=FALSE,colour=FALSE)+theme_light()"},{"path":"estacionariedad.html","id":"precios-de-un-activo","chapter":"5 Estacionariedad","heading":"5.5 Precios de un activo","text":"Veamos la serie de tiempo\nFigure 5.5: Serie de tiempo de los retornos de año en los últimos 20 años\n","code":"\n#Primero determinamos el lapso de tiempo\npd<-Sys.Date()-(365*20) #primer fecha\npd\n#> [1] \"2002-11-27\"\nld<-Sys.Date() #última fecha\nld\n#> [1] \"2022-11-22\"\n#Intervalos de tiempo\nint<-\"monthly\"\n#Datos a elegir\ndt<-c(\"AMZN\")\ndt2<-c(\"TSLA\")\n#Descargando los valores\ndata1<- BatchGetSymbols(tickers = dt,\n                       first.date = pd,\n                       last.date = ld,\n                       freq.data = int,\n                       do.cache = FALSE,\n                       thresh.bad.data = 0)\n#Generando data frame con los valores\ndata_precio_amzn<-data1$df.tickers\ncolnames(data_precio_amzn)\n#>  [1] \"ticker\"              \"ref.date\"           \n#>  [3] \"volume\"              \"price.open\"         \n#>  [5] \"price.high\"          \"price.low\"          \n#>  [7] \"price.close\"         \"price.adjusted\"     \n#>  [9] \"ret.adjusted.prices\" \"ret.closing.prices\"\n\n#necesitamos convertir la serie de tiempo de precios en retornos continuos compuestos de los precios de apertura\ndata_precio_amzn$ccrAMZN<-c(NA ,100*diff(log(data_precio_amzn$price.open)))#agregamos un valor NA al principio\ndata_precio_amzn$ccrAMZN#estos son los retornos\n#>   [1]           NA   1.79658060 -22.98950701  13.39221448\n#>   [5]   0.95260416  14.27998202  11.55626991  24.11122449\n#>   [9]  -0.46684144  13.08785513  11.63599301   3.89974592\n#>  [13]  12.48104071  -0.73260401  -3.06108260  -4.08140972\n#>  [17] -16.32741069   0.99457279   0.06902105   9.61879412\n#>  [21]  11.67844638 -33.59147712  -0.57381482   7.69997922\n#>  [25] -18.78100714  15.60691856  11.66713068  -4.43506452\n#>  [29] -20.41392362  -1.23405211  -6.96531282   9.64353557\n#>  [33]  -6.77486160  30.02382912  -5.40177077   6.39945115\n#>  [37] -12.58398922  20.12391421  -2.92703823  -7.77281354\n#>  [41] -15.93630872  -2.10477279  -4.11970307  -1.60415929\n#>  [45]  10.64572285 -37.21478392  15.01070027   3.59739571\n#>  [49]  17.58906665   5.43570463  -4.00357496  -1.90531667\n#>  [53]   3.54637914   1.33891100  42.77167396  11.98170332\n#>  [57]  -0.13070948  12.66409736   2.27857959  15.63296023\n#>  [61]  -6.26135927   2.56510858   5.74113839 -18.78533471\n#>  [65] -21.72447597  13.78662202   7.15014819   3.44753666\n#>  [69] -11.63053849   5.54650897   8.53074641 -14.71605773\n#>  [73] -24.20236452 -29.39126222  20.09953181  13.15576837\n#>  [77]   8.77225235  13.27882326   9.60320128  -2.73678724\n#>  [81]   7.64068237   2.50334747  -6.96036997  13.59745291\n#>  [85]  24.90536163  14.32806129  -0.50514401 -10.08447333\n#>  [89]  -3.70473986  13.45839135   1.02565002  -9.33660068\n#>  [93] -13.76436786   8.99531736   5.87517724  21.76202538\n#>  [97]   4.58513429   8.56726887   1.22598823  -6.16865517\n#> [101]   1.74979032   4.53458328   7.93222740  -0.25978671\n#> [105]   4.72685785   9.04110889  -4.41608958   0.80039290\n#> [109]  -4.18766494  -8.13529694  -8.68550170  -1.18960511\n#> [113]   3.43828044   9.60224827  14.70991690  -9.58159747\n#> [117]   9.53799598   2.08880372   5.85976366   2.83140800\n#> [121]  -8.65274050   7.52661134   1.39202437   4.89612270\n#> [125]  -2.12710012   1.39936277  -5.02332605   5.76221809\n#> [129]   3.66491122   8.27850152  -6.24554343   9.85520147\n#> [133]  15.15285156   8.73395739  -0.05013788 -10.51934673\n#> [137]  -0.06687288  -5.92858190 -10.58568315   2.74371861\n#> [141]   4.15753522  -3.80625428   8.04816141  -5.42111518\n#> [145]  -5.03065911   9.90317538  -7.85404256  11.32156221\n#> [149]   8.43295383  -2.32429615  13.01462014   1.54061721\n#> [153]   2.05813986  20.15392877  -7.39490376   2.34828935\n#> [157]  20.47843365   7.17052347  -2.62563883 -12.67694180\n#> [161]  -3.85435390   5.96629237  11.72089295   8.23387458\n#> [165]  -0.49783030   5.76252931   1.44112456   8.10699776\n#> [169]  -4.52676184  -6.00796092   0.72964776   8.98955770\n#> [173]   2.83447462   4.01536256   4.38443827   7.35281333\n#> [177]  -2.61760725   2.36894617  -1.20285851  -2.07377928\n#> [181]  13.68712240   5.85471090  -0.00427124  20.93976645\n#> [185]   4.63815978  -6.55115525   9.77684681   4.61358018\n#> [189]   2.75160333   5.84583306  12.74521370  -0.22279328\n#> [193] -21.94794383   8.60716489 -18.86826361  11.20213025\n#> [197]   0.98664739   8.39682297   7.12720047  -9.38002536\n#> [201]   8.85565506  -2.70183034  -5.58782369  -1.36520548\n#> [205]   2.37757442   0.91249016   3.83805218   6.98245156\n#> [209]  -5.31693095   1.37938043  18.97247680   4.64889431\n#> [213]  11.92308161  14.25393454   9.27398656  -8.41337555\n#> [217]  -4.66642316   4.05671846   2.52393793  -0.84885499\n#> [221]  -3.59427728  -0.31861156  11.12179968  -7.17375312\n#> [225]   5.72503641  -2.40180912   4.18486227  -6.11473001\n#> [229]   2.18899134   5.30616390  -5.62793333 -11.06465380\n#> [233]   1.80527180   7.20896224 -29.34750864  -0.11853658\n#> [237] -13.99459654  23.88072732  -6.86965832 -10.37744549\n#> [241]  -8.82127306\n#tenemos 20 retornos a lo largo de 20 años\nret_20_amazn<-ggplot(data=data_precio_amzn, aes(x=ref.date))+geom_line(aes(y=ccrAMZN))+labs(title=\"Retornos de AMZN en los últimos 20 años\",y=\"Retornos\", x=\"Años\")+theme_light()\nret_20_amazn"},{"path":"estacionariedad.html","id":"serie-de-tiempo","chapter":"5 Estacionariedad","heading":"5.5.1 Serie de tiempo","text":"Primero que nada es importante cargar los datos un objeto series de\ntiempo. Esto nos lo permite la función ts(). Además debemos serciorarnos\nde que los datos esten en orden cronológico.","code":"\ndata_precio_amzn<-data_precio_amzn[order(data_precio_amzn$ref.date),]\nhead(data_precio_amzn)#dado que ya estaba en orden cronológico nuestro df no cambia\n#> # A tibble: 6 × 11\n#>   ticker ref.date     volume price…¹ price…² price…³ price…⁴\n#>   <chr>  <date>        <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n#> 1 AMZN   2002-11-27   1.65e8   1.19     1.22   1.17    1.17 \n#> 2 AMZN   2002-12-02   3.11e9   1.21     1.25   0.922   0.944\n#> 3 AMZN   2003-01-02   3.38e9   0.960    1.16   0.928   1.09 \n#> 4 AMZN   2003-02-03   2.32e9   1.10     1.12   0.980   1.10 \n#> 5 AMZN   2003-03-03   3.28e9   1.11     1.40   1.07    1.30 \n#> 6 AMZN   2003-04-01   4.39e9   1.28     1.46   1.21    1.43 \n#> # … with 4 more variables: price.adjusted <dbl>,\n#> #   ret.adjusted.prices <dbl>, ret.closing.prices <dbl>,\n#> #   ccrAMZN <dbl>, and abbreviated variable names\n#> #   ¹​price.open, ²​price.high, ³​price.low, ⁴​price.close\n#hagamos el objeto ts\nret_amazn_ts<-ts(data_precio_amzn$ccrAMZN)\nplot(ret_amazn_ts)#de esta manera podemos ver que se cargo bien debido a que es igual al ggplot"},{"path":"estacionariedad.html","id":"estacionariedad-1","chapter":"5 Estacionariedad","heading":"5.5.2 Estacionariedad","text":"\nFigure 5.6: Lag Plot que nos muestra la correlación entre 20 lags\n\nFigure 5.7: Función de Autocorrelación de los retornos de AMZN en los ultimos 20 años\nLa Figura 5.6 nos idica la manera en la que se\ncorrelacionan los lags, evidentemente se puede ver ningún tipo de\ncorrelacioo1ón visible. Similarmente la Figura 5.7 en\ndonde se muestra la función de autocorrelación. Expecto al primer lag\n–que muestra correlacion debido que se esta comparando consigo\nmismo– es evidente que hay correlacioo1ón fuerte entre ninguno de\nlos lags. Por lo mismo, sería difícil poder encontrar y estimar valores\nfuturos debido que la Figura 5.6 y la Figura\n5.7 indican que la serie de tiempo de los retornos de\nAMZN de la Figura 5.5 es completamente aleatorio y \nhay estacionariedad.","code":"\n#MA_m5<-forecast::ma(ret_amazn_ts,order=11,centre=TRUE)\n#plot(ret_amazn_ts)+lines(MA_m5, col=\"red\", lwd=2)\ngglagplot(ret_amazn_ts,lags=20,do.lines=FALSE,colour=FALSE)+theme_light()\nACF_ret_amazn_ts<-acf(ret_amazn_ts,na.action = na.pass)"},{"path":"procesos-estacionarios-univariados.html","id":"procesos-estacionarios-univariados","chapter":"6 Procesos estacionarios univariados","heading":"6 Procesos estacionarios univariados","text":"En este capítulo analizaremos el método o metodología de análisis de\nseries de tiempo propuesto por Box y Jenkins (1970). Los modelos\npropuestos dentro de está metodología o conjunto de métodos se han\nvuelto indispensables para efectos de realizar pronósticos de corto\nplazo.En este sentido, se analizarán los métodos más importantes en series de\ntiempo: Autoregresivos (AR) y de Medias Móviles (MA). Asimismo, se\nrealizará un análisis de los procesos que resultan de la combinación de\nambos, conocida como ARMA, los cuales son más comúnmente usados para\nrealizar pronósticos.","code":""},{"path":"procesos-estacionarios-univariados.html","id":"procesos-autoregresivos-ar","chapter":"6 Procesos estacionarios univariados","heading":"6.1 Procesos Autoregresivos (AR)","text":"Los procesos autoregresivos tienen su origen en el trabajo de Cochrane y\nOrcutt de 1949, mediante el cual analizaron los residuales de una\nregresión clásica como un proceso autoregresivo. Puede consultarse el\napéndice para la discusión del modelo de regresión clásica.","code":""},{"path":"procesos-estacionarios-univariados.html","id":"ar1","chapter":"6 Procesos estacionarios univariados","heading":"6.1.1 AR(1)","text":"Como primer caso analizaremos al proceso autoregresivo de primer orden,\n\\(AR(1)\\), el cual podemos definir como una Ecuación Lineal en Diferencia\nEstocástica de Primer Orden. Diremos que una Ecuación Lineal en\nDiferencia de Primer Orden es estocástica si en su representación\nanalítica considera un componente estocástico como en la ecuación\n(6.1) descrita continuación:\\[\\begin{equation}\n    X_t = a_0 + a_1 X_{t-1} + U_t\n\\tag{6.1}\n\\end{equation}\\]Donde \\(a_0\\) es un término constante, \\(U_t\\) es un proceso estacionario,\ncon media cero (0), una varianza finita y constante (\\(\\sigma^2\\)) y una\ncovarianza que depende de la distancia entre \\(t\\) y cualquier \\(t-s\\)\n(\\(\\gamma_s\\))–que depende de los valores pasados o futuros de la\nvariable–, \\(X_0\\) es el valor inicial de \\(X_t\\). obstante, en general\nvamos asumir que la covarianza será cero (0), por lo que tendremos un\nproceso puramente aleatorio. Considerando la ecuación (6.1) y\nun proceso de sustitución sucesivo podemos establecer lo siguiente,\nempezando con \\(X_1\\): \\[\\begin{eqnarray*}\n    X_{1} & = & a_0 + a_1 X_{0} + U_{1}\n\\end{eqnarray*}\\]Para \\(X_2\\): \\[\\begin{eqnarray*}\nX_{2} & = & a_0 + a_1 X_{1} + U_{2} \\\\\n    & = & a_0 + a_1 (a_0 + a_1 X_{0} + U_{1}) + U_{2} \\\\\n    & = & a_0 + a_1 a_0 + a_1^2 X_{0} + a_1 U_{1} + U_{2}\n\\end{eqnarray*}\\]Para \\(X_3\\): \\[\\begin{eqnarray*}\nX_{3} & = & a_0 + \\alpha X_{2} + U_{3} \\\\\n    & = & a_0 + a_1 (a_0 + a_1 a_0 + a_1^2 X_{0} + a_1 U_{1} + U_{2}) + U_{3} \\\\\n    & = & a_0 + a_1 a_0 + a_1^2 a_0 + a_1^3 X_{0} + a_1^2 U_{1} + a_1 U_{2} + U_{3}\n\\end{eqnarray*}\\]Así, para cualquier \\(X_t\\), \\(t = 1, 2, 3, \\ldots\\), obtendríamos:\n\\[\\begin{eqnarray}\nX_{t} & = & a_0 + a_1 X_{t - 1} + U_{t} \\nonumber \\\\\n    & = & a_0 + a_1 (a_0 + a_1 a_0 + a_1^2 a_0 + \\ldots + a_1^{t-2} a_0 + a_1^{t-1} X_{0} \\nonumber \\\\\n    &   & + a_1^{t-2} U_{1} + \\ldots + a_1 U_{t - 2} + U_{t - 1}) + U_{t} \\nonumber \\\\\n    & = & a_0 + a_1 a_0 + a_1^2 a_0 + a_1^3 a_0 + \\ldots + a_1^{t-1} a_0 + a_1^{t} X_{0} \\nonumber \\\\\n    &   & + a_1^{t-1} U_{1} + \\ldots a_1^2 U_{t - 2} + a_1 U_{t - 1} + U_{t} \\nonumber \\\\\n    & = & (1 + a_1 + a_1^2 + a_1^3 + \\ldots + a_1^{t-1}) a_0 + a_1^{t} X_{0} \\nonumber \\\\\n    &   & + a_1^{t-1} U_{1} + \\ldots + a_1^2 U_{t - 2} + a_1 U_{t - 1} + U_{t}  \\nonumber\\\\\n    & = & \\frac{1 - a_1^t}{1 - a_1} a_0 + a_1^{t} X_{0} + \\sum^{t-1}_{j = 0} a_1^{j} U_{t - j}\n    \\tag{6.2}\n\\end{eqnarray}\\]De esta forma en la ecuación (6.2) observamos un proceso que\nes explicado por dos partes: una que depende del tiempo y otra que\ndepende de un proceso estocástico. Asimismo, debe notarse que la\ncondición de convergencia es idéntica que en el caso de ecuaciones en\ndiferencia estudiadas al inicio del curso: \\(\\lvert a_1 \\lvert < 1\\), por\nlo que cuando \\(t \\\\infty\\), la expresión (6.2) será la\nsiguiente:\\[\\begin{equation}\n    X_t = \\frac{1}{1 - a_1} a_0 + \\sum^{\\infty}_{j = 0} a_1^{j} U_{t - j}\n    \\tag{6.3}\n\\end{equation}\\]Así, desaparece la parte dependiente del tiempo y únicamente prevalece\nla parte que es dependiente del proceso estocástico. Esta es la solución\nde largo plazo del proceso \\(AR(1)\\), la cual depende del proceso\nestocástico. Notemos, además, que esta solución implica que la variable\no la serie de tiempo \\(X_t\\) es tambien un proceso estocástico que hereda\nlas propiedades de \\(U_t\\). Así, \\(X_t\\) es también un proceso estocástico\nestacionario, como demostraremos más adelante.Observemos que la ecuación (6.3) se puede reescribir si\nconsideramos la formulación que en la literatura se denomina como la\ndescomposición de Wold, en la cual se define que es posible asumir que\n\\(\\psi_j = a_1^j\\) y se considera el caso en el cual\n\\(\\lvert a_1 \\lvert< 1\\), de esta forma tendremos que por ejemplo cuando:\n\\[\\begin{equation*}\n    \\sum^{\\infty}_{j = 0} \\psi^2_j = \\sum^{\\infty}_{j = 0} a_1^{2j} = \\frac{1}{1 - a_1^2}\n\\end{equation*}\\]Alternativamente y de forma similar las ecuaciones en diferencia\nestudiadas previamente podemos escribir el proceso \\(AR(1)\\) mediante el\nuso del operador rezago como:\\[\\begin{eqnarray}\n    X_t & = & a_0 + a_1 L X_t + U_t \\nonumber \\\\\n    X_t - a_1 L X_t & = & a_0 + U_t \\nonumber \\\\\n    (1 - a_1 L) X_t & = & a_0 + U_t \\nonumber \\\\\n    X_t & = & \\frac{a_0}{1 - a_1 L} + \\frac{1}{1 - a_1 L} U_t\n    \\tag{6.4}\n\\end{eqnarray}\\]En esta última ecuación retomamos el siguiente término para reescribirlo\ncomo:\\[\\begin{equation}\n    \\frac{1}{1 - a_1 L} = 1 + a_1 L + a_1^2 L^2 + a_1^3 L^3 + \\ldots\n    \\tag{6.5}\n\\end{equation}\\]Tomando este resultado para sustituirlo en ecuación (6.4),\nobtenemos la siguiente expresión:\\[\\begin{eqnarray}\nX_t & = & (1 + a_1 L + a_1^2 L^2 + a_1^3 L^3 + \\ldots) a_0 + (1 + a_1 L + a_1^2 L^2 + a_1^3 L^3 + \\ldots) U_t \\nonumber \\\\\n    & = & (1 + a_1 + a_1^2 + a_1^3 + \\ldots) a_0 + U_t + a_1 U_{t-1} + a_1^2 U_{t-2} + a_1^3 U_{t-3} + \\ldots \\nonumber \\\\\n    & = & \\frac{a_0}{1 - a_1} + \\sum^{\\infty}_{j = 0} a_1^j U_{t-j}\n    \\tag{6.6}\n\\end{eqnarray}\\]Donde la condición de convergencia y estabilidad del proceso descrito en\nesta ecuación es que \\(\\lvert a_1 \\lvert < 1\\). Por lo que hemos\ndemostrado que mediante el uso del operador de rezago es posible llegar\nal mismo resultado que obtuvimos mediante el procedimiento de\nsustituciones iterativas.La ecuación (6.6) se puede interpretar como sigue. La\nsolución o trayectoria de equilibrio de un AR(1) se divide en dos\npartes. La primera es una constante que depende de los valores de \\(a_0\\)\ny \\(a_1\\). La segunda parte es la suma ponderada de las desviaciones o\nerrores observados y acumulados en el tiempo hasta el momento \\(t\\).Ahora obtendremos los momentos que describen la serie de tiempo cuando\nse trata de un porceso \\(AR(1)\\). Para ello debemos obtener la media, la\nvarianza y las covarianzas de \\(X_t\\). Para los siguientes resultados\ndebemos recordar y tener en mente que si \\(U_t\\) es un proceso puramente\naleatorio, entonces:\\(\\mathbb{E}[U_t] = 0\\) para todo \\(t\\)\\(Var[U_t] = \\sigma^2\\) para todo \\(t\\)\\(Cov[U_t, U_s] = 0\\) para todo \\(t \\neq s\\)Dicho lo anterior y partiendo de la ecuación (6.6), el primer\nmomento o valor esperado de la serie de tiempo será el siguiente:\n\\[\\begin{eqnarray}\n\\mathbb{E}[X_t] & = & \\mathbb{E} \\left[ \\frac{a_0}{1 - a_1} + \\sum^{\\infty}_{j = 0} a_1^j U_{t-j} \\right] \\nonumber \\\\\n    & = & \\frac{a_0}{1 - a_1} + \\sum^{\\infty}_{j = 0} a_1^j \\mathbb{E}[U_{t-j}] \\nonumber \\\\\n    & = & \\frac{a_0}{1 - a_1} = \\mu\n    \\tag{6.7}\n\\end{eqnarray}\\]Respecto de la varianza podemos escribir la siguiente expresión partir\nde la ecuación (6.6):\\[\\begin{eqnarray}\nVar[X_t] & = & \\mathbb{E}[(X_t - \\mu)^2] \\nonumber \\\\\n    & = & \\mathbb{E} \\left[ \\left( \\frac{a_0}{1 - a_1} + \\sum^{\\infty}_{j = 0} a_1^j U_{t-j} - \\frac{a_0}{1 - a_1} \\right)^2 \\right] \\nonumber \\\\\n    & = & \\mathbb{E}[(U_{t} + a_1 U_{t-1} + a_1^2 U_{t-2} + a_1^3 U_{t-3} + \\ldots)^2] \\nonumber \\\\\n    & = & \\mathbb{E}[U^2_{t} + a_1^2 U^2_{t-1} + a_1^4 U^2_{t-2} + a_1^6 U^2_{t-3} + \\ldots \\nonumber \\\\\n    &   & + 2 a_1 U_t U_{t-1} + 2 a_1^2 U_t U_{t-2} + \\ldots] \\nonumber \\\\\n    & = & \\mathbb{E}[U^2_{t}] + a_1^2 \\mathbb{E}[U^2_{t-1}] + a_1^4 \\mathbb{E}[U^2_{t-2}] + a_1^6 \\mathbb{E}[U^2_{t-3}] + \\ldots \\nonumber \\\\\n    & = & \\sigma^2 + a_1^2 \\sigma^2 + a_1^4 \\sigma^2 + a_1^6 \\sigma^2 + \\ldots \\nonumber \\\\\n    & = & \\sigma^2 (1 + a_1^2 + a_1^4 + a_1^6 + \\ldots) \\nonumber \\\\\n    & = & \\sigma^2 \\frac{1}{1 - a_1^2} = \\gamma(0)\n    \\tag{6.8}\n\\end{eqnarray}\\]Previo analizar la covarianza de la serie recordemos que para el\nproceso puramente aleatorio \\(U_t\\) su varianza y covarianza puede verse\ncomo \\(\\mathbb{E}[U_t, U_s] = \\sigma^2\\), para \\(t = s\\), y\n\\(\\mathbb{E}[U_t, U_s] = 0\\), para cualquier otro caso, respectivamente.Dicho lo anterior, partiendo de la ecuación (6.6) la\ncovarianza de la serie estará dada por:\\[\\begin{eqnarray}\nCov(X_t, X_{t-\\tau}) & = & \\mathbb{E}[(X_t - \\mu)(X_{t-\\tau} - \\mu)] \\nonumber \\\\\n    & = & \\mathbb{E} \\left[ \\left( \\frac{a_0}{1 - a_1} + \\sum^{\\infty}_{j = 0} a_1^j U_{t-j} - \\frac{a_0}{1 - a_1} \\right) \\right. \\nonumber \\\\\n    &   & \\left. \\times \\left( \\frac{a_0}{1 - a_1} + \\sum^{\\infty}_{j = 0} a_1^j U_{t-\\tau-j} - \\frac{a_0}{1 - a_1} \\right) \\right] \\nonumber \\\\\n    & = & a_1^{\\tau} \\mathbb{E}[U^2_{t-\\tau} + a_1 U^2_{t-\\tau-1} + a_1^2 U^2_{t-\\tau-2} + a_1^3 U^2_{t-\\tau-3} + \\ldots] \\nonumber \\\\\n    & = & a_1^{\\tau} \\sigma^2 \\frac{1}{1 - a_1^2} = \\gamma(\\tau)\n    \\tag{6.9}\n\\end{eqnarray}\\]Notése que con estos resultados en las ecuaciones (6.8) y\n(6.9) podemos construir la función de autocorrelación teórica\ncomo sigue:\\[\\begin{eqnarray}\n\\rho(\\tau) & = & \\frac{\\gamma(\\tau)}{\\gamma(0)} \\nonumber \\\\\n    & = & a_1^\\tau\n    \\tag{6.10}\n\\end{eqnarray}\\]Donde \\(\\tau = 1, 2, 3, \\ldots\\) y \\(\\lvert a_1 \\lvert < 1\\). Este último\nresultado significa que cuando el proceso autoregresivo es de orden 1\n(es decir, AR(1)) la función de autocorrelación teóricamente es igual al\nparámetro \\(a_1\\) elevado al número de rezagos considerados. obstante,\nnote que esto significa que la autocorrelación observada sea como lo\nexpresa en planteamiento anterior. Por el contrario, una observación\nsencilla mostraría que la autocorrelación observada sería ligeramente\ndistinta la autocorrelación teórica.Ahora veámos algunos ejemplos. En el primer ejemplo simularemos una\nserie y mostraremos el analísis de un proceso construído considerando un\nproceso puramente aleatorio como componente \\(U_t\\).1\nPor su parte, en un segundo ejemplo aplicaremos el análisis una serie\nde tiempo de una variable económica observada.2Para el primer ejemplo consideremos un proceso dado por la forma de un\n\\(AR(1)\\) como en la ecuación (6.4) cuya solución esta dada por la\necuación (6.6). En especifico, supongamos que el término o\ncomponente estocástico \\(U_t\\) es una serie generada partir de numeros\naleatorios de una función normal con media \\(0\\) y desviación estándar\n\\(4\\). Los detalles del proceso simulado se muestra en las siguientes\ngráficas.","code":""},{"path":"procesos-estacionarios-univariados.html","id":"ejemplo-ar1","chapter":"6 Procesos estacionarios univariados","heading":"6.1.1.1 EJEMPLO AR(1)","text":"Por lo tanto tenemos la serie de tiempo \\(AR(1)\\):\n\\[ X_t= 5+0.9X_{t-1}+U_t\\]En este caso el termino estocastico tiene una media de \\(0\\) y una\ndesviación estándar constante \\(\\sigma^2=4\\)\nFigure 6.1: AR(1) considerando \\(X_t=5+0.9X_{t-1}+U_t\\) ; \\(X_0=50\\) y que \\(U_t\\)~\\(N(0, 4)\\) y que \\(U_t \\sim \\mathcal{N}(0, 4)\\)\n\nFigure 6.2: \\(X_t = \\frac{5}{1 - 0.9} + \\sum_{j = 0}^{t-1} 0.9^j U_{t-j}\\), y que \\(U_t \\sim \\mathcal{N}(0, 4)\\)}\n\nFigure 6.3: Función de autocorrelación de un AR(1): \\(\\rho(\\tau)=\\frac{\\gamma( au)}{\\gamma(0)}\\)\n\nFigure 6.4: Función de autocorrelación de un AR(1): \\(\\rho(\\tau)= a_1^\\tau\\)\n\nFigure 6.5: Función de autocorrelación de un AR(1): \\(\\rho(\\tau)= a_1^\\tau\\)\n\nFigure 6.6: AR(1) considerando en conjunto \\(X_t = 5 + 0.9 X_{t-1} + U_t\\); \\(X_0 = 50\\) y \\(X_t = \\frac{5}{1 - 0.9} + \\sum_{j = 0}^{t-1} 0.9^j U_{t-j}\\), y que \\(U_t \\sim \\mathcal{N}(0, 4)\\)\nLa Figura 6.1 ilustra el comportamiento que se debería\nobservar en una serie considerando el procedimiento iterativo de\nconstrucción. Por su parte, la Figura 6.2 ilustra el\nproceso o trayectoria de la solución de la serie de tiempo. Finalmente,\nlas Figuras 6.3 y 6.3 muestran el\ncorrelograma calculado considerando una función de autocorrelación\naplicada al porceso real y una función de autocorrelación aplicada al\nproceso teórico, respectivamente.Recordemos que una trayectoria de equilibrio o solución de un \\(AR(1)\\) es\ncomo se muestra en la ecuación (6.6). Así, nuestra serie\nsimulada cumple con la característica de que los errores son más\nrelevantes cuando la serie es corta. Por el contrario, los errores son\nmenos relevantes, cuando la serie es muy larga. La Figura\n6.6 ilustra esta observación de la trayectoria de\nequilibrio.","code":"\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(latex2exp)\nsource(\"funciones/arroots.R\")\nsource(\"funciones/plot.armaroots.R\")\nsource(\"funciones/maroots.R\")\n\n# Parametros:\na0 <- 5; a1 <- 0.9; X_0 <- (a0/(1 - a1)); T <- 1000\n# Definimos un data frame para almacenar el proceso, agregamos una columna para el tiempo\nX_t <- data.frame(Tiempo = c(0:T))\n\n#  Parte estocastica de la serie de tiempo:\nset.seed(12345)\n\n# Agregamos un término estocástico al data frame\nX_t$U_t <- rnorm(T+1, mean = 0, sd = 4)\n# Agregamos columnas con NA's para un proceso teorico y uno real\nX_t$X_t <- NA\nX_t$XR_t <- NA\n\n# La serie teórica inicia en un valor inicial X_0\nX_t$X_t[1] <- X_0\n\n# La serie real inicia en un valor inicial X_0\nX_t$XR_t[1] <- X_0\n\n# Agregamos una columna para la función de Autocorrelación teórica:\nX_t$rho <-NA\n\nfor (i in 2:(T + 1)) {\n  # Real:\n  X_t$XR_t[i] = a0 + a1*X_t$XR_t[i-1] + X_t$U_t[i-1]\n  \n  # Teórico:\n  X_t$X_t[i] = X_t$X_t[i-1] + (a1^(i-1))*X_t$U_t[i-1]\n  \n  # Autocorrelación:\n  X_t$rho[i-1] = a1^(i-1)\n}\n\nhead(X_t)\n#>   Tiempo        U_t      X_t     XR_t      rho\n#> 1      0  2.3421153 50.00000 50.00000 0.900000\n#> 2      1  2.8378641 52.10790 52.34212 0.810000\n#> 3      2 -0.4372133 54.40657 54.94577 0.729000\n#> 4      3 -1.8139887 54.08785 54.01398 0.656100\n#> 5      4  2.4235498 52.89769 51.79859 0.590490\n#> 6      5 -7.2718239 54.32877 54.04228 0.531441#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2\n#> 3.4.0.\n#> ℹ Please use `linewidth` instead.\n\nggplot(data = X_t, aes(x = Tiempo, y = X_t)) + \n  geom_line(size = 0.5, color = \"#0F531C\") +\n  theme_light() + \n  xlab(\"Tiempo\") + \n  ylab(TeX(\"$X_t$\")) + \n  theme(plot.title = element_text(size = 11, face = \"bold\", hjust = 0)) + \n  theme(plot.subtitle = element_text(size = 10, hjust = 0)) + \n  theme(plot.caption = element_text(size = 10, hjust = 0)) +\n  theme(plot.margin = unit(c(1,1,1,1), \"cm\")) +\n  labs(\n    title = \"Comportamiento del Proceso Teórico\",\n    subtitle = \"Con un error con Distribución Normal (media = 0, desviación estándar = 4)\",\n    caption = \"Fuente: Elaboración propia.\"\n  )\n\nacf(X_t$XR_t, lag.max = 30, col = \"blue\", \n    ylab = \"Autocorrelacion\",\n    xlab=\"Rezagos\", \n    main=\"Funcion de Autocorrelacion Real\")\n\nbarplot(X_t$rho[1:30], names.arg = c(1:30), col = \"blue\", border=\"blue\", density = c(10,20), \n        ylab = \"Autocorrelacion\", \n        xlab=\"Rezagos\", \n        main=\"Funcion de Autocorrelacion Teórica\")\n\nacf(X_t$XR_t, lag.max = 30, col = \"blue\", \n    ylab = \"Autocorrelacion\",\n    xlab=\"Rezagos\", \n    main=\"Funcion de Autocorrelacion Real\")\n\nggplot(data = X_t, aes(x = Tiempo)) +\n  geom_line(aes(y = XR_t), size = 0.5, color = \"darkred\") +\n  geom_line(aes(y = X_t), size = 1, color = \"#0F531C\") +\n  theme_bw() + \n  xlab(\"Tiempo\") + \n  ylab(TeX(\"$X_t$\")) + \n  theme(plot.title = element_text(size = 11, face = \"bold\", hjust = 0)) + \n  theme(plot.subtitle = element_text(size = 10, hjust = 0)) + \n  theme(plot.caption = element_text(size = 10, hjust = 0)) +\n  theme(plot.margin = unit(c(1,1,1,1), \"cm\")) +\n  labs(\n    title = \"Comportamiento de los Procesos Real y Teórico\",\n    subtitle = \"Con un error con Distribución Normal (media = 0, desviación estándar = 4)\",\n    caption = \"Fuente: Elaboración propia.\"\n  )"},{"path":"procesos-estacionarios-univariados.html","id":"ar2","chapter":"6 Procesos estacionarios univariados","heading":"6.1.2 AR(2)","text":"Una vez analizado el caso de \\(AR(1)\\) analizaremos el caso del \\(AR(2)\\).\nLa ecuación generalizada del proceso autoregresivo de orden 2 (denotado\ncomo \\(AR(2)\\)) puede ser escrito como:\\[\\begin{equation}\n    X_t = a_0 + a_1 X_{t-1} + a_2 X_{t-2} + U_t\n    \\tag{6.11}\n\\end{equation}\\]Donde \\(U_t\\) denota un proceso puramente aleatorio con media cero (\\(0\\)), varianza constante (\\(\\sigma^2\\)) y autocovarianza cero (\\(Cov(U_t, U_s) = 0\\), con \\(t \\neq s\\)), y un parametro \\(a_2 \\neq 0\\). Así, utilizando el operador rezago podemos reescribir la ecuación (6.11) como:\n\\[\\begin{eqnarray*}\n    X_t - a_1 X_{t-1} - a_2 X_{t-2} & = & a_0 + U_t \\\\\n    (1 - a_1 L^1 - a_2 L^2) X_t & = & a_0 + U_t\n\\end{eqnarray*}\\]Donde, vamos denotar \\(\\alpha (L) = (1 - a_1 L^1 - a_2 L^2)\\), y lo llamaremos como un polinomio que depende del operador rezago y que es distinto de cero. De esta forma podemos reescribir la ecuación (6.11) como:\n\\[\\begin{equation}\n    \\alpha(L) X_t = a_0 + U_t\n    \\tag{6.12}\n\\end{equation}\\]Ahora, supongamos que existe el inverso multiplicativo del polinomio \\(\\alpha(L)\\), el cual será denotado como: \\(\\alpha^{-1}(L)\\) y cumple con que:\n\\[\\begin{equation}\n    \\alpha^{-1}(L) \\alpha(L) = 1   \n      \\tag{6.13}\n\\end{equation}\\]Así, podemos escribir la solución la ecuación (6.11) como:\n\\[\\begin{equation}\n    X_t = \\alpha^{-1}(L) \\delta + \\alpha^{-1}(L) U_t\n\\end{equation}\\]Si utilizamos el hecho que \\(\\alpha^{-1}(L)\\) se puede descomponer través del procedimiento de Wold en un polinomio de forma similar el caso de \\(AR(1)\\), tenemos que:\n\\[\\begin{equation}\n    \\alpha^{-1}(L) = \\psi_0 + \\psi_1 L + \\psi_2 L^2 + \\ldots\n      \\tag{6.14}\n\\end{equation}\\]Por lo tanto, el inverso multiplicativo \\(\\alpha^{-1}(L)\\) se puede ver como:\n\\[\\begin{equation}\n    1 = (1 - a_1 L^1 - a_2 L^2) (\\psi_0 + \\psi_1 L + \\psi_2 L^2 + \\ldots)\n      \\tag{6.15}\n\\end{equation}\\]Desarrollando la ecuación (6.15) tenemos la sigueinte expresión:\\[\\begin{eqnarray}\n    1 & = & \\psi_0 & + & \\psi_1 L & + & \\psi_2 L^2 & + & \\psi_3 L^3 & + & \\ldots \\\\\n     &  &  & - & a_1 \\psi_0 L & - & a_1 \\psi_1 L^2 & - & a_1 \\psi_2 L^3 & - & \\ldots \\\\\n    & &  &  &  & - & a_2 \\psi_0 L^2  & - & a_2 \\psi_1 L^3 & - & \\ldots\n\\end{eqnarray}\\]Ahora, podemos agrupar todos los términos en función del exponente asociado al operador rezago \\(L\\). La siguiente es una solución partícular y es una de las múltiples que podrían existir que cumpla con la ecuación (6.15). Sin embargo, para efectos del análisis sólo necesitamos una de esas soluciones. Utilizaremos las siguientes condiciones que deben cumplirse en una de las posibles soluciones:\\[\\begin{eqnarray}\n    L^0 : &   & \\Rightarrow & \\psi_0 = 1 \\\\\n    L : & \\psi_1 - a_1 \\psi_0 = 0 & \\Rightarrow & \\psi_1 = a_1$ \\\\\n    L^2 : & \\psi_2 - a_1 \\psi_1 - a_2 \\psi_0 = 0 & \\Rightarrow & \\psi_2 = ^2_1 + a_2 \\\\\n    L^3 : & \\psi_3 - a_1 \\psi_2 - a_2 \\psi_1 = 0 & \\Rightarrow & \\psi_3 = ^3_1 + 2 a_1 a_2$ \\\\\n    \\vdots & \\vdots & \\vdots & \\vdots\n\\end{eqnarray}\\]De esta forma podemos observar que en el límite siempre obtendremos una ecuación del tipo \\(\\psi_j - a_1 \\psi_{j-1} - a_2 \\psi_{j-2} = 0\\) asociada cada uno de los casos en que exista un \\(L^j\\), donde \\(j \\neq 0, 1\\), y la cual siempre podremos resolver conociendo que las condiciones iniciales son: \\(\\psi_0 = 1\\) y \\(\\psi_1 = a_1\\).Así, de las relaciones antes mencionadas y considerando que \\(\\alpha^{-1} (L)\\) aplicada una constante como \\(a_0\\), tendrá como resultado otra constante. De esta forma podemos escribir que la solución del proceso AR(2) en la ecuación (6.11) será dada por una expresión como sigue:\n\\[\\begin{equation}\n    X_t = \\frac{\\delta}{1 - a_1 - a_2} + \\sum^{\\infty}_{j = 0} \\psi_{t - j} U_{t - j}\n    \\tag{6.16}\n\\end{equation}\\]Donde todos los parametros \\(\\psi_i\\) está determinado por los parámtros \\(a_0\\), \\(a_1\\) y \\(a_2\\). En particular, \\(\\psi_0 = 1\\) y \\(\\psi_1 = a_1\\) como describimos anteriormente. Al igual que en el caso del \\(AR(1)\\), en la ecuación (6.16) las condiciones de estabilidad estarán dadas por las soluciones del siguiente polinomio característico:3\n\\[\\begin{equation}\n    \\lambda^2 - \\lambda a_1 - a_2 = 0\n    \\tag{6.17}\n\\end{equation}\\]Así, la condición de estabilidad de la trayectoria es que \\(\\lvert\\lambda_i\\lvert < 1\\), para \\(= 1, 2\\). Es decir, es necesario que cada una de las raíces sea, en valor absoluto, siempre menor que la unidad. Estas son las condiciones de estabilidad para el proceso \\(AR(2)\\).Finalmente, al igual que en un \\(AR(1)\\), continuación determinamos los momentos de una serie que sigue un proceso \\(AR(2)\\). Iniciamos con la determinación de la media de la serie:\n\\[\\begin{equation}\n    \\mathbb{E}[X_t] = \\mu = \\frac{a_0}{1 - a_1 - a_2}\n    \\tag{6.18}\n\\end{equation}\\]Lo anterior es cierto puesto que \\(\\mathbb{E}[U_{t - }] = 0\\), para todo \\(= 0, 1, 2, \\ldots\\). Para determinar la varianza utilizaremos las siguientes relaciones basadas en el uso del valor esperado, varianza y covarianza de la serie. Adicionalmente, para simplificar el trabajo asumamos que \\(a_0 = 0\\), lo cual implica que \\(\\mu = 0\\). Dicho lo anterior, partamos de:\n\\[\\begin{eqnarray}\n    \\mathbb{E}[X_t X_{t - \\tau}] & = & \\mathbb{E}[(a_1 X_{t-1} + a_2 X_{t-2} + U_t) X_{t - \\tau}]\\\\\n    & = & a_1 \\mathbb{E}[X_{t - 1} X_{t - \\tau}] + a_2 \\mathbb{E}[X_{t - 2} X_{t - \\tau}] + \\mathbb{E}[U_{t} X_{t - \\tau}]\n    \\tag{6.19}\n\\end{eqnarray}\\]Donde \\(\\tau = 0, 1, 2, 3, \\ldots\\) y que \\(\\mathbb{E}[U_{t} X_{t - \\tau}] = 0\\) para todo \\(\\tau \\neq 0\\).4 Dicho esto, podemos derivar el valor del valor esperado para diferentes valores de \\(\\tau\\):\\[\\begin{eqnarray}\n    \\tau = 0: & \\gamma(0) & = & \\alpha_1 \\gamma(1) + \\alpha_2 \\gamma(2) + \\sigma^2 \\\\\n    \\tau = 1: & \\gamma(1) & = & \\alpha_1 \\gamma(0) + \\alpha_2 \\gamma(1) \\\\\n    \\tau = 2: & \\gamma(2) & = & \\alpha_1 \\gamma(1) + \\alpha_2 \\gamma(0) \\\\\n    \\vdots & \\vdots & \\vdots & \\vdots\n\\end{eqnarray}\\]Donde debe ser claro que \\(\\mathbb{E}[(X_{t} - \\mu)(X_{t - \\tau} - \\mu)] = \\mathbb{E}[X_{t} X_{t - \\tau}] = \\gamma(\\tau)\\). Así, en general cuando \\(\\tau \\neq 0\\):\n\\[\\begin{equation}\n    \\gamma(\\tau) = a_1 \\gamma(\\tau - 1) + a_2 \\gamma(\\tau - 2)\n\\end{equation}\\]Realizando la sustitución recursiva y solucionando el sistema respectivo obtenemos que las varianza y covarianzas estaran determinadas por:\n\\[\\begin{equation}\n    Var[X_t] = \\gamma(0) = \\frac{1 - a_2}{(1 + a_2)[(1 - a_2)^2 - ^2_1]} \\sigma^2\n    \\tag{6.20}\n\\end{equation}\\]\\[\\begin{equation}\n    \\gamma(1) = \\frac{a_1}{(1 + a_2)[(1 - a_2)^2 - ^2_1]} \\sigma^2\n    \\tag{6.21}\n\\end{equation}\\]\\[\\begin{equation}\n    \\gamma(2) = \\frac{^2_1 + a_2 - ^2_2}{(1 + a_2)[(1 - a_2)^2 - ^2_1]} \\sigma^2\n    \\tag{6.22}\n\\end{equation}\\]Recordemos que las funciones de autocorrelación se obtienen de la división de cada unas de las funciones de covarianza (\\(\\gamma(\\tau)\\)) por la varianza (\\(\\gamma(0)\\)). Así, podemos construir la siguiente expresión:\n\\[\\begin{equation}\n    \\rho(\\tau) - a_1 \\rho(\\tau - 1) - a_2 \\rho(\\tau - 2) = 0\n    \\tag{6.23}\n\\end{equation}\\]Para el segundo ejemplo consideremos una aplicación una serie de\ntiempo en especifico:","code":""},{"path":"procesos-estacionarios-univariados.html","id":"ejemplo-ar2","chapter":"6 Procesos estacionarios univariados","heading":"6.1.2.1 EJEMPLO AR(2)","text":"Recordando el tema pasado y la serie en la que evaluamos los cambios de\nprecio del ACTIVO AMZN como si fueran retornos:\nFigure 5.5: Serie de tiempo de los precios de apertura de año en los últimos 20 años\nPrimero que nada es importante cargar los datos un objeto series de\ntiempo. Esto nos lo permite la función ts(). Además debemos serciorarnos\nde que los datos esten en orden cronológico.Dado que queremos saber si existe un proceso \\(AR(2)\\) en estos cambio\ndebemos calcularlo. Para ello utilizamos la función \\(lm()\\) que realizará una regresión lineal y veremos la relación de los valores con sus valores pasados en \\(t-2\\):Veamos la tabla de la regresión lineal:\nAR(2) de los precios de apertura de AMZN\nLa tabla anterior claramente indica que hay una relación entre el valor del precio y sus valores anteriores en un proceso AR(2).Así pues es importante modificar la serie de tiempo para ilustrar como se puede controlar los efectos de los autoregresores \\(AR(2)\\). Para ello, utilizaremos la función \\(arima()\\). En “order” tenemos un vector \\(c(p,d,q)\\) que corresponde \\(p\\) el grado de AR, \\(d\\) el grado de diferención y \\(q\\) el grado de MA que utilizaremos. El valor \\(q\\) quedaráá en \\(0\\) por ahora pero será analizado más adelante.Veamos si las raices inversas mantienen la estabilidad al ser menores 1.\nFigure 6.7: Raices AR(2) Inversas de la serie de tiempo\nClaramente se puede en la Figura 6.7 ver que los valores de las raices inversas están dentro del circulo unitario y, por consiguiente son menores 1. Ahora veamos como se ve la estimación ajustada AR(2) con el plot original.\nFigure 6.8: Diferencia entre la serie de tiempo original de precios de AMZN y su AR(2)\nConsecuentemente en la Figura 6.8 es posible ver la manera en la que se suaviza un poco la línea lo cual debe ayudarnos hacer una mejor estimación. Ahora veamos \\(AR(p)\\).","code":"\n#install.packages(\"pacman\")\n#pacman nos permite cargar varias librerias en una sola línea\nlibrary(pacman)\npacman::p_load(tidyverse,BatchGetSymbols,ggplot2,lubridate,readxl,forecast,stats,stargazer,knitr)\n#Primero determinamos el lapso de tiempo\npd<-as.Date(\"2002/9/01\") #primer fecha\npd\n#> [1] \"2002-09-01\"\n#> [1] \"2021-09-18\"\nld<- as.Date(\"2021/10/01\")#última fecha\nld\n#> [1] \"2021-10-01\"\n#Intervalos de tiempo\nint<-\"monthly\"\n\n#Datos a elegir\ndt<-c(\"AMZN\")\n\n#Descargando los valores\ndata1<- BatchGetSymbols(tickers = dt,\n                       first.date = pd,\n                       last.date = ld,\n                       freq.data = int,\n                       do.cache = FALSE,\n                       thresh.bad.data = 0)\n\n#Generando data frame con los valores\ndata_precio_amzn<-data1$df.tickers\ncolnames(data_precio_amzn)\n#>  [1] \"ticker\"              \"ref.date\"           \n#>  [3] \"volume\"              \"price.open\"         \n#>  [5] \"price.high\"          \"price.low\"          \n#>  [7] \"price.close\"         \"price.adjusted\"     \n#>  [9] \"ret.adjusted.prices\" \"ret.closing.prices\"\nret_20_amazn<-ggplot(data=data_precio_amzn, aes(x=ref.date))+\n  geom_line(aes(y=price.open))+\n  labs(title=\"Precios de apertura de AMZN en los últimos 20 años\",y=\"Retornos\", x=\"Año\")+\n  theme_light()\nret_20_amazn\ndata_precio_amzn<-data_precio_amzn[order(data_precio_amzn$ref.date),]\nhead(data_precio_amzn)#dado que ya estaba en orden cronológico nuestro df no cambia\n#> # A tibble: 6 × 10\n#>   ticker ref.date     volume price…¹ price…² price…³ price…⁴\n#>   <chr>  <date>        <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n#> 1 AMZN   2002-09-03   2.92e9   0.736   0.896   0.712   0.796\n#> 2 AMZN   2002-10-01   4.07e9   0.812   1.01    0.800   0.968\n#> 3 AMZN   2002-11-01   4.13e9   0.961   1.23    0.91    1.17 \n#> 4 AMZN   2002-12-02   3.11e9   1.21    1.25    0.922   0.944\n#> 5 AMZN   2003-01-02   3.38e9   0.960   1.16    0.928   1.09 \n#> 6 AMZN   2003-02-03   2.32e9   1.10    1.12    0.980   1.10 \n#> # … with 3 more variables: price.adjusted <dbl>,\n#> #   ret.adjusted.prices <dbl>, ret.closing.prices <dbl>,\n#> #   and abbreviated variable names ¹​price.open,\n#> #   ²​price.high, ³​price.low, ⁴​price.close\n#hagamos el objeto ts\nprice_amazn_ts<-ts(data_precio_amzn$price.open, frequency = 12)\nplot(price_amazn_ts)#de esta manera podemos ver que se cargo bien debido a que es igual al ggplot\npriceopen<-data_precio_amzn$price.open\npriceopen_amazn<-data.matrix(priceopen)\nlags<-function(mat,p){\n  for(i in 1:p){\n    uno<-mat[,1]\n    mat<-cbind(mat,lag(uno,i))}\n  as.data.frame(mat)\n}\n\ndf.lags<-lags(priceopen_amazn,2)\nar2_amazn<-lm(V1~., data=df.lags)\nAR_price_amazn_ts<-arima(price_amazn_ts,order=c(2,0,0),method = \"ML\")\nAR_price_amazn_pl<-Arima(price_amazn_ts,order=c(2,0,0),method = \"ML\")\nautoplot(AR_price_amazn_ts)+theme_light()\nplot(AR_price_amazn_pl$x,col=\"black\", main = \"Diferencia entre la serie de tiempo original y AR(2)\",xlab=\"Tiempo\",ylab=\"Precio\")+lines(fitted(AR_price_amazn_pl),col=\"blue\")#> integer(0)"},{"path":"procesos-estacionarios-univariados.html","id":"arp","chapter":"6 Procesos estacionarios univariados","heading":"6.1.3 AR(p)","text":"Veremos ahora una generalización de los procesos autoregresivos (AR). Esta generalización es conocida como un proceso \\(AR(p)\\) y que puede ser descrito por la siguiente ecuación en diferencia estocástica:\n\\[\\begin{equation}\n    X_t = a_0 + a_1 X_{t-1} + a_2 X_{t-2} + a_3 X_{t-3} + \\ldots + a_p X_{t-p} + U_t\n    \\tag{6.24}\n\\end{equation}\\]Donde \\(a_p \\neq 0\\), y \\(U_t\\) es un proceso puramente aleatorio con media cero (0), varianza constante (\\(\\sigma^2\\)) y covarianza cero (0). Usando el operador rezago, \\(L^k\\), para \\(k = 0, 1, 2, \\ldots, p\\), obtenemos la siguiente expresión de la ecuación (6.24):\n\\[\\begin{equation}\n    (1 - a_1 L - a_2 L^2 - a_3 L^3 - \\ldots - a_p L^p) X_t = a_0 + U_t\n    \\tag{6.25}\n\\end{equation}\\]Definamos el polinomio \\(\\alpha(L)\\) como:\n\\[\\begin{equation}\n    \\alpha(L) = 1 - a_1 L - a_2 L^2 - a_3 L^3 - \\ldots - a_p L^p\n    \\tag{6.26}\n\\end{equation}\\]De forma similar que en los procesos \\(AR(1)\\) y \\(AR(2)\\), las condiciones de estabilidad del proceso \\(AR(p)\\) estarán dadas por la solución de la ecuación característica:\n\\[\\begin{equation}\n    \\lambda^p - a_1 \\lambda^{p-1} - a_2 \\lambda^{p-2} - a_3 \\lambda^{p-3} - \\ldots - a_p = 0\n        \\tag{6.27}\n\\end{equation}\\]Así, solo si el polinomio anterior tiene raíces cuyo valor absoluto sea menor uno (\\(\\lvert\\lambda_i\\lvert < 1\\)) y si \\(1 - a_1 L - a_2 L^2 - a_3 L^3 - \\ldots - a_p L^p < 1\\) podremos decir que el proceso es convergente y estable. Lo anterior significa que la ecuación (6.26) puede expresarse en términos de la descomposición de Wold o como la suma infinita de términos como:\n\\[\\begin{equation}\n    \\frac{1}{1 - a_1 L  - a_2 L^2 - a_3 L^3  - \\ldots - a_p L^p} = \\psi_0 + \\psi_1 L + \\psi_2 L^2 + \\psi_3 L^3 + \\ldots\n\\tag{6.28}\n\\end{equation}\\]Donde, por construcción de \\(\\alpha(L) \\alpha^{-1}(L) = 1\\) implica que \\(\\psi_0 = 1\\). De forma similar los proceso AR(1) y AR(2), es posible determinar el valor de los coefieentes \\(\\psi_j\\) en términos de los coefientes \\(a_i\\). Así, la solución del proceso \\(AR(p)\\) estará dada por:\n\\[\\begin{equation}\n    X_t = \\frac{a_0}{1 - a_1  - a_2 - a_3  - \\ldots - a_p} + \\sum^{\\infty}_{j = 0} \\psi_j U_{t-j}\n    \\tag{6.29}\n\\end{equation}\\]Considerando la solución de la ecuación (6.24) expresada en la ecuación (6.29) podemos determinar los momentos del proceso y que estarán dados por una media como:\n\\[\\begin{equation}\n    \\mathbb{E}[X_t] = \\mu = \\frac{a_o}{1 - a_1  - a_2 - a_3  - \\ldots - a_p}\n        \\tag{6.30}\n\\end{equation}\\]Lo anterior, considerado que \\(\\mathbb{E}[U_t] = 0\\), para todo \\(t\\). Para determinar la varianza del proceso, sin pérdida de generalidad, podemos definir una ecuación: \\(\\gamma(\\tau) = \\mathbb{E}[X_{t - \\tau} X_t]\\), la cual (omitiendo la constante, ya que la correlación de una constante con cuaquier variable aleatoria que depende del tiempo es cero (0)) puede ser escrita como:\n\\[\\begin{equation}\n    \\gamma(\\tau) = \\mathbb{E}[(X_{t - \\tau}) \\cdot (a_1 X_{t-1} + a_2 X_{t-2} + a_3 X_{t-3} + \\ldots + + a_p X_{t-p} + U_t)]\n        \\tag{6.31}\n\\end{equation}\\]Donde \\(\\tau = 0, 1, 2, \\ldots, p\\) y \\(a_0 = 0\\), lo que implica que \\(\\mu = 0\\). De lo anterior obtenemos el siguiente conjunto de ecuaciones mediante sustituciones de los valores de \\(\\tau\\):\n\\[\\begin{eqnarray}\n    \\gamma(0) & = & a_1 \\gamma(1) + a_2 \\gamma(2) + \\ldots + a_p \\gamma(p) + \\sigma^2 \\nonumber \\\\\n    \\gamma(1) & = & a_1 \\gamma(0) + a_2 \\gamma(1) + \\ldots + a_p \\gamma(p-1) \\nonumber \\\\\n    \\vdots \\nonumber \\\\\n    \\gamma(p) & = & a_1 \\gamma(p-1) + a_2 \\gamma(p-2) + \\ldots + a_p \\gamma(0) \\nonumber\n\\end{eqnarray}\\]De esta forma, es fácil observar que la ecuación general para \\(p > 0\\) estará dada por:\n\\[\\begin{equation}\n    \\gamma(p) - a_1 \\gamma(\\tau - 1) - a_2 \\gamma(\\tau - 2) - \\ldots - a_p \\gamma(\\tau - p) = 0\n    \\tag{6.32}\n\\end{equation}\\]Dividiendo la ecuación (6.32) por \\(\\gamma(0)\\), se obtiene la siguiente ecuación:\n\\[\\begin{equation}\n    \\rho(p) - a_1 \\rho(\\tau - 1) + a_2 \\rho(\\tau - 2) + \\ldots + a_p \\rho(\\tau - p) = 0\n        \\tag{6.33}\n\\end{equation}\\]Así, podemos escribir el siguiente sistema de ecuaciones:\n\\[\\begin{eqnarray}\n    \\rho(1) & = & a_1 + a_2 \\rho(1) + a_3 \\rho(2) + \\ldots + a_p \\rho(p-1) \\nonumber \\\\\n    \\rho(2) & = & a_1 \\rho(1) + a_2 + a_3 \\rho(1) + \\ldots + a_p \\rho(p-2) \\nonumber \\\\\n    & \\vdots & \\nonumber \\\\\n    \\rho(p) & = & a_1 \\rho(p-1) + a_2 \\rho(p-2) + \\ldots + a_p \\nonumber\n\\end{eqnarray}\\]Lo anterior se puede expresar como un conjunto de vectores y matrices de la siguiente forma:\n\\[\\begin{equation}\n    \\left[\n    \\begin{array}{c}\n        \\rho(1) \\\\\n        \\rho(2) \\\\\n        \\vdots \\\\\n        \\rho(p)\n    \\end{array}\n    \\right]\n    =\n    \\left[\n    \\begin{array}{c c c c}\n        1 & \\rho(1) & \\ldots & \\rho(p - 1) \\\\\n        \\rho(1) & 1 & \\ldots & \\rho(p - 2) \\\\\n        \\rho(2) & \\rho(1) & \\ldots & \\rho(p - 3) \\\\\n        \\vdots & \\vdots & \\ldots & \\vdots \\\\\n        \\rho(p - 1) & \\rho(p - 2) & \\ldots & 1 \\\\\n    \\end{array}\n    \\right]\n    \\left[\n    \\begin{array}{c}\n        a_1 \\\\\n        a_2 \\\\\n        a_3 \\\\\n        \\vdots \\\\\n        a_p \\\\\n    \\end{array}\n    \\right]\n\\tag{6.34}\n\\end{equation}\\]De lo anterior podemos escribir la siguiente ecuación que es una forma alternativa para expresar los valores de los coefientes \\(a_i\\) de la la solución del proceso \\(AR(p)\\):\n\\[\\begin{equation}\n    \\mathbf{\\rho} = \\mathbf{R} \\mathbf{}\n  \\tag{6.35}\n\\end{equation}\\]Es decir, podemos obtener la siguiente expresión:\n\\[\\begin{equation}\n    \\mathbf{} = \\mathbf{R}^{-1} \\mathbf{\\rho}\n    \\tag{6.36}\n\\end{equation}\\]","code":""},{"path":"procesos-estacionarios-univariados.html","id":"ejemplo-arp","chapter":"6 Procesos estacionarios univariados","heading":"6.1.3.1 Ejemplo AR(p)","text":"Veamos primero la función de autocorrelacion y el plot de los lags.\nFigure 6.9: Función de Autocorrelación parcial de la serie de tiempo de AMZN\n\nFigure 6.10: Función de Autocorrelación de la serie de tiempo de AMZN\n\nFigure 6.11: 5 plots de correlacion de los lags de la serie de tiempo de AMZN\nDado que se puede ver en las Figuras 6.10, 6.9 y 6.11 que los valores de lag=1 tienen correlación resulta relevante hacer un AR(1), que corresponde 1 años. Así pues:AR:Regresión lineal:\nAR(1) de los precios de apertura de AMZN\nRaices:\nFigure 6.12: Raices AR(1) Inversas de la serie de tiempo\nPlot:\nFigure 6.13: Diferencia entre la serie de tiempo original de precios de AMZN y su AR(1)\n","code":"\npacf(price_amazn_ts, main=\"\", lag.max = 60)\nacf(price_amazn_ts, main=\"\", lag.max = 60)\nlag.plot(price_amazn_ts, main=\"\", lags=5)\nAR_price_amazn_ts_1<-arima(price_amazn_ts,order=c(1,0,0),method = \"ML\")\nAR_price_amazn_pl_1<-Arima(price_amazn_ts,order=c(1,0,0),method = \"ML\")\ndf.lags.1<-lags(priceopen_amazn,1)\nar20_amazn<-lm(V1~., data=df.lags.1)\nautoplot(AR_price_amazn_ts_1)+theme_light()\nplot(AR_price_amazn_pl_1$x,col=\"black\", main = \"Diferencia entre la serie de tiempo original y AR(1)\",xlab=\"Tiempo\",ylab=\"Precio\")+lines(fitted(AR_price_amazn_pl_1),col=\"red\")#> integer(0)"},{"path":"procesos-estacionarios-univariados.html","id":"función-de-autocorrelación-parcial","chapter":"6 Procesos estacionarios univariados","heading":"6.2 Función de Autocorrelación Parcial","text":"Ahora introduciremos el concepto de Función de Autocorrelación Parcial (PACF, por sus siglas en inglés). Primero, dadas las condiciones de estabilidad y de convergencia, si suponemos que un proceso AR, MA, ARMA o ARIMA tienen toda la información de los rezagos de la serie en conjunto y toda la información de los promedio móviles del término de error, resulta importante construir una métrica para distinguir el efecto de \\(X_{t - \\tau}\\) o el efecto de \\(U_{t - \\tau}\\) (para cualquier \\(\\tau\\)) sobre \\(X_t\\) de forma individual.La idea es construir una métrica de la correlación que existe entre las diferentes varibles aleatorias, si para tal efecto se ha controlado el efecto del resto de la información. Así, podemos definir la ecuación que puede responder este planteamiento como:\n\\[\\begin{equation}\n    X_t = \\phi_{k1} X_{t-1} + \\phi_{k2} X_{t-2} + \\ldots + \\phi_{kk} X_{t-k} + U_t\n    \\tag{6.37}\n\\end{equation}\\]Donde \\(\\phi_{ki}\\) es el coeficiente de la variable dada con el rezago \\(\\) si el proceso tiene un órden \\(k\\). Así, los coeficientes \\(\\phi_{kk}\\) son los coeficientes de la autocorrelación parcial (considerando un proceso AR(k)). Observemos que la autocorrelaicón parcial mide la correlación entre \\(X_t\\) y \\(X_{t-k}\\) que se mantiene cuando el efecto de las variables \\(X_{t-1}\\), \\(X_{t-2}\\), \\(\\ldots\\) y \\(X_{t-k-1}\\) en \\(X_{t}\\) y \\(X_{t-k}\\) ha sido eliminado.Dada la expresión considerada en la ecuación (6.37), podemos resolver el problema de establecer el valor de cada \\(\\phi_{ki}\\) mediante la solución del sistema que se representa en lo siguiente:Table 6.1:  Relación entre la Función de autocorrelación y la Función de autocorrelación parcial de una serie \\(X_t\\).\\[\\begin{equation}\n    \\left[\n    \\begin{array}{c}\n        \\rho(1) \\\\\n        \\rho(2) \\\\\n        \\vdots \\\\\n        \\rho(k)\n    \\end{array}\n    \\right]\n    =\n    \\left[\n    \\begin{array}{c c c c}\n        1 & \\rho(1) & \\ldots & \\rho(k - 1)\\\\\n        \\rho(1) & 1 & \\ldots & \\rho(k - 2)\\\\\n        \\rho(2) & \\rho(1) & \\ldots & \\rho(k - 3)\\\\\n        \\vdots & \\vdots & \\ldots & \\vdots\\\\\n        \\rho(k - 1) & \\rho(k - 2) & \\ldots & 1\\\\\n    \\end{array}\n    \\right]\n    \\left[\n    \\begin{array}{c}\n        \\phi_{k1} \\\\\n        \\phi_{k2} \\\\\n        \\phi_{k3} \\\\\n        \\vdots \\\\\n        \\phi_{kk} \\\\\n    \\end{array}\n    \\right]\n    \\tag{6.38}    \n\\end{equation}\\]Del cual se puede derivar una solución, resoviendo por el método de cramer, o cualquier otro método que consideremos y que permita calcular la solución de sistemas de ecuaciones.Posterior al análisis analítico platearemos un enfoque para interpretar las funciones de autocorrelación y autocorrelación parcial. Este enfoque pretende aportar al principio de parcimonia, en el cual podemos identificar el número de parámetros que posiblemente puede describir mejor la serie en un modelo ARMA(p, q).En el Cuadro 6.1 se muestra un resumen de las caranterísticas que debemos observar para determinar el número de parámetros de cada uno de los componentes AR y MA. Lo anterior por observación de las funciones de autocorrelación y autocorrelación parcial. Este enfoque es el más formal, más adelante implemtaremos uno más formal y que puede ser más claro de cómo determinar el númeto de parámetros.","code":""},{"path":"procesos-estacionarios-univariados.html","id":"procesos-de-medias-móviles-ma","chapter":"6 Procesos estacionarios univariados","heading":"6.3 Procesos de Medias Móviles (MA)","text":"","code":""},{"path":"procesos-estacionarios-univariados.html","id":"ma1","chapter":"6 Procesos estacionarios univariados","heading":"6.3.1 MA(1)","text":"Una vez planteado el proceso generalizado de \\(AR(p)\\), iniciamos el planteamiento de los proceso de medias móviles, denotados como \\(MA(q)\\). Iniciemos con el planteamiento del proceso \\(MA(1)\\), que se puede escribir como una ecuación como la siguiente:\n\\[\\begin{equation}\n    X_t = \\mu + U_t - b_1 U_{t-1}\n    \\tag{6.39}\n\\end{equation}\\]O como:\n\\[\\begin{equation}\n    X_t - \\mu = (1 - b_1 L) U_{t}\n    \\tag{6.40}\n\\end{equation}\\]Donde \\(U_t\\) es un proceso puramente aleatorio, es decir, con \\(\\mathbb{E}[U_t] = 0\\), \\(Var[U_t] = \\sigma^2\\), y \\(Cov[U_t, U_s] = 0\\). Así, un proceso \\(MA(1)\\) puede verse como un proceso AR con una descomposición de Wold en la que \\(\\psi_0 = 1\\), \\(\\psi_1 = - b_1\\) y \\(\\psi_j = 0\\) para todo \\(j > 1\\).Al igual que los procesos autoregresivos, determinaremos los momentos de un proceso \\(MA(1)\\). En el caso de la media observamos que será:\n\\[\\begin{eqnarray}\n    \\mathbb{E}[X_t] & = & \\mu + \\mathbb{E}[U_t] - b_1 \\mathbb{E}[U_{t - 1}] \\nonumber \\\\\n    & = & \\mu\n    \\tag{6.41}\n\\end{eqnarray}\\]Por su parte la varianza estará dada por:\n\\[\\begin{eqnarray}\n    Var[X_t] & = & \\mathbb{E}[(X_t - \\mu)^2] \\nonumber \\\\\n    & = & \\mathbb{E}[(U_t - b_1 U_{t-1})^2] \\nonumber \\\\\n    & = & \\mathbb{E}[U_t^2 - 2 b_1 U_t U_{t-1} + b_1^2 U_{t - 1}^2] \\nonumber \\\\\n    & = &\\mathbb{E}[U_t^2] - 2 b_1 \\mathbb{E}[U_t U_{t-1}] + b_1^2 \\mathbb{E}[U_{t - 1}^2]] \\nonumber \\\\\n    & = & \\sigma^2 + b_1^2 \\sigma^2 \\nonumber \\\\\n    & = & (1 + b_1^2) \\sigma^2 = \\gamma(0)\n    \\tag{6.42}\n\\end{eqnarray}\\]De esta forma, la varianza del proceso es constante en cualquier periodo \\(t\\). Para determinar la covarianza utilizaremos la siguiente ecuación:\n\\[\\begin{eqnarray}\n    \\mathbb{E}[(x_t - \\mu)(x_{t + \\tau} - \\mu)] & = & \\mathbb{E}[(U_t - b_1 U_{t-1})(U_{t + \\tau} - b_1 U_{t + \\tau - 1})] \\nonumber \\\\\n    & = & \\mathbb{E}[U_t U_{t + \\tau} - b_1 U_t U_{t + \\tau - 1} - b_1 U_{t - 1} U_{t + \\tau} \\nonumber \\\\\n    &   & + b_1^2 U_{t - 1} U_{t + \\tau - 1}] \\nonumber \\\\\n    & = & \\mathbb{E}[U_t U_{t + \\tau}] - b_1 \\mathbb{E}[U_t U_{t + \\tau - 1}] \\nonumber \\\\\n    &   & - b_1 \\mathbb{E}[U_{t - 1} U_{t + \\tau}] + b_1^2 \\mathbb{E}[U_{t - 1} U_{t + \\tau - 1}]\n    \\tag{6.43}\n\\end{eqnarray}\\]Si hacemos sustituciones de diferentes valores de \\(\\tau\\) en la ecuación (6.43) notaremos que la covarianza será distinta de cero únicamente para el caso de \\(\\tau = 1, -1\\). En ambos casos tendremos como resultado:\n\\[\\begin{eqnarray}\n    \\mathbb{E}[(x_t - \\mu)(x_{t + 1} - \\mu)] & = & \\mathbb{E}[(x_t - \\mu)(x_{t - 1} - \\mu)] \\nonumber \\\\\n    & = & - b_1 \\mathbb{E}[U_t U_{t}] \\nonumber \\\\\n    & = & - b_1 \\mathbb{E}[U_{t - 1} U_{t - 1}] \\nonumber \\\\\n    & = & - b_1^2 \\sigma^2 = \\gamma(1)\n        \\tag{6.44}\n\\end{eqnarray}\\]De esta forma tendremos que las funciones de autocorrelación estarán dadas por los siguientes casos:\n\\[\\begin{eqnarray}\n    \\rho(0) & = & 1 \\nonumber \\\\\n    \\rho(1) & = & \\frac{- b_1}{1 + b_1^2} \\nonumber \\\\\n    \\rho(\\tau) & = & 0 \\text{ para todo } \\tau > 1 \\nonumber\n\\end{eqnarray}\\]Ahora regresando la ecuación (6.39),e su solución la podemos expresar como:\n\\[\\begin{eqnarray}\n    U_ t & = & - \\frac{\\mu}{1 - b_1} + \\frac{1}{1 - b_1 L} X_t \\nonumber \\\\\n    & = & - \\frac{\\mu}{1 - b_1} + X_t + b_1 X_{t-1} + b_1^2 X_{t-2} + \\ldots \\nonumber\n\\end{eqnarray}\\]Donde la condición para que se cumpla esta ecuación es que \\(\\lvert b_1 \\lvert< 1\\). La manera de interpretar esta condición es como una condición de estabilidad de la solución y cómo una condición de invertibilidad. Notemos que un \\(MA(1)\\) (y en general un \\(MA(q)\\)) es equivalente un \\(AR(\\infty)\\), es decir, cuando se invierte un MA se genera un AR con infinitos rezagos.En esta sección desarrollaremos un ejemplo, primero explicaremos en qué consiste una modelación del tipo \\(MA(q)\\) y después platearemos un ejemplo en concreto.","code":""},{"path":"procesos-estacionarios-univariados.html","id":"maq","chapter":"6 Procesos estacionarios univariados","heading":"6.3.2 MA(q)","text":"En general, el proceso de medias móviles de orden \\(q\\), \\(MA(q)\\), puede ser escrito como:\n\\[\\begin{equation}\n    X_t = \\mu + U_t - b_1 U_{t-1} - b_2 U_{t-2} - \\ldots - b_q U_{t-q}\n    \\tag{6.45}\n\\end{equation}\\]Podemos reescribir la ecuación (6.45) utilizando el operador rezago, así tendrémos el proceso de \\(MA(q)\\) como:\n\\[\\begin{eqnarray}\n    X_t - \\mu & = & (1 - b_1 L - b_2 L^2 - \\ldots - b_q L^q) U_{t} \\nonumber \\\\\n    X_t - \\mu & = & \\beta(L) U_t\n    \\tag{6.46}\n\\end{eqnarray}\\]Donde \\(U_t\\) es un proceso puramente aleatorio con \\(\\mathbb{E}[U_t] = 0\\), \\(Var[U_t] = \\mathbb{E}[U_t^2] = 0\\) y \\(Cov[U_t, U_s] = \\mathbb{E}[U_t, U_s] = 0\\), y \\(\\beta(L) = 1 - b_1 L - b_2 L^2 - \\ldots - b_q L^q\\) es un polinomio del operador rezago \\(L\\). la ecuación (6.46) puede ser interpretada como un proceso \\(AR(q)\\) sobre la serie \\(U_t\\).Ahora determinemos los momentos de un proceso \\(MA(q)\\):\n\\[\\begin{eqnarray}\n    \\mathbb{E}[X_t] & = & \\mathbb{E}[\\mu + U_t - b_1 U_{t-1} - b_2 U_{t-2} - \\ldots - b_q U_{t-q}] \\nonumber \\\\\n    & = & \\mu + \\mathbb{E}[U_t] - b_1 \\mathbb{E}[U_{t-1}] - b_2 \\mathbb{E}[U_{t-2}] - \\ldots - b_q \\mathbb{E}[U_{t-q}] \\nonumber \\\\\n    & = & \\mu\n    \\tag{6.47}\n\\end{eqnarray}\\]En el caso de la varianza tenemos que se puede expresar como:\n\\[\\begin{eqnarray}\n    Var[X_t] & = & \\mathbb{E}[(X_t - \\mu)^2] \\nonumber \\\\\n    & = & \\mathbb{E}[(U_t - b_1 U_{t-1} - b_2 U_{t-2} - \\ldots - b_q U_{t-q})^2] \\nonumber \\\\\n    & = & \\mathbb{E}[U_t^2 + b_1^2 U_{t-1}^2 + b_2^2 U_{t-2}^2 + \\ldots + b_q^2 U_{t-q}^2 \\nonumber \\\\\n    &   & - 2 b_1 U_t U_{t - 1} - \\ldots - 2 b_{q - 1} b_q U_{t - q + 1} U_{t - q}] \\nonumber \\\\\n    & = & \\mathbb{E}[U_t^2] + b_1^2 \\mathbb{E}[U_{t-1}^2] + b_2^2 \\mathbb{E}[U_{t-2}^2] + \\ldots + b_q^2 \\mathbb{E}[U_{t-q}^2] \\nonumber \\\\\n    &   & - 2 b_1 \\mathbb{E}[U_t U_{t - 1}] - \\ldots - 2 b_{q - 1} b_q \\mathbb{E}[U_{t - q + 1} U_{t - q}] \\nonumber \\\\\n    & = & \\sigma^2 + b^2_1 \\sigma^2 + b^2_2 \\sigma^2 + \\ldots + b^2_q \\sigma^2 \\nonumber \\\\\n    & = & (1 + b^2_1 + b^2_2 + \\ldots + b^2_q) \\sigma^2\n    \\tag{6.48}\n\\end{eqnarray}\\]En el caso de las covarianzas podemos utilizar una idea similar al caso del \\(AR(p)\\), construir una expresión general para cualquier rezago \\(\\tau\\):\n\\[\\begin{eqnarray}\n    Cov[X_t, X_{t + \\tau}] & = & \\mathbb{E}[(X_t - \\mu)(X_{t + \\tau} - \\mu)] \\nonumber \\\\\n    & = & \\mathbb{E}[(U_t - b_1 U_{t-1} - b_2 U_{t-2} - \\ldots - b_q U_{t-q}) \\nonumber \\\\\n    &   & (U_{t + \\tau} - b_1 U_{t + \\tau -1} - b_2 U_{t + \\tau -2} - \\ldots - b_q U_{t + \\tau - q})] \\nonumber\n\\end{eqnarray}\\]La expresión anterior se puede desarrollar para múltiples casos de \\(\\tau = 1, 2, \\ldots, q\\). De esta forma tenemos el siguiente sistema:\n\\[\\begin{eqnarray}\n    \\tau = 1 & : & \\gamma(1) = (- b_1 + b_1 b_2 + \\ldots + b_{q-1} b_q) \\sigma^2 \\nonumber \\\\\n    \\tau = 2 & : & \\gamma(2) = (- b_2 + b_1 b_3 + \\ldots + b_{q-2} b_q) \\sigma^2 \\nonumber \\\\\n    & \\vdots & \\nonumber \\\\\n    \\tau = q & : & \\gamma(q) = b_q \\sigma^2 \\nonumber\n\\end{eqnarray}\\]Donde \\(\\gamma(\\tau) = 0\\) para todo \\(\\tau > q\\). Es decir, todas las autocovarianzas y autocorrelaciones con ordenes superiores \\(q\\) son cero (0). De esta forma, esta caracterítica teórica permite identificar el orden de \\(MA(q)\\) visualizando la función de autocorrelación y verificando partir de cual valor de rezago la autocorrelación es significaiva.Regresando al problema original que es el de determinar una solución para la eucación @ref(eq:MAq_EQ), tenemos que dicha solución estará dada por un \\(AR(\\infty)\\) en términos de \\(U_t\\):\n\\[\\begin{eqnarray}\n    U_t & = & - \\frac{\\mu}{1 - b_1 - b_2 - \\ldots - b_q} + \\beta(L)^{-1} X_t \\nonumber \\\\\n    &   & - \\frac{\\mu}{1 - b_1 - b_2 - \\ldots - b_q} + \\sum_{j = 0}^{\\infty} c_j X_{t-j}\n    \\tag{6.49}\n\\end{eqnarray}\\]Donde se cumple que: \\(1 = (1 - b_1 L^1 - b_2 L^2 - \\ldots - b_q L^q)(1 - c_1 L - c_2 L^2 - \\ldots)\\) y los coeficientes \\(c_j\\) se pueden determinar por un método de coeficientes indeterminados y en términos de los valores \\(b_i\\). De igual forma que en el caso de la ecuación (6.24), en la ecuación (6.49) se deben cumplir condiciones de estabilidad asociadas con las raíces del polinomio carácterististico dado por:\n\\[\\begin{equation}\n    1 - b_1 x - b_2 x^2 - \\ldots b_q x^q = 0\n    \\tag{6.50}\n\\end{equation}\\]El cual debe cumplir que \\(\\lvert x_i \\lvert < 1\\) y que \\(1 - b_1 - b_2 - \\ldots b_q < 1\\).Ahora veamos el ejemplo.","code":""},{"path":"procesos-estacionarios-univariados.html","id":"ejemplo-maq","chapter":"6 Procesos estacionarios univariados","heading":"6.3.2.1 Ejemplo MA(q)","text":"Cuando nos fijamos en las figuras 6.9 y 6.10 es claro que hay un proceso MA(1). Es decir que picos de un año estan causando efectos en valores futuros. Así pues, escogemos un valor de \\(q=1\\).MA:Raices MA(1):\nFigure 6.14: Raices MA(1) Inversas de la serie de tiempo\nDado que el valor de la raiz inversa de MA(1) en la Figura 6.14 sabemos que tenemos una serie convergente y estable.Plot:\nFigure 6.15: Diferencia entre la serie de tiempo original de precios de AMZN y su AR(1)\nEn la Figura 6.15, es muy fácil ver que los efectos de los picos son muy bajos ahora, lo cual nos permitirá hacer mejores estimaciones. Evidentemente querremos hacer el proceso AR(p) y MA(q) simultaneamente para obtener los mejores resultados.","code":"\nMA_price_amazn_ts_1<-arima(price_amazn_ts,order=c(0,0,1),method = \"ML\")\nMA_price_amazn_pl_1<-Arima(price_amazn_ts,order=c(0,0,1),method = \"ML\")\nautoplot(MA_price_amazn_ts_1)+theme_light()\nplot(MA_price_amazn_pl_1$x,col=\"black\", main = \"Diferencia entre la serie de tiempo original y MA(1)\",xlab=\"Tiempo\",ylab=\"Precio\")+lines(fitted(MA_price_amazn_pl_1),col=\"red\")#> integer(0)"},{"path":"procesos-estacionarios-univariados.html","id":"procesos-armap-q-y-arimap-d-q","chapter":"6 Procesos estacionarios univariados","heading":"6.4 Procesos ARMA(p, q) y ARIMA(p, d, q)","text":"Hemos establecido algunas relaciones las de los porcesos AR y los procesos MA, es decir, cómo un \\(MA(q)\\) de la serie \\(X_t\\) puede ser reexpresada como un \\(AR(\\infty)\\) de la serie \\(U_t\\), y viceversa un \\(AR(p)\\) de la serie \\(X_t\\) puede ser reeexpresada como un \\(MA(\\infty)\\).En este sentido, para cerrar esta sección veámos el caso de la especificación que conjunta ambos modelos en un modelo general conocido como \\(ARMA(p, q)\\) o \\(ARIMA(p, d, q)\\). La diferencia entre el primero y el segundo es las veces que su tuvo que diferenciar la serie analizada, registro que se lleva en el índice \\(d\\) de los paramétros dentro del concepto \\(ARIMA(p, d, q)\\). obstante, en general nos referiremos al modelo como \\(ARMA(p, q)\\) y dependerá del analista si modela la serie en niveles (por ejemplo, en logaritmos) o en diferencias logarítmicas (o diferencias sin logaritmos).","code":""},{"path":"procesos-estacionarios-univariados.html","id":"arma1-1","chapter":"6 Procesos estacionarios univariados","heading":"6.4.1 ARMA(1, 1)","text":"Dicho lo anterior vamos empezar con el análisis de un \\(ARMA(1, 1)\\). Un proceso \\(ARMA(1, 1)\\) puede verse como:\n\\[\\begin{equation}\n    X_t = \\delta + a_1 X_{t - 1} + U_t - b_1 U_{t - 1}\n    \\tag{6.51}\n\\end{equation}\\]Aplicando el operado rezago podemos rescribir la ecuación (6.51) como:\n\\[\\begin{equation}\n    (1 - a_1 L) X_t = \\delta + (1 - b_1 L) U_t\n     \\tag{6.52}\n\\end{equation}\\]Donde \\(U_t\\) es un proceso pueramente aleatorio como en los casos de \\(AR(p)\\) y \\(MA(q)\\), y \\(X_t\\) puede ser una serie en niveles o en diferencias (ambas, en términos logarítmicos).Así, el modelo \\(ARIMA (p, q)\\) también tiene una representación de Wold que estará dada por las siguientes expresiones:\n\\[\\begin{equation}\n    X_t = \\frac{\\delta}{1 - a_1} + \\frac{1 - b_1 L}{1 - a_1 L} U_t\n    \\tag{6.53}\n\\end{equation}\\]Donde \\(a_1 \\neq b_1\\), puesto que en caso contrario \\(X_t\\) sería un proceso puramente aleatorio con una media \\(\\mu = \\frac{\\delta}{1 - a_1}\\). Así, podemos reescribir la descomposición de Wold partir del componente de la ecuación (6.53)\n\\[\\begin{equation}\n    \\frac{1 - b_1 L}{1 - a_1 L} = \\psi_0 + \\psi_1 L + \\psi_2 L^2 + \\psi_3 L^3 + \\ldots\n    \\tag{6.54}\n\\end{equation}\\]Está ecuación es equivalente la expresión:\n\\[\\begin{eqnarray}\n    (1 - b_1 L) & = & (1 - a_1 L)(\\psi_0 + \\psi_1 L + \\psi_2 L^2 + \\psi_3 L^3 + \\ldots) \\nonumber \\\\\n    & = & \\psi_0 + \\psi_1 L + \\psi_2 L^2 + \\psi_3 L^3 + \\ldots \\nonumber \\\\\n    &   & - a_1 \\psi_0 L - a_1 \\psi_1 L^2 - a_2 \\psi_2 L^3 - a_1 \\psi_3 L^4 - \\ldots \\nonumber\n\\end{eqnarray}\\]De esta forma podemos establecer el siguiente sistema de coeficientes indeterminados:\n\\[\\begin{eqnarray}\n    L^0 : &   & \\Rightarrow  &  \\psi_0 = 1  \\\\\n     L^1 :  &  \\psi_1 - a_1 \\psi_0 = - b_1  &  \\Rightarrow  &  \\psi_1 = a_1 - b_1  \\\\\n     L^2 :  &  \\psi_2 - a_1 \\psi_1 = 0  &  \\Rightarrow  &  \\psi_2 = a_1(a_1 - b_1)  \\\\\n     L^3 :  &  \\psi_3 - a_1 \\psi_2 = 0  &  \\Rightarrow  &  \\psi_3 = ^2_1(a_1 - b_1)  \\\\\n     \\vdots  &  \\vdots  &  \\vdots  &  \\vdots  \\\\\n     L^j :  &  \\psi_j - a_1 \\psi_{j - 1} = 0  &  \\Rightarrow  &  \\psi_j = ^{j - 1}_1(a_1 - b_1)\n\\end{eqnarray}\\]Así, la solución la ecuación (6.51) estará dada por la siguiente generalización:\n\\[\\begin{equation}\n    X_t = \\frac{\\delta}{1 - a_1} + U_t + (a_1 - b_1) U_{t - 1} + a_1(a_1 - b_1) U_{t - 2} + a_1^2(a_1 - b_1) U_{t - 3} + \\ldots\n\\tag{6.55}\n\\end{equation}\\]En la ecuación (6.55) las condiciones de estabilidad y de invertibilidad del sistema (de un MA un AR, y viceversa) estarán dadas por: \\(\\lvert a_1 \\lvert < 1\\) y \\(\\lvert b_1 \\lvert< 1\\). Adicionalmente, la ecuación (6.55) expresa cómo una serie que tiene un comportamiento \\(ARMA(1, 1)\\) es equivalente una serie modelada bajo un \\(MA(\\infty)\\).Al igual que en los demás modelos, ahora determinaremos los momentos del proceso \\(ARMA(1, 1)\\). La media estará dada por:\n\\[\\begin{eqnarray}\n    \\mathbb{E}[X_t] & = & \\mathbb{E}[\\delta + a_1 X_{t-1} + U_t - b_1 U_{t-1}] \\nonumber \\\\\n    & = & \\delta + a_1 \\mathbb{E}[X_{t-1}] \\nonumber \\\\\n    & = & \\frac{\\delta}{1 - a_1} \\nonumber \\\\\n    & = & \\mu\n    \\tag{6.56}\n\\end{eqnarray}\\]Donde hemos utilizado que \\(\\mathbb{E}[X_t] = \\mathbb{E}[X_{t-1}] = \\mu\\). Es decir, la media de un \\(ARMA(1, 1)\\) es idéntica la de un \\(AR(1)\\).Para determinar la varianza tomaremos una estrategía similar los casos de \\(AR(p)\\) y \\(MA(q)\\). Por lo que para todo \\(\\tau \\geq 0\\), y suponiendo por simplicidad que \\(\\delta = 0\\) (lo que implica que \\(\\mu = 0\\)) tendremos:\n\\[\\begin{eqnarray}\n    \\mathbb{E}[X_{t-\\tau} X_t] & = & \\mathbb{E}[(X_{t-\\tau}) \\cdot (a_1 X_{t-1} + U_t - b_1 U_{t-1})] \\nonumber \\\\\n    & = & a_1 \\mathbb{E}[X_{t-\\tau} X_{t-1}] + \\mathbb{E}[X_{t-\\tau} U_t] - b_1 \\mathbb{E}[X_{t-\\tau} U_{t-1}]\n    \\tag{6.57}\n\\end{eqnarray}\\]De la ecuación (6.57) podemos determinar una expresión para el caso de \\(\\tau = 0\\):\n\\[\\begin{eqnarray}\n    \\mathbb{E}[X_{t} X_t] & = & \\gamma(0) \\nonumber \\\\\n    & = & a_1 \\gamma(1) + \\mathbb{E}[U_t X_t] - b_1 \\mathbb{E}[X_t U_{t-1}] \\nonumber \\\\\n    & = & a_1 \\gamma(1) + \\sigma^2 + b_1 \\mathbb{E}[U_{t-1} (a_1 X_{t-1} + U_t - b_1 U_{t-1})] \\nonumber \\\\\n    & = & a_1 \\gamma(1) + \\sigma^2 - b_1 a_1 \\sigma^2 + b_1 \\sigma^2 \\nonumber \\\\\n    & = & a_1 \\gamma(1) + (1 - b_1 (a_1 - b_1)) \\sigma^2\n        \\tag{6.58}\n\\end{eqnarray}\\]Para el caso en que \\(\\tau = 1\\):\n\\[\\begin{eqnarray}\n    \\mathbb{E}[X_{t-1} X_t] & = & \\gamma(1) \\nonumber \\\\\n    & = & a_1 \\gamma(0) + \\mathbb{E}[X_{t-1} U_t] - b_1 \\mathbb{E}[X_{t-1} U_{t-1}] \\nonumber \\\\\n    & = & a_1 \\gamma(0) - b_1 \\sigma^2\n        \\tag{6.59}\n\\end{eqnarray}\\]Estas últimas expresiones podemos resolverlas como sistema para determinar los siguientes valores:\n\\[\\begin{eqnarray}\n    \\gamma(0) & = & \\frac{1 + b_1^2 - 2 a_1 b_1}{1 - a_1^2} \\sigma^2\n    \\tag{6.60}\n\\end{eqnarray}\\]\\[\\begin{eqnarray}\n    \\gamma(1) & = & \\frac{(a_1 - b_1)(1 - a_1 b_1)}{1 - a_1^2} \\sigma^2\n    \\tag{6.61}\n\\end{eqnarray}\\]En general para cualquier valor \\(\\tau \\geq 2\\) tenemos que la autocovarianza y la función de autocorrelación serán:\n\\[\\begin{eqnarray}\n    \\gamma(\\tau) = a_1 \\gamma(\\tau - 1) \\\\\n    \\tag{6.62}\n    \\end{eqnarray}\\]\\[\\begin{eqnarray}    \n    \\rho(\\tau) = a_1 \\rho(\\tau - 1)\n    \\tag{6.63}\n\\end{eqnarray}\\]Por ejemplo, para el caso de \\(\\tau = 1\\) tendríamos:\n\\[\\begin{equation}\n    \\rho(1) = \\frac{(a_1 - b_1)(1 - a_1 b_1)}{1 + b_1^2 - 2 a_1 b_1}\n    \\tag{6.64}\n\\end{equation}\\]De esta forma, la función de autocorrelación oscilará en razón de los valores que tome \\(a_1\\) y \\(b_1\\).","code":""},{"path":"procesos-estacionarios-univariados.html","id":"armap-q","chapter":"6 Procesos estacionarios univariados","heading":"6.4.2 ARMA(p, q)","text":"La especificación general de un \\(ARMA(p, q)\\) (donde \\(p, q \\\\mathbb{N}\\)) puede ser descrita por la siguiente ecuación:\n\\[\\begin{eqnarray}\n    X_t & = & \\delta + a_1 X_{t - 1} + a_2 X_{t - 2} + \\ldots + a_p X_{t - p} \\nonumber \\\\\n    &   & + U_t - b_1 U_{t - 1} - b_2  U_{t - 2} - \\ldots - b_q  U_{t - q}\n    \\tag{6.65}\n\\end{eqnarray}\\]Donde \\(U_t\\) es un proceso puramente aleatorio, y \\(X_t\\) puede ser modelada en niveles o en diferencias (ya sea en logaritmos o sin transformación logarítmica).Mediante el uso del operador rezago se puede escribir la ecuación (6.65) como:\n\\[\\begin{equation}\n    (1 - a_1 L - a_2 L^2 - \\ldots - a_p L^p) X_t = \\delta + (1 - b_1 L - b_2 L^2 - \\ldots - b_q L^q) U_t\n    \\tag{6.66}\n\\end{equation}\\]En la ecuación (6.66) definamos dos polinomios: \\(\\alpha(L) = (1 - a_1 L - a_2 L^2 - \\ldots - a_p L^p)\\) y \\(\\beta(L) = (1 - b_1 L - b_2 L^2 - \\ldots - b_q L^q)\\). Así, podemos reescribir la ecuación (6.66) como:\n\\[\\begin{equation}\n    \\alpha(L) X_t = \\delta + \\beta(L) U_t\n    \\tag{6.67}\n\\end{equation}\\]Asumiendo que existe el polinomio inverso tal que: \\(\\alpha(L)^{-1}\\alpha(L) = 1\\).La solución entonces puede ser escrita como:\n\\[\\begin{eqnarray}\n    X_t & = & \\alpha(L)^{-1} \\delta + \\alpha(L)^{-1} \\beta(L) U_t \\nonumber \\\\\n    & = & \\frac{\\delta}{1 - a_1 - a_2 - \\ldots - a_p} + \\frac{\\beta(L)}{\\alpha(L)} U_t \\nonumber \\\\\n    & = & \\frac{\\delta}{1 - a_1 - a_2 - \\ldots - a_p} + U_t + \\psi_1 L U_t + \\psi_2 L^2 U_t + \\ldots\n    \\tag{6.68}\n\\end{eqnarray}\\]Donde la ecuación (6.68) nos permite interpretar que un ARMA(p, q) se puede reexpresar e interpreetar como un \\(MA(\\infty)\\) y donde las condiciones para la estabilidad de la solución y la invertibilidad es que las ráices de los polinomios característicos \\(\\alpha(L)\\) y \\(\\beta(L)\\) son en valor absoluto menores 1.Adicionalmente, la fracción en la ecuación (6.68) se puede descomponer como en la forma de Wold:\n\\[\\begin{equation}\n    \\frac{\\beta(L)}{\\alpha(L)} = 1 + \\psi_1 L + \\psi_2 L^2 + \\ldots\n    \\tag{6.69}\n\\end{equation}\\]Bajo los supuestos de estacionariedad del componente \\(U_t\\), los valores de la media y varianza de un proceso \\(ARMA(p, q)\\) serán como describimos ahora. Para el caso de la media podemos partir de la ecuación (6.68) para generar:\n\\[\\begin{eqnarray}\n    \\mathbb{E}[X_t] & = & \\mathbb{E}\\left[ \\frac{\\delta}{1 - a_1 - a_2 - \\ldots - a_p} + U_t + \\psi_1 U_{t-1} + \\psi_2 U_{t-2} + \\ldots \\right] \\nonumber \\\\\n    & = & \\frac{\\delta}{1 - a_1 - a_2 - \\ldots - a_p} \\nonumber \\\\\n    & = & \\mu\n    \\tag{6.70}\n\\end{eqnarray}\\]Esta expresión indica que en general un proceso \\(ARMA(p, q)\\) converge una media idéntica la de un porceso \\(AR(p)\\). Para determinar la varianza utilizaremos la misma estratégia que hemos utilizado para otros modelos \\(AR(p)\\) y \\(MA(q)\\).Sin pérdida de generalidad podemos asumir que \\(\\delta = 0\\), lo que implica que \\(\\mu = 0\\), de lo que podemos establecer una expresión de autocovarianzas para cualquier valor \\(\\tau = 0, 1, 2, \\ldots\\):\n\\[\\begin{eqnarray}\n    \\gamma(\\tau) & = & \\mathbb{E}[X_{t-\\tau} X_t] \\nonumber \\\\\n    & = & \\mathbb{E}[X_{t-\\tau} (\\delta + a_1 X_{t - 1} + a_2 X_{t - 2} + \\ldots + a_p X_{t - p} \\nonumber \\\\\n    &   & + U_t - b_1 U_{t - 1} - b_2  U_{t - 2} - \\ldots - b_q  U_{t - q})] \\nonumber \\\\\n    & = & a_1 \\gamma(\\tau - 1) + a_2 \\gamma(\\tau - 2) + \\ldots + a_p \\gamma(\\tau - p) \\nonumber \\\\\n    &   & + \\mathbb{E}[X_{t-\\tau} U_{t}] - b_1  \\mathbb{E}[X_{t-\\tau} U_{t-1}] - \\ldots  - b_q  \\mathbb{E}[X_{t-\\tau} U_{t-q}]\n    \\tag{6.71}\n\\end{eqnarray}\\]","code":""},{"path":"procesos-estacionarios-univariados.html","id":"selección-de-las-constantes-p-q-d-en-un-arp-un-maq-un-armap-q-o-un-arimap-d-q","chapter":"6 Procesos estacionarios univariados","heading":"6.5 Selección de las constantes p, q, d en un AR(p), un MA(q), un ARMA(p, q) o un ARIMA(p, d, q)","text":"Respecto de cómo estimar un proceso ARMA(p, q) –en general utilizaremos este modelo para discutir, pero lo planteado en esta sección es igualmente aplicable en cualquier otro caso como aquellos modelos que incluyen variables exogénas– existen diversas formas de estimar los paramétros \\(a_i\\) y \\(b_i\\): ) por máxima verosimilitd y ii) por mínimos cuadrados órdinarios. El primer caso requiere que conozcamos la distribución del proceso aleatorio \\(U_t\\). El segundo, por el contrario, requiere el mismo supuesto. obstante, para el curso utilizaremos el método de máxima verosimilitud.Otra duda que debe quedar hasta el momento es ¿cómo determinar el orden \\(p\\) y \\(q\\) del proceso ARMA(p, q)? La manera más convencional y formal que existe para tal efecto es utilizar los criterios de información. Así, el orden se elije de acuerdo aquel críterio de información que resulta ser el mínimo. En el caso de \\(d\\) se selecciona revisando la gráfica que parezca más estacionaria–más adelante mostraremos un proceso más formal para su selección.Los criterios de información más comunes son los siguientes:FPE (Final Prediction Error):\n\\[\\begin{equation}\nFPE = \\frac{T+m}{T-m}\\frac{1}{T}\\sum_{t=1}^{T} \\left( \\hat{U}_t^{(p)} \\right) ^2\n\\tag{6.72}\n\\end{equation}\\]FPE (Final Prediction Error):\n\\[\\begin{equation}\nFPE = \\frac{T+m}{T-m}\\frac{1}{T}\\sum_{t=1}^{T} \\left( \\hat{U}_t^{(p)} \\right) ^2\n\\tag{6.72}\n\\end{equation}\\]Akaike:\n\\[\\begin{equation}\nAIC = ln \\left[ \\frac{1}{T} \\sum_{t=1}^{T} \\left( \\hat{U}_t^{(p)} \\right) ^2 \\right] + m \\frac{2}{T}\n     \\tag{6.73}\n\\end{equation}\\]Akaike:\n\\[\\begin{equation}\nAIC = ln \\left[ \\frac{1}{T} \\sum_{t=1}^{T} \\left( \\hat{U}_t^{(p)} \\right) ^2 \\right] + m \\frac{2}{T}\n     \\tag{6.73}\n\\end{equation}\\]Schwarz:\n\\[\\begin{equation}\nSC = ln \\left[ \\frac{1}{T} \\sum_{t=1}^{T} \\left( \\hat{U}_t^{(p)} \\right) ^2 \\right] + m \\frac{ln(T)}{T}\n   \\tag{6.74}\n\\end{equation}\\]Schwarz:\n\\[\\begin{equation}\nSC = ln \\left[ \\frac{1}{T} \\sum_{t=1}^{T} \\left( \\hat{U}_t^{(p)} \\right) ^2 \\right] + m \\frac{ln(T)}{T}\n   \\tag{6.74}\n\\end{equation}\\]Hannan - Quinn:\n\\[\\begin{equation}\nHQ = ln \\left[ \\frac{1}{T} \\sum_{t=1}^{T} \\left( \\hat{U}_t^{(p)} \\right) ^2 \\right] + m \\frac{2 ln(ln(T))}{T}\n   \\tag{6.75}\n\\end{equation}\\]Hannan - Quinn:\n\\[\\begin{equation}\nHQ = ln \\left[ \\frac{1}{T} \\sum_{t=1}^{T} \\left( \\hat{U}_t^{(p)} \\right) ^2 \\right] + m \\frac{2 ln(ln(T))}{T}\n   \\tag{6.75}\n\\end{equation}\\]Donde \\(\\hat{U}_t^{(p)}\\) son los residuales estimados mediante un proceso ARIMA y \\(m\\) es el número de parametros estimados: \\(m = p + q + 0 + 1\\) (ya que asumimos que \\(d = 0\\)). Una propiedad que se debe perder de vista es que los criterios de información cumplen la siguiente relación:\n\\[\\begin{equation}\n    orden(SC) \\leq orden(HQ) \\leq orden(AIC)\n    \\tag{6.76}\n\\end{equation}\\]Por esta razón, durante el curso solo utilizaremos el criterio se Akaike para determinar el orden óptimo del proceso ARMA, ya que ello garantiza el orden más grande posible.Veamos el ejemplo.","code":""},{"path":"procesos-estacionarios-univariados.html","id":"ejemplo-arma","chapter":"6 Procesos estacionarios univariados","heading":"6.5.1 Ejemplo ARMA","text":"Para este caso coemzaremos por agregar dos series de tiempo. Una correspone una transformación logarítmica de los valores de los precios y, otra, corresponde la diferentcia logaritmica. Esto dado que:\n\\[log(X_t)-log(X_{t-k})\\sim\\frac{X_t-X_{t-k}}{X_t}\\].Tranformacion de la serie originalLas tres seriesEs claro que este pico se da en 2020 desde junio hasta julio, probablemente causado por el aumento del uso en amzn durante la pandemia. Esos son outlier que debemos considerar. Por ello hay que marcarlos con una variable dummy.Como lo vimos en las figuras 6.9 y 6.10, es claro que debemos usar un valor \\(q\\) de \\(1\\) y \\(p\\) de \\(0\\), esto debido que cuando vemos 6.9 hay una correlación de ninguno de los valores. Asímismo, también necesitamos diferenciar, lo cual corresponde al valor \\(d\\). Así pues, dado que en las funciones de autocorrelacion solo vemos uno o dos picos arriba de la linea punteada azul, podemos asumir que el valor de diferenciación debe ser uno o dos, en este caso dos. Esto se debe seguir como regla de dedo, pero en general se debe usar el valor que minimice la desviacion estandar y también el que minimice AIC. Por tanto nuestro modelo debe ser \\(arima(0,2,1)\\).Utilizamos la funcion “auto.arima” para confirmar que nuestra serie de tiempo deba ser \\(arima(0,2,1)\\), el arima que debos usar es aquel que minimize el indice AIC.\nARMA(0,2,1) de los precios de apertura de AMZN\nRaices MA():\nFigure 6.16: Raices ARMA(1) Inversas de la serie de tiempo\nDado esto, sabemos claramente que podremos analizar de mejor manera estos valores y, en consecuencia, hacer mejores estimaciones.","code":"\n#original\nprice_amazn_ts<-ts(data_precio_amzn$price.open, frequency = 12, start=c(2002,09))\n#logartimo\nlprice_amazn_ts<-ts(log(data_precio_amzn$price.open), frequency = 12)\n#diferencias logaritmicas(cambio porcential)\ndlprice_amazn_ts<-ts(log(data_precio_amzn$price.open)-lag(log(data_precio_amzn$price.open),1), frequency = 12)\npar(mfrow = c(3,1))\nplot(price_amazn_ts, xlab = \"Tiempo\", \n     main = \"Precios de apertura\",\n     col = \"darkgreen\")\n\nplot(lprice_amazn_ts, xlab = \"Tiempo\", \n     main = \"LN Precios de apertura\",\n     col = \"darkblue\")\n\nplot(dlprice_amazn_ts, xlab = \"Tiempo\", \n     main = \"Diff LN de precios de apertura\", \n     col = \"darkred\")\n#Generamos el rango de tiempo\njunio2020 <- seq.Date(\n  from=as.Date(\"2020-06-01\"),\n  to=as.Date(\"2020-6-29\"),\n  by=\"day\")\njulio2020 <- seq.Date(\n  from=as.Date(\"2020-07-01\"),\n  to=as.Date(\"2020-07-29\"),\n  by=\"day\")\nagosto2020 <- seq.Date(\n  from=as.Date(\"2020-08-01\"),\n  to=as.Date(\"2020-08-29\"),\n  by=\"day\")\nsept2020 <- seq.Date(\n  from=as.Date(\"2020-09-01\"),\n  to=as.Date(\"2020-09-29\"),\n  by=\"day\")\noct2020 <- seq.Date(\n  from=as.Date(\"2020-10-01\"),\n  to=as.Date(\"2020-10-29\"),\n  by=\"day\")\nnov2020 <- seq.Date(\n  from=as.Date(\"2020-11-01\"),\n  to=as.Date(\"2020-11-29\"),\n  by=\"day\")\ndiciembre2020 <- seq.Date(\n  from=as.Date(\"2020-12-01\"),\n  to=as.Date(\"2020-12-29\"),\n  by=\"day\")\n#Añadimos valores 1 y 0 dependiento si estan dentro(1) o no\ndata_precio_amzn$junio2020<-ifelse(data_precio_amzn$ref.date%in%junio2020,1,0)\ndata_precio_amzn$julio2020<-ifelse(data_precio_amzn$ref.date%in%julio2020,1,0)\ndata_precio_amzn$agosto2020<-ifelse(data_precio_amzn$ref.date%in%agosto2020,1,0)\ndata_precio_amzn$sept2020<-ifelse(data_precio_amzn$ref.date%in%sept2020,1,0)\ndata_precio_amzn$oct2020<-ifelse(data_precio_amzn$ref.date%in%oct2020,1,0)\ndata_precio_amzn$nov2020<-ifelse(data_precio_amzn$ref.date%in%nov2020,1,0)\ndata_precio_amzn$diciembre2020<-ifelse(data_precio_amzn$ref.date%in%diciembre2020,1,0)\n#ts\njunio2020ts<-ts(data_precio_amzn$junio2020, frequency = 12, start=c(2002,10))\njulio2020ts<-ts(data_precio_amzn$julio2020, frequency = 12, start=c(2002,10))\nagosto2020ts<-ts(data_precio_amzn$agosto2020, frequency = 12, start=c(2002,10))\nsep2020ts<-ts(data_precio_amzn$sept2020, frequency = 12, start=c(2002,10))\noct2020ts<-ts(data_precio_amzn$oct2020, frequency = 12, start=c(2002,10))\nnov2020ts<-ts(data_precio_amzn$nov2020, frequency = 12, start=c(2002,10))\ndiciembre2020ts<-ts(data_precio_amzn$diciembre2020, frequency = 12, start=c(2002,10))\n#espacio de prediccion\n#dummies\ntimepred <- seq.Date(\n  from=as.Date(\"2022-10-01\"),\n  to=as.Date(\"2023-10-10\"),\n  by=\"month\")\njunio2020 <- rep(0,13)\njulio2020 <- rep(0,13)\nagosto2020 <- rep(0,13)\nsep2020 <- rep(0,13)\noct2020 <- rep(0,13)\nnov2020 <- rep(0,13)\ndiciembre2020 <- rep(0,13)\n#data frame\npred.df <- data.frame(timepred,junio2020,julio2020, agosto2020,sep2020, oct2020, nov2020, diciembre2020)\n#dummies\nd.enero <- seq.Date(\n  from=as.Date(\"2023-01-01\"),\n  to=as.Date(\"2023-01-29\"),\n  by=\"day\")\nd.junio <- seq.Date(\n  from=as.Date(\"2023-06-01\"),\n  to=as.Date(\"2023-06-29\"),\n  by=\"day\")\nd.dic <- seq.Date(\n  from=as.Date(\"2023-12-01\"),\n  to=as.Date(\"2023-12-29\"),\n  by=\"day\")\npred.df$d.enero<-ifelse(pred.df$timepred%in%d.enero,1,0)\npred.df$d.junio<-ifelse(pred.df$timepred%in%d.junio,1,0)\npred.df$d.dic<-ifelse(pred.df$timepred%in%d.dic,1,0)\n#series de tiempo predictivas\nf.junio2020 <- ts(pred.df$junio2020, frequency = 12, start=c(2022,10))\nf.agosto2020 <- ts(pred.df$junio2020, frequency = 12, start=c(2022,10))\nf.sep2020 <- ts(pred.df$junio2020, frequency = 12, start=c(2022,10))\nf.oct2020 <- ts(pred.df$junio2020, frequency = 12, start=c(2022,10))\nf.nov2020 <- ts(pred.df$junio2020, frequency = 12, start=c(2022,10))\nf.julio2020 <- ts(pred.df$junio2020, frequency = 12, start=c(2022,10))\nf.enero <- ts(pred.df$d.enero, frequency = 12, start=c(2022,10))\nf.junio <- ts(pred.df$d.junio, frequency = 12, start=c(2022,10))\nf.diciembre <- ts(pred.df$d.dic, frequency = 12, start=c(2022,10))\nauto.arima(price_amazn_ts, trace=TRUE)\n#> \n#>  Fitting models using approximations to speed things up...\n#> \n#>  ARIMA(2,2,2)(1,0,1)[12]                    : Inf\n#>  ARIMA(0,2,0)                               : 1458.056\n#>  ARIMA(1,2,0)(1,0,0)[12]                    : 1396.532\n#>  ARIMA(0,2,1)(0,0,1)[12]                    : 1296.885\n#>  ARIMA(0,2,1)                               : 1295.472\n#>  ARIMA(0,2,1)(1,0,0)[12]                    : 1308.746\n#>  ARIMA(0,2,1)(1,0,1)[12]                    : Inf\n#>  ARIMA(1,2,1)                               : 1297.615\n#>  ARIMA(0,2,2)                               : 1296.649\n#>  ARIMA(1,2,0)                               : 1382.899\n#>  ARIMA(1,2,2)                               : 1299.616\n#> \n#>  Now re-fitting the best model(s) without approximations...\n#> \n#>  ARIMA(0,2,1)                               : 1305.63\n#> \n#>  Best model: ARIMA(0,2,1)\n#> Series: price_amazn_ts \n#> ARIMA(0,2,1) \n#> \n#> Coefficients:\n#>           ma1\n#>       -0.9716\n#> s.e.   0.0140\n#> \n#> sigma^2 = 17.95:  log likelihood = -650.79\n#> AIC=1305.58   AICc=1305.63   BIC=1312.43\n#sin embargo hay que controlar con nuestra variable dummy\nARIMA_price_amzn_ts_111<-arima(price_amazn_ts,order=c(0,2,1),\n                               method = \"ML\",xreg=cbind(julio2020ts,agosto2020ts,sep2020ts,oct2020ts))\nARIMA_price_amzn_pl_111<-Arima(price_amazn_ts,order=c(0,2,1),\n                               method = \"ML\",xreg=cbind(julio2020ts,agosto2020ts,sep2020ts,oct2020ts))\nautoplot(ARIMA_price_amzn_ts_111)+theme_light()"},{"path":"procesos-estacionarios-univariados.html","id":"pronósticos","chapter":"6 Procesos estacionarios univariados","heading":"6.5.2 Pronósticos","text":"Para pronósticar el valor de la serie es necesario determinar cuál es el valor esperado de la serie en un momento \\(t + \\tau\\) condicional en que ésta se comporta como un \\(AR(p)\\), un \\(MA(q)\\) o un \\(ARMA(p, q)\\) y que los valores antes de \\(t\\) están dados. Por lo que el pronóstico de la serie estará dado por una expresión:\\[\\begin{eqnarray}\n    \\mathbb{E}_t[X_{t+\\tau}] = \\delta + a_1 \\mathbb{E}_t[X_{t+\\tau-1}] + a_2 \\mathbb{E}_t[X_{t+\\tau-2}] + \\ldots + + a_p \\mathbb{E}_t[X_{t+\\tau-p}]\n    \\tag{6.77}\n\\end{eqnarray}\\]Valores:\nFigure 6.17: Predicción de \\(Arima(1,1,1)\\)\n","code":"\nARIMA_price_amzn_ts_111_F<-predict(ARIMA_price_amzn_ts_111,n.ahead=13,newxreg=cbind(f.julio2020,f.agosto2020,f.sep2020,f.oct2020))\n\nforecast.Arima <- forecast(ARIMA_price_amzn_pl_111,h=13, xreg=cbind(f.julio2020,f.agosto2020,f.sep2020,f.oct2020))\nkable(forecast.Arima,\"html\")\nplot(forecast.Arima, main =\"Forecast ARIMA(0,2,1) de los precios de AMZN\")"},{"path":"estacionalidad.html","id":"estacionalidad","chapter":"7 Estacionalidad","heading":"7 Estacionalidad","text":"Recordando el tema pasado y la serie en la que evaluamos los cambios de\nprecio del ACTIVO AMZN como si fueran retornos:Para este caso coemzaremos por agregar dos series de tiempo. Una correspone una transformación logarítmica de los valores de los precios y, otra, corresponde la diferentcia logaritmica. Esto dado que:\n\\[log(X_t)-log(X_{t-k})\\sim\\frac{X_t-X_{t-k}}{X_t}\\].Tranformacion de la serie originalLas tres series\nFigure 7.1: Serie de timpo LN, DiffLn y AMZN\nTomemos la de diferencias logarítmicas como ejemplo.\nFigure 7.2: Diff LN de precios de apertura de AMZN\nAquí es posible ver que hay un proceso estacional muy claro. Cuando vemos el plot (6.51) es dificil verlo con exactitud. Sin embargo altas y bajas constantes como lo vemos en la figura son buenos indicadores de que puede haber un proceso estacional: de estaciones.Como lo sospechabamos este proceso es puramente estacional y se utilizan estaciones AR(p) y MA(p), pero de manera estacional.\nARMA(1,1,1) de los precios de apertura de AMZN\nRaices MA():\nFigure 6.16: Raices ARMA(1) Inversas de la serie de tiempo\n$pred\nJan Feb Mar Apr\n2021\n2022 0.024195377 0.007637307 0.045710596 0.023410540\nMay Jun Jul Aug\n2021\n2022 0.027143074 0.036196514 0.035377125 0.031611992\nSep Oct Nov Dec\n2021 0.033631775 0.023154560\n2022 0.043629025 0.025191369 0.006187273$se\nJan Feb Mar Apr May\n2021\n2022 0.1035781 0.1035781 0.1035781 0.1035781 0.1035781\nJun Jul Aug Sep Oct\n2021\n2022 0.1035781 0.1035781 0.1035781 0.1035781 0.1035781\nNov Dec\n2021 0.1035781 0.1035781\n2022 0.1047972Valores:\nFigure 6.17: Predicción de \\(Arima(1,1,1)\\)\n","code":"\n#install.packages(\"pacman\")\n#pacman nos permite cargar varias librerias en una sola línea\nlibrary(pacman)\npacman::p_load(tidyverse,BatchGetSymbols,ggplot2,lubridate,readxl,forecast,stats,stargazer,knitr)\n#Primero determinamos el lapso de tiempo\npd<-as.Date(\"2002/9/30\") #primer fecha\npd\n#> [1] \"2002-09-30\"\n#> [1] \"2021-09-18\"\nld<- as.Date(\"2021/09/30\")#última fecha\nld\n#> [1] \"2021-09-30\"\n#Intervalos de tiempo\nint<-\"monthly\"\n\n#Datos a elegir\ndt<-c(\"AMZN\")\n\n#Descargando los valores\ndata1<- BatchGetSymbols(tickers = dt,\n                       first.date = pd,\n                       last.date = ld,\n                       freq.data = int,\n                       do.cache = FALSE,\n                       thresh.bad.data = 0)\n\n#Generando data frame con los valores\ndata_precio_amzn<-data1$df.tickers\ncolnames(data_precio_amzn)\n#>  [1] \"ticker\"              \"ref.date\"           \n#>  [3] \"volume\"              \"price.open\"         \n#>  [5] \"price.high\"          \"price.low\"          \n#>  [7] \"price.close\"         \"price.adjusted\"     \n#>  [9] \"ret.adjusted.prices\" \"ret.closing.prices\"\n#original\nprice_amazn_ts<-ts(data_precio_amzn$price.open, frequency = 12, start=c(2002,09))\n#logartimo\nlprice_amazn_ts<-ts(log(data_precio_amzn$price.open), frequency = 12, , start=c(2002,09))\n#diferencias logaritmicas(cambio porcential)\ndlprice_amazn_ts<-ts(log(data_precio_amzn$price.open)-lag(log(data_precio_amzn$price.open),1), frequency = 12, , start=c(2002,10))\npar(mfrow = c(3,1))\nplot(price_amazn_ts, xlab = \"Tiempo\", \n     main = \"Precios de apertura\",\n     col = \"darkgreen\")\n\nplot(lprice_amazn_ts, xlab = \"Tiempo\", \n     main = \"LN Precios de apertura\",\n     col = \"darkblue\")\n\nplot(dlprice_amazn_ts, xlab = \"Tiempo\", \n     main = \"Diff LN de precios de apertura\", \n     col = \"darkred\")\nplot(dlprice_amazn_ts, xlab = \"Tiempo\", \n     main = \"Diff LN de precios de apertura\", \n     col = \"darkred\")\nauto.arima(dlprice_amazn_ts, trace=TRUE)\n#> \n#>  Fitting models using approximations to speed things up...\n#> \n#>  ARIMA(2,0,2)(1,0,1)[12] with non-zero mean : Inf\n#>  ARIMA(0,0,0)            with non-zero mean : -372.9626\n#>  ARIMA(1,0,0)(1,0,0)[12] with non-zero mean : -383.6435\n#>  ARIMA(0,0,1)(0,0,1)[12] with non-zero mean : -373.4978\n#>  ARIMA(0,0,0)            with zero mean     : -364.0697\n#>  ARIMA(1,0,0)            with non-zero mean : -370.14\n#>  ARIMA(1,0,0)(2,0,0)[12] with non-zero mean : -391.6751\n#>  ARIMA(1,0,0)(2,0,1)[12] with non-zero mean : -396.7544\n#>  ARIMA(1,0,0)(1,0,1)[12] with non-zero mean : -381.5713\n#>  ARIMA(1,0,0)(2,0,2)[12] with non-zero mean : -395.4488\n#>  ARIMA(1,0,0)(1,0,2)[12] with non-zero mean : -382.5532\n#>  ARIMA(0,0,0)(2,0,1)[12] with non-zero mean : -400.4947\n#>  ARIMA(0,0,0)(1,0,1)[12] with non-zero mean : -384.7126\n#>  ARIMA(0,0,0)(2,0,0)[12] with non-zero mean : -394.407\n#>  ARIMA(0,0,0)(2,0,2)[12] with non-zero mean : -399.2569\n#>  ARIMA(0,0,0)(1,0,0)[12] with non-zero mean : -386.7607\n#>  ARIMA(0,0,0)(1,0,2)[12] with non-zero mean : -385.6341\n#>  ARIMA(0,0,1)(2,0,1)[12] with non-zero mean : -398.8431\n#>  ARIMA(1,0,1)(2,0,1)[12] with non-zero mean : -394.6581\n#>  ARIMA(0,0,0)(2,0,1)[12] with zero mean     : -391.1477\n#> \n#>  Now re-fitting the best model(s) without approximations...\n#> \n#>  ARIMA(0,0,0)(2,0,1)[12] with non-zero mean : -375.1609\n#> \n#>  Best model: ARIMA(0,0,0)(2,0,1)[12] with non-zero mean\n#> Series: dlprice_amazn_ts \n#> ARIMA(0,0,0)(2,0,1)[12] with non-zero mean \n#> \n#> Coefficients:\n#>          sar1     sar2    sma1    mean\n#>       -0.9433  -0.0537  0.8062  0.0233\n#> s.e.   0.1790   0.0869  0.1669  0.0062\n#> \n#> sigma^2 = 0.01094:  log likelihood = 192.72\n#> AIC=-375.43   AICc=-375.16   BIC=-358.28\nSARIMA_price_amzn_ts_201<-arima(dlprice_amazn_ts,\n                        order=c(0,0,0), \n                        seasonal = c(2,0,1),\n                        method = \"ML\")\nSARIMA_price_amzn_pl_201<-Arima(dlprice_amazn_ts,\n                        order=c(0,0,0), \n                        seasonal = c(2,0,1),\n                        method = \"ML\")\nautoplot(SARIMA_price_amzn_ts_201)+theme_light()\nSARIMA_price_amzn_ts_201_F<-predict(SARIMA_price_amzn_ts_201,n.ahead=13)\nSARIMA_price_amzn_ts_201_F\nSARIMA_price_amzn_PL_201_F <-forecast(SARIMA_price_amzn_pl_201,h=13)\nkable(SARIMA_price_amzn_PL_201_F,\"html\")\nplot(SARIMA_price_amzn_PL_201_F, main =\"Forecast SARIMA(0,0,0)(2,0,1)  de los precios de AMZN\")"},{"path":"raiz-unitaria.html","id":"raiz-unitaria","chapter":"8 Raiz Unitaria","heading":"8 Raiz Unitaria","text":"","code":""},{"path":"raiz-unitaria.html","id":"definición-y-formas-de-no-estacionariedad","chapter":"8 Raiz Unitaria","heading":"8.1 Definición y formas de No Estacionariedad","text":"Hasta ahora hemos planteado una serie de ténicas de regresión que aplican sólo procesos o series estacionarias. En esta sección relajaremos la definición de estacionariedad y plantearemos pruebas para determinar cuando una serie es estacionaria bajo tres diferentes especificiones: (1) estacionariedad al rededor de una tendencia determinística; (2) estacionariedad al rededor de una media, y (3) estacionariedad al rededor del cero.continuación discutiremos cómo es posible que una serie sea estacionaria al rededor de una tendencia determinística. Diremos que una tendencia es determinística si ésta puede ser aproximada o modelada por un polinomio en función de \\(t\\), la cual incluye posibles transformaciones logarítmicas.Bajo este enfoque, el proceso está lejos de cumplir con la defición de estacionariedad que hemos establecido en capítulos previos, pero relajaremos el supuesto y reconoceremos que una serie puede ser estacionaria en varianza bajo una tendencia determinística. Así, diremos que la serie será descrita por una ecuación dada por:\n\\[\\begin{equation}\n    Y_t = \\sum^m_{j = 0} \\delta_j t^j + X_t\n    \\tag{8.1}\n\\end{equation}\\]Donde \\(X_t\\) es un proceso \\(ARMA(p, q)\\) con media cero, que se puede ver como:\n\\[\\begin{equation}\n    \\alpha(L) X_t = \\beta(L) U_t\n        \\tag{8.2}\n\\end{equation}\\]Entonces, los momentos y variaza de la ecuación (8.1) estaran dados por:\n\\[\\begin{equation}\n    \\mathbb{E}[Y_t] = \\sum^m_{j = 0} \\delta_j t^j = \\mu_t\n        \\tag{8.3}\n\\end{equation}\\]Dada la ecuación ((8.3) podemos plantear la siguiente ecuación de covarainzas:\n\\[\\begin{eqnarray}\n    \\mathbb{E}[(Y_t - \\mu_t) \\cdot (Y_{t+\\tau} - \\mu_{t+\\tau})] & = & \\mathbb{E}[X_t \\cdot X_{t+\\tau}] \\nonumber \\\\\n    & = & \\gamma_X(\\tau)\n            \\tag{8.4}\n\\end{eqnarray}\\]Utilizando el resultado de la ecuación (8.4) podemos establecer que:\n\\[\\begin{equation}\n    \\mathbb{E}[(Y_t - \\mu_t)^2] = \\mathbb{E}[X_t^2] = \\sigma_X^2\n\\tag{8.5}\n\\end{equation}\\]Así, las ecuaciones (8.3) y (8.5) significan que el proceso descrito por la ecuación (8.1) es estacionario pero en varianza. De esta forma partir de este momento diremos que una serie será estacionaria al rededor de una tendencia determinística si cumple con las condiciones establecidas en las ecuaciones (8.3), (8.5) y (8.1).Dicho lo anterior estudiaremos el concepto de raíz unitaria de un proceso estocástico o de una serie de tiempo. Partamos de platear que un proceso AR(1) tiene raíz unitaria cuando el cual el coeficiente \\(a_1 = 1\\), es decir:\n\\[\\begin{equation}\n    Y_t = Y_{t-1} + U_t\n    \\tag{8.6}\n\\end{equation}\\]Donde \\(U_t\\) es un proceso pueramente aleatorio con media cero, varianza constante y autocovarianza cero (0), al cual nos referiremos simplemente como ruido blanco. Supongamos ahora que incluímos un término constante en la ecuación (8.6), de forma que tenemos:\n\\[\\begin{equation}\n    Y_t = \\delta + Y_{t-1} + U_t\n    \\tag{8.7}\n\\end{equation}\\]Tomando la ecuación (8.7) y suponiendo que existe un valor inicial \\(Y_0\\) de la serie podemos plantear la sguiente secuencia de expresiones:\n\\[\\begin{eqnarray*}\n    Y_1 & = & \\delta + Y_0 + U_1 \\\\\n    Y_2 & = & \\delta + Y_1 + U_2 \\\\\n    & = & \\delta + (\\delta + Y_0 + U_1) + U_2 \\\\\n    & = & 2 \\times \\delta + Y_0 + U_1 + U_2\n\\end{eqnarray*}\\]Si repitieramos la sustitución sucesiva anterior hasta el momento \\(t\\) encontrariamos que la ecuación de la solución general que describe un \\(AR(1)\\) con término constante que tiene raíz unitaria es de la forma:\n\\[\\begin{equation}\n    Y_t = t \\delta + Y_0 + \\sum_{=1}^t U_i\n    \\tag{8.8}\n\\end{equation}\\]La ecuación (8.8) es equivalente la ecuación (8.1). la ecuación (8.8) también se le conoce como proceso con Drift o con término constante, indistintamente, ya que el componente de Drift suele asociarse la posibilidad de incorporar el efecto de los residuales pasados, lo cual es posible simplemente agregando una constante.sSi revisamos el comportamiento de sus momentos y varianza de la ecuación (8.8) encontramos que:\n\\[\\begin{eqnarray*}\n    \\mathbb{E}[Y_t] & = & Y_0 + \\delta t = \\mu_t \\\\\n    Var[Y_t] & = & t \\sigma^2 = \\gamma(0, t) \\\\\n    Cov(Y_t, Y_{t+\\tau}) & = & (t - \\tau) \\sigma^2 = \\gamma(t, \\tau)\n\\end{eqnarray*}\\]De esta forma, la ecuación (8.8) describe un proceso estacionario, sólo podría ser estacionario si \\(t = 1\\), en cualquier otro caso sería estacionario en varianza. Ahora hagamos un resumen y acordemos notación que se utilizará en esta sección. Supongamos un proceso o serie de tiempo que es decrito por la siguiente ecuación:\n\\[\\begin{equation}\n    Y_t = \\delta + Y_{t-1} + X_t\n    \\tag{8.9}\n\\end{equation}\\]Donde \\(X_t\\) es un \\(ARMA(p, q)\\) con media cero. Si definimos \\(\\Delta Y_t = Y_t - Y_{t-1}\\), entonces la ecuación (8.9) la podemos escribir como:\n\\[\\begin{equation}\n    \\Delta Y_t = \\delta + X_t\n    \\tag{8.10}\n\\end{equation}\\]la ecuación (8.10) la denominaremos como un proceso estacionario en diferencias o simplemente como un proceso integrado. Así utilizaremos la siguiente definición.Sea un proceso estocástico \\(Y\\), decimos que este es integrado de orden \\(d\\), \\((d)\\), si este puede transformarse uno estacionacionario, que sea invertible, mediante la diferenciación del mismo \\(d\\)-veces, es decir:\n\\[\\begin{equation}\n    (1 - L)^d Y_t = \\delta + X_t\n    \\tag{8.11}\n\\end{equation}\\]Donde \\(X_t\\) es un proceso \\(ARMA(p, q)\\). De lo cual se infiere que en la ecuación (8.11) \\(Y_t\\) será una \\(ARIMA(p, d, q)\\), el cual contiene \\(d\\) raíces unitarias. estos procesos también se les conoce como procesos con tendencia estocástica.Dada la discusión annterior, continuación platearemos un resumen de cuales son los dos casos los cuales nos referiremos como procesos que son estacionarios en media, pero que si lo son en varianza. Estos casos son:\n\\[\\begin{eqnarray}\n    Y_t & = & Y_0 + \\delta t + U_t \\\\\n        \\tag{8.12}\n\\end{eqnarray}\\]\\[\\begin{eqnarray}\n    Y_t & = & Y_0 + \\delta + \\sum_{= 1}^t U_t\n            \\tag{8.13}\n\\end{eqnarray}\\]Ambos casos son estacionarios en media, pero si lo son en varianza. De ambos podemos decir que los choques o innovaciones del término de error tienen un efecto transitorio en el primero, pero permanentes en el segundo.","code":""},{"path":"raiz-unitaria.html","id":"pruebas-de-raíces-unitarias","chapter":"8 Raiz Unitaria","heading":"8.2 Pruebas de Raíces Unitarias","text":"En esta sección plantearemos una serie de pruebas estadísticas para determinar cuando una serie puede ser estacionaria bajo tres posibles casos: (1) estacionariedad al rededor de una tendencia determinística; (2) estacionariedad al rededor de una media, y (3) estacionariedad al rededor del cero.","code":""},{"path":"raiz-unitaria.html","id":"dickey---fuller-df","chapter":"8 Raiz Unitaria","heading":"8.3 Dickey - Fuller (DF)","text":"Partamos de una forma del proceso \\(Y_t\\) dada por:\n\\[\\begin{equation}\n    Y_t = \\sum_{j = 0}^m \\delta_j t^j + X_t\n\\tag{8.14}\n\\end{equation}\\]Donde \\(X_t\\) es un \\(ARMA(p, q)\\) con media cero. Esta prueba asume que \\(m = 1\\), por lo que utilizaremos un modelo del tipo:\n\\[\\begin{equation}\n    Y_t = \\alpha + \\delta t + \\rho Y_{t-1} + U_t\n    \\tag{8.15}\n\\end{equation}\\]Si, el \\(AR(1)\\) planteado tiene raíz unitaria, es decir, \\(\\rho = 1\\), entonces tendríamos:\n\\[\\begin{eqnarray*}\n    Y_t & = & \\alpha + \\delta t + Y_{t-1} + U_t \\\\\n    \\Delta Y_t & = & \\alpha + \\delta t + U_t\n\\end{eqnarray*}\\]De esta forma, para determinar si una serie tiene raíz unitaria basta con probar la hipótesis nula de que \\(H_0 : \\rho = 1\\), junto con las diferentes combinaciones que impliquen restricciones respecto \\(\\delta\\) y \\(\\alpha\\).En resumen, la prueba DF consiste en asumir un modelo general dado por la ecuación (8.15) y probar tres especificaciones distintas que serían válidas bajo \\(H_0 : \\rho = 1\\):Modelo : con intercepto y tendencia:\n\\[\\begin{equation*}\n     \\Delta Y_t = \\alpha + \\delta t + \\beta Y_{t-1} + U_t\n\\end{equation*}\\]\nBuscamos probar si \\(H_0 : \\beta = \\rho - 1 = 0\\) contra \\(H_a : \\beta < 0\\), por lo que es una prueba de una cola. Otra forma de decirlo, es probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario al rededor de una tendencia determinística.Modelo : con intercepto y tendencia:\n\\[\\begin{equation*}\n     \\Delta Y_t = \\alpha + \\delta t + \\beta Y_{t-1} + U_t\n\\end{equation*}\\]\nBuscamos probar si \\(H_0 : \\beta = \\rho - 1 = 0\\) contra \\(H_a : \\beta < 0\\), por lo que es una prueba de una cola. Otra forma de decirlo, es probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario al rededor de una tendencia determinística.Modelo B: con intercepto:\n\\[\\begin{equation*}\n     \\Delta Y_t = \\alpha + \\beta Y_{t-1} + U_t\n\\end{equation*}\\]\nBuscamos probar si \\(H_0 : \\beta = \\rho - 1 = 0\\) contra \\(H_a : \\beta < 0\\), por lo que es una prueba de una cola. Otra forma de decirlo, es probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario al rededor de una constante.Modelo B: con intercepto:\n\\[\\begin{equation*}\n     \\Delta Y_t = \\alpha + \\beta Y_{t-1} + U_t\n\\end{equation*}\\]\nBuscamos probar si \\(H_0 : \\beta = \\rho - 1 = 0\\) contra \\(H_a : \\beta < 0\\), por lo que es una prueba de una cola. Otra forma de decirlo, es probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario al rededor de una constante.Modelo C: sin intercepto y tendencia:\n\\[\\begin{equation*}\n     \\Delta Y_t = \\beta Y_{t-1} + U_t\n\\end{equation*}\\]\nBuscamos probar si \\(H_0 : \\beta = \\rho - 1 = 0\\) contra \\(H_a : \\beta < 0\\), por lo que es una prueba de una cola. Otra forma de decirlo, es probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario sin considerar una constante o una tendencia determinística, es decir, es un proceso puramente aleatorio.Modelo C: sin intercepto y tendencia:\n\\[\\begin{equation*}\n     \\Delta Y_t = \\beta Y_{t-1} + U_t\n\\end{equation*}\\]\nBuscamos probar si \\(H_0 : \\beta = \\rho - 1 = 0\\) contra \\(H_a : \\beta < 0\\), por lo que es una prueba de una cola. Otra forma de decirlo, es probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario sin considerar una constante o una tendencia determinística, es decir, es un proceso puramente aleatorio.","code":""},{"path":"raiz-unitaria.html","id":"dickey---fuller-aumentada-adf","chapter":"8 Raiz Unitaria","heading":"8.3.1 Dickey - Fuller Aumentada (ADF)","text":"diferencia de un modelo AR(1) para el caso de una prueba DF como en la ecuación (8.15), en una prueba ADF se asume que el proceso es un AR(p) de la forma (por simplicidad hemos omitido el término constante y el término de tendencia determinística):\\[\\begin{equation}\n    Y_t = a_1 Y_{t-1} + a_2 Y_{t-2} + \\ldots + a_p Y_{t-p} + U_t\n    \\tag{8.16}\n\\end{equation}\\]Haciendo una sustitución de términos similar las que hemos planteado en otras secciones podemos reexpresar la ecuación (8.16) en su versión en diferencias siguiendo el proceso:\n\\[\\begin{equation}\n    Y_t = \\rho Y_{t-1} + \\theta_1 \\Delta Y_{t-1} + \\theta_2 \\Delta Y_{t-2} + \\ldots + + \\theta_{p-1} \\Delta Y_{t-p+1} + U_t\n                \\tag{8.17}\n\\end{equation}\\]Donde \\(\\rho = \\theta_0 = \\sum_{j = 1}^p a_j\\), \\(\\theta_i = - \\sum_{j = + 1}^p a_j\\), \\(= 1, 2, \\ldots, p-1\\). Así, si el proceso AR(p) tiene raíz unitaria entonces ceremos que:\n\\[\\begin{eqnarray*}\n    1 - a_1 - a_2 - \\ldots - a_p & = & 0 \\\\\n    \\rho & = & 1\n\\end{eqnarray*}\\]De donde podemos establecer que el modelo general de una prueba ADF será:\n\\[\\begin{equation}\n    \\Delta Y_{t-1} = \\alpha + \\beta t + (\\rho - 1) Y_{t-1} + \\theta_1 \\Delta Y_{t-1} + \\theta_2 \\Delta Y_{t-2} + \\ldots + \\theta_k \\Delta Y_{t-k} + U_t\n            \\tag{8.18}\n\\end{equation}\\]Donde \\(U_t\\) es un proceso puramente aleatorio y \\(k\\) es elegido de tal manera que los residuales sean un proceso puramente aleatorio. En resumen, la prueba DF consiste en asumir un modelo general dado por la ecuación (8.16), que incluya constante y tendencia, y probar tres especificaciones distintas que serían válidas bajo \\(H_0 : \\rho = 1\\):Modelo : con intercepto y tendencia:\n\\[\\begin{equation*}\n     \\Delta Y_t = \\alpha + \\delta t + \\beta Y_{t-1} + \\theta_1 \\Delta Y_{t-1} + \\theta_2 \\Delta Y_{t-2} + \\ldots + \\theta_{p-1} \\Delta Y_{t-p+1} + U_t\n\\end{equation*}\\]\nBuscamos probar si \\(H_0 : \\beta = \\rho - 1 = 0\\) contra \\(H_a : \\beta < 0\\), por lo que es una prueba de una cola. Otra forma de decirlo, es probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario al rededor de una tendencia determinística.Modelo : con intercepto y tendencia:\n\\[\\begin{equation*}\n     \\Delta Y_t = \\alpha + \\delta t + \\beta Y_{t-1} + \\theta_1 \\Delta Y_{t-1} + \\theta_2 \\Delta Y_{t-2} + \\ldots + \\theta_{p-1} \\Delta Y_{t-p+1} + U_t\n\\end{equation*}\\]\nBuscamos probar si \\(H_0 : \\beta = \\rho - 1 = 0\\) contra \\(H_a : \\beta < 0\\), por lo que es una prueba de una cola. Otra forma de decirlo, es probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario al rededor de una tendencia determinística.Modelo B: con intercepto:\n\\[\\begin{equation*}\n     \\Delta Y_t = \\alpha + \\beta Y_{t-1} + \\theta_1 \\Delta Y_{t-1} + \\theta_2 \\Delta Y_{t-2} + \\ldots + \\theta_{p-1} \\Delta Y_{t-p+1} + U_t\n\\end{equation*}\\]\nBuscamos probar si \\(H_0 : \\beta = \\rho - 1 = 0\\) contra \\(H_a : \\beta < 0\\), por lo que es una prueba de una cola. Otra forma de decirlo, es probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario al rededor de una constante.Modelo B: con intercepto:\n\\[\\begin{equation*}\n     \\Delta Y_t = \\alpha + \\beta Y_{t-1} + \\theta_1 \\Delta Y_{t-1} + \\theta_2 \\Delta Y_{t-2} + \\ldots + \\theta_{p-1} \\Delta Y_{t-p+1} + U_t\n\\end{equation*}\\]\nBuscamos probar si \\(H_0 : \\beta = \\rho - 1 = 0\\) contra \\(H_a : \\beta < 0\\), por lo que es una prueba de una cola. Otra forma de decirlo, es probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario al rededor de una constante.Modelo C: sin intercepto y tendencia:\n\\[\\begin{equation*}\n     \\Delta Y_t = \\beta Y_{t-1} + \\theta_1 \\Delta Y_{t-1} + \\theta_2 \\Delta Y_{t-2} + \\ldots + \\theta_{p-1} \\Delta Y_{t-p+1} + U_t\n\\end{equation*}\\]\nBuscamos probar si \\(H_0 : \\beta = \\rho - 1 = 0\\) contra \\(H_a : \\beta < 0\\), por lo que es una prueba de una cola. Otra forma de decirlo, es probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario sin considerar una constante o una tendencia determinística, es decir, es un proceso puramente aleatorio.Modelo C: sin intercepto y tendencia:\n\\[\\begin{equation*}\n     \\Delta Y_t = \\beta Y_{t-1} + \\theta_1 \\Delta Y_{t-1} + \\theta_2 \\Delta Y_{t-2} + \\ldots + \\theta_{p-1} \\Delta Y_{t-p+1} + U_t\n\\end{equation*}\\]\nBuscamos probar si \\(H_0 : \\beta = \\rho - 1 = 0\\) contra \\(H_a : \\beta < 0\\), por lo que es una prueba de una cola. Otra forma de decirlo, es probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario sin considerar una constante o una tendencia determinística, es decir, es un proceso puramente aleatorio.","code":""},{"path":"raiz-unitaria.html","id":"phillips---perron-pp","chapter":"8 Raiz Unitaria","heading":"8.3.2 Phillips - Perron (PP)","text":"Una tercera prueba es la de PP, la cual también está basada en una AR(1) dado por la ecuación:\n\\[\\begin{equation}\n    Y_t = d \\eta + \\rho Y_{t-1} + U_t\n    \\tag{8.19}\n\\end{equation}\\]Donde \\(d\\) incluye cualquiera de los componentes determinísticos como constante y tendencia. Al igual que los casos pasados, la hipótesis probar era \\(H_0 : \\rho = 1\\) contra la alternativa \\(H_a : |{\\rho}| < 1\\), y asumimos una estructura MA(q) es el término de error de la forma \\(U_t = \\psi(L) \\varepsilon_t = \\psi_0 \\varepsilon_t + \\psi_1 \\varepsilon_{t-1} + \\ldots + \\psi_p \\varepsilon_{t-p}\\), con \\(\\varepsilon_t\\) es un ruido blanco con media cero y varianza \\(\\sigma^2\\). En este modelo se elige el valor \\(p\\) que hace que el componente sea un MA(p). Las tablas estadísticas de PP para esta prueba pueden utilizar una estadística \\(Z_\\tau\\) o \\(Z_\\rho\\), las cuales se pueden emplear indistintamente.En resumen, la prueba PP consiste en asumir un modelo general dado por la ecuación (8.19) y probar dos especificaciones distintas que serían válidas bajo \\(H_0 : \\rho = 1\\), ambas considerando un compenente Drift:Modelo : con intercepto y tendencia:\n\\[\\begin{equation*}\n     Y_t = \\alpha + \\delta t + \\rho Y_{t-1} + U_t\n\\end{equation*}\\]\nBuscamos probar si \\(H_0 : \\rho = 1\\) contra \\(H_a : |{\\rho}| < 1\\), por lo que es una prueba de una cola. Otra forma de decirlo, es probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario al rededor de una tendencia determinística.Modelo : con intercepto y tendencia:\n\\[\\begin{equation*}\n     Y_t = \\alpha + \\delta t + \\rho Y_{t-1} + U_t\n\\end{equation*}\\]\nBuscamos probar si \\(H_0 : \\rho = 1\\) contra \\(H_a : |{\\rho}| < 1\\), por lo que es una prueba de una cola. Otra forma de decirlo, es probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario al rededor de una tendencia determinística.Modelo B: con intercepto:\n\\[\\begin{equation*}\n     Y_t = \\alpha + \\rho Y_{t-1} + U_t\n\\end{equation*}\\]\nBuscamos probar si \\(H_0 : \\rho = 1\\) contra \\(H_a : |{\\rho}| < 1\\), por lo que es una prueba de una cola. Otra forma de decirlo, es probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario al rededor de una constante.Modelo B: con intercepto:\n\\[\\begin{equation*}\n     Y_t = \\alpha + \\rho Y_{t-1} + U_t\n\\end{equation*}\\]\nBuscamos probar si \\(H_0 : \\rho = 1\\) contra \\(H_a : |{\\rho}| < 1\\), por lo que es una prueba de una cola. Otra forma de decirlo, es probamos si el proceso tiene raíz unitaria contra si el proceso es estacionario al rededor de una constante.","code":""},{"path":"raiz-unitaria.html","id":"kwiatkowsky---phillips---schmidt---shin-kpss","chapter":"8 Raiz Unitaria","heading":"8.3.3 Kwiatkowsky - Phillips - Schmidt - Shin (KPSS)","text":"La prueba KPSS considera que el proceso es estacionario bajo la hipótesis nula, lo cual hace una diferencia respecto de las anteriores pruebas. El modelo considerado es:\n\\[\\begin{equation}\n    Y_t = \\delta t + \\xi_t + U_t\n                \\tag{8.20}\n\\end{equation}\\]Donde \\(U_t\\) es un proceso estacionario y \\(\\xi_t\\) es un ruido blanco descrito por la forma: \\(\\xi_t = \\xi_{t-1} + \\varepsilon_t\\), donde \\(\\varepsilon_t\\) es un proceso normalmente distribuido con media cero y varianza \\(\\sigma^2_\\varepsilon\\).Así, bajo la hipótesis nula \\(H_0 : \\sigma^2_\\varepsilon = 0\\), \\(\\xi\\) se vuelve una constante y el proceso puede tener una tendencia estacionaria. Dado el planteamiento de la prueba, los valores críticos al 95% son:0.146, para un modelo con tendencia0.463, para un modelo con constante","code":""},{"path":"raiz-unitaria.html","id":"ejemplo","chapter":"8 Raiz Unitaria","heading":"8.4 Ejemplo","text":"","code":"\n#install.packages(\"pacman\")\n#pacman nos permite cargar varias librerias en una sola línea\nlibrary(pacman)\npacman::p_load(tidyverse,BatchGetSymbols,ggplot2,lubridate,readxl,forecast,stats,stargazer,knitr,tseries)\n#Primero determinamos el lapso de tiempo\npd<-as.Date(\"2002/9/30\") #primer fecha\npd\n#> [1] \"2002-09-30\"\n#> [1] \"2021-09-18\"\nld<- as.Date(\"2021/09/30\")#última fecha\nld\n#> [1] \"2021-09-30\"\n#Intervalos de tiempo\nint<-\"monthly\"\n\n#Datos a elegir\ndt<-c(\"AMZN\")\n\n#Descargando los valores\ndata1<- BatchGetSymbols(tickers = dt,\n                       first.date = pd,\n                       last.date = ld,\n                       freq.data = int,\n                       do.cache = FALSE,\n                       thresh.bad.data = 0)\n\n#Generando data frame con los valores\ndata_precio_amzn<-data1$df.tickers\ncolnames(data_precio_amzn)\n#>  [1] \"ticker\"              \"ref.date\"           \n#>  [3] \"volume\"              \"price.open\"         \n#>  [5] \"price.high\"          \"price.low\"          \n#>  [7] \"price.close\"         \"price.adjusted\"     \n#>  [9] \"ret.adjusted.prices\" \"ret.closing.prices\"\n#original\nprice_amazn_ts<-ts(data_precio_amzn$price.open, frequency = 12, start=c(2002,09))\n#logartimo\nlprice_amazn_ts<-ts(log(data_precio_amzn$price.open), frequency = 12,start=c(2002,09))\n#diferencias logaritmicas(cambio porcential)\ndlprice_amazn_ts<-ts(log(data_precio_amzn$price.open)-lag(log(data_precio_amzn$price.open),1), frequency = 12, start=c(2002,10))"},{"path":"raiz-unitaria.html","id":"df-test","chapter":"8 Raiz Unitaria","heading":"8.5 DF test","text":"","code":"\ndlprice_amazn_ts<-na.omit(dlprice_amazn_ts)\nadf.test(dlprice_amazn_ts, k=0) # DF test\n#> Warning in adf.test(dlprice_amazn_ts, k = 0): p-value\n#> smaller than printed p-value\n#> \n#>  Augmented Dickey-Fuller Test\n#> \n#> data:  dlprice_amazn_ts\n#> Dickey-Fuller = -15.125, Lag order = 0, p-value =\n#> 0.01\n#> alternative hypothesis: stationary"},{"path":"raiz-unitaria.html","id":"adf-test","chapter":"8 Raiz Unitaria","heading":"8.6 ADF test","text":"","code":"\nadf.test(dlprice_amazn_ts) # DF test\n#> Warning in adf.test(dlprice_amazn_ts): p-value smaller than\n#> printed p-value\n#> \n#>  Augmented Dickey-Fuller Test\n#> \n#> data:  dlprice_amazn_ts\n#> Dickey-Fuller = -5.8711, Lag order = 6, p-value =\n#> 0.01\n#> alternative hypothesis: stationary"},{"path":"raiz-unitaria.html","id":"pp-test","chapter":"8 Raiz Unitaria","heading":"8.7 PP test","text":"","code":"\npp.test(dlprice_amazn_ts) # DF test\n#> Warning in pp.test(dlprice_amazn_ts): p-value smaller than\n#> printed p-value\n#> \n#>  Phillips-Perron Unit Root Test\n#> \n#> data:  dlprice_amazn_ts\n#> Dickey-Fuller Z(alpha) = -221.54, Truncation lag\n#> parameter = 4, p-value = 0.01\n#> alternative hypothesis: stationary"},{"path":"raiz-unitaria.html","id":"pp-test-1","chapter":"8 Raiz Unitaria","heading":"8.8 PP test","text":"","code":"\npp.test(dlprice_amazn_ts) # DF test\n#> Warning in pp.test(dlprice_amazn_ts): p-value smaller than\n#> printed p-value\n#> \n#>  Phillips-Perron Unit Root Test\n#> \n#> data:  dlprice_amazn_ts\n#> Dickey-Fuller Z(alpha) = -221.54, Truncation lag\n#> parameter = 4, p-value = 0.01\n#> alternative hypothesis: stationary"},{"path":"raiz-unitaria.html","id":"kpss-test","chapter":"8 Raiz Unitaria","heading":"8.9 KPSS test","text":"Claramente podemos ver que todos los tests arrojan que se rechaza la hipótesis nula de \\(\\beta_0=1\\) y, por consiguiente, es estacionaria.","code":"\nkpss.test(dlprice_amazn_ts) # DF test\n#> Warning in kpss.test(dlprice_amazn_ts): p-value greater than\n#> printed p-value\n#> \n#>  KPSS Test for Level Stationarity\n#> \n#> data:  dlprice_amazn_ts\n#> KPSS Level = 0.033772, Truncation lag parameter = 4,\n#> p-value = 0.1"},{"path":"volatilidad-y-heterocedasticidad.html","id":"volatilidad-y-heterocedasticidad","chapter":"9 Volatilidad y Heterocedasticidad","heading":"9 Volatilidad y Heterocedasticidad","text":"En esta lección analizaremos la manera en la que la volatilidad puede afectar la manera en la que estudiamos las series de tiempo. Como definición, la volatilidad es la desviación estándar de un determinado activo. Lo que quiere decir esto es que la desviación estándar de los activos es siempre constante y, por consiguiente, puede reaccionar de distintas manera distintos cambios de precio. Esto es de relevancia para los siguientes modelos que utilizaremos llamados Auto Regressive Conditional Heteroskedasticity (ARCH) y Generalized Auto Regressive Conditional Heteroskedasticity (GARCH), pues intentan modelar la heterocedasticidad condicional de los modelos.Algunos de los problemas de la volatilidad es que es directamente observable porque la volatilidad de un activo depende de valores entre días o de la noche la mañana, por lo que es difícil ver qué tanto afecta o cambia.Aquí las principales características de la volatilidad.La volatilidad puede ser alta o baja en ciertos periodos de tiempo. esto se le conoce como volatility clusters.\n- La volatilidad evoluciona continuamente con el tiempo, por lo que los picos de volatilidad son raros.\n- La volatilidad tiene al infinito, tiene una varianza alrededor de un valor fijo. En términos que hemos mencionado antes, la volatilidad es estacionaria.\n- La volatilidad reacciona de maneras diferentes picos o valles en los valores de los precios. esto se le conoce como leverage effect.Sean \\(a_t=r_t-\\mu_t\\) los residuales de una ecuación de media. La serie \\(a_t^2\\) es utilizada para determinar la heterocedasticidad condicional, lo cual se conoce como el efecto ARCH. Existen dos tests:\n- El test se aplica el Ljung-Box estadístico \\(Q(m)\\) la serie \\({a_t^2}\\) y fue construido por Mcleod y Li (1983). La hipótesis nula es que los primeros \\(m\\) lags de la Función de Autocorrelación de la serie \\(a_t^2\\) son iguales cero.\n- El segundo esta basado en el Multiplicador de Lagrange de Engle (1982) y es equivalente utilizar un \\(F\\) test para probar que \\(\\alpha_i=0\\) donde \\((=1,\\dots,m)\\) en \\[a_t^2=\\alpha_0+\\alpha_1a_{t-1}^2+\\dots+\\alpha_m a_{t-m}^2+e_t\\]\nPor lo que los test funcionan igual. Específicamente, la hipótesis nula es \\(H_0:\\alpha_0=\\dots=\\alpha_m=0\\). Determinemos que \\(SSR_0=\\sum_{t=m+1}^T(a_t^2-\\bar{w})^2\\), donde \\(\\bar{w}=\\frac{1}{T}\\sum_{t=1}^Ta_t^2\\) es la media muestral de \\(a_t^2\\) y \\(SSR_1=\\sum_{t=m+1}^Te_t^2\\). Por lo que F es igual \\[F=\\frac{(SSR_0-SSR_1)/m}{SSR_1/(T-2m-1)}\\]. De esa manera rechazamos la hipotesis nula, como regla de dedo si \\(F>X_m^2(\\alpha)\\) y decimos que existe un efecto ARCH. Cabe aclarar que \\(X_m^2(\\alpha)\\) es el porcentíl \\(100(1-\\alpha)\\)esimo alto de \\(X_m^2\\). O el \\(p\\)-value de F es menor que \\(\\alpha\\).","code":""},{"path":"volatilidad-y-heterocedasticidad.html","id":"ejemplo-1","chapter":"9 Volatilidad y Heterocedasticidad","heading":"9.1 Ejemplo","text":"","code":"\n#install.packages(\"pacman\")\n#pacman nos permite cargar varias librerias en una sola línea\nlibrary(pacman)\npacman::p_load(tidyverse,BatchGetSymbols,ggplot2,lubridate,readxl,forecast,stats,stargazer,knitr,tseries, aTSA, TSA)\n#Primero determinamos el lapso de tiempo\npd<-as.Date(\"2002/9/30\") #primer fecha\npd\n#> [1] \"2002-09-30\"\n#> [1] \"2021-09-18\"\nld<- as.Date(\"2021/09/30\")#última fecha\nld\n#> [1] \"2021-09-30\"\n#Intervalos de tiempo\nint<-\"monthly\"\n\n#Datos a elegir\ndt<-c(\"AMZN\")\n\n#Descargando los valores\ndata1<- BatchGetSymbols(tickers = dt,\n                       first.date = pd,\n                       last.date = ld,\n                       freq.data = int,\n                       do.cache = FALSE,\n                       thresh.bad.data = 0)\n\n#Generando data frame con los valores\ndata_precio_amzn<-data1$df.tickers\ncolnames(data_precio_amzn)\n#>  [1] \"ticker\"              \"ref.date\"           \n#>  [3] \"volume\"              \"price.open\"         \n#>  [5] \"price.high\"          \"price.low\"          \n#>  [7] \"price.close\"         \"price.adjusted\"     \n#>  [9] \"ret.adjusted.prices\" \"ret.closing.prices\"\n#original\nprice_amazn_ts<-ts(data_precio_amzn$price.open, frequency = 12, start=c(2002,09))\n#logartimo\nlprice_amazn_ts<-ts(log(data_precio_amzn$price.open), frequency = 12,start=c(2002,09))\n#diferencias logaritmicas(cambio porcential)\ndlprice_amazn_ts<-ts(log(data_precio_amzn$price.open)-lag(log(data_precio_amzn$price.open),1), frequency = 12, start=c(2002,10))\ndlprice_amazn_ts<-na.omit(dlprice_amazn_ts)"},{"path":"volatilidad-y-heterocedasticidad.html","id":"pruebas-de-heterocedasticidad-condicionada","chapter":"9 Volatilidad y Heterocedasticidad","heading":"9.2 Pruebas de Heterocedasticidad Condicionada","text":"","code":""},{"path":"volatilidad-y-heterocedasticidad.html","id":"mcleod-li","chapter":"9 Volatilidad y Heterocedasticidad","heading":"9.2.1 McLeod & Li","text":"Dadas todas las pruebas podemos ver que hay heterocedasticidad menos en el de diferencias de logaritmos.","code":"\nMcLeod.Li.test(y=dlprice_amazn_ts)\nBox.test(lprice_amazn_ts,lag=12,type=\"Ljung\")\n#> \n#>  Box-Ljung test\n#> \n#> data:  lprice_amazn_ts\n#> X-squared = 2310.6, df = 12, p-value < 2.2e-16\nMcLeod.Li.test(y=lprice_amazn_ts)\nMcLeod.Li.test(y=price_amazn_ts)\nmod1 <- arima(dlprice_amazn_ts,order = c(1,0,0))\narch.test(mod1, output = TRUE)\n#> ARCH heteroscedasticity test for residuals \n#> alternative: heteroscedastic \n#> \n#> Portmanteau-Q test: \n#>      order    PQ  p.value\n#> [1,]     4  2.52 6.41e-01\n#> [2,]     8  4.51 8.08e-01\n#> [3,]    12 46.10 6.68e-06\n#> [4,]    16 48.74 3.63e-05\n#> [5,]    20 71.95 8.74e-08\n#> [6,]    24 85.13 9.07e-09\n#> Lagrange-Multiplier test: \n#>      order     LM  p.value\n#> [1,]     4 149.98 0.00e+00\n#> [2,]     8  70.53 1.15e-12\n#> [3,]    12  21.89 2.52e-02\n#> [4,]    16  13.71 5.48e-01\n#> [5,]    20   8.27 9.84e-01\n#> [6,]    24   5.99 1.00e+00\nmod2 <- arima(lprice_amazn_ts,order = c(1,0,0))\narch.test(mod2, output = TRUE)\n#> ARCH heteroscedasticity test for residuals \n#> alternative: heteroscedastic \n#> \n#> Portmanteau-Q test: \n#>      order    PQ  p.value\n#> [1,]     4  2.67 6.15e-01\n#> [2,]     8  4.04 8.54e-01\n#> [3,]    12 43.16 2.12e-05\n#> [4,]    16 45.87 1.02e-04\n#> [5,]    20 64.29 1.51e-06\n#> [6,]    24 79.84 6.45e-08\n#> Lagrange-Multiplier test: \n#>      order     LM  p.value\n#> [1,]     4 122.98 0.00e+00\n#> [2,]     8  60.92 9.88e-11\n#> [3,]    12  21.53 2.83e-02\n#> [4,]    16  13.25 5.83e-01\n#> [5,]    20   8.46 9.81e-01\n#> [6,]    24   6.14 1.00e+00\n#mod3 <- estimate(price_amazn_ts,p = 1)\n#arch.test(mod3, output = TRUE)"},{"path":"heterocedasticidad-condicionada.html","id":"heterocedasticidad-condicionada","chapter":"10 Heterocedasticidad condicionada","heading":"10 Heterocedasticidad condicionada","text":"","code":""},{"path":"heterocedasticidad-condicionada.html","id":"modelos-arch-y-garch-univariados","chapter":"10 Heterocedasticidad condicionada","heading":"10.1 Modelos ARCH y GARCH Univariados","text":"Estos modelos de Heterocedásticidad Condicional Autoregresiva (ARCH, por sus siglas en inglés) y modelos Heterocedásticidad Condicional Autoregresiva Generalizados (GARCH, por sus siglas en inglés) tienen la característica de modelar situaciones como las que ilustra la Figutra 10.1. Es decir: 1) existen zonas donde la variación de los datos es mayor y zonas donde la variación es más estable –estas situaciones se les conoce como de variabilidad por clúster–, y 2) los datos corresponden innformación de alta frecuencia.\nFigure 10.1: Rendimientos (diferenccias logarítmicas) de tres acciones seleccionadas: Apple, Pfizer, Tesla, enero 2011 noviembre de 2020\nPara plantear el modelo supongamos –por simplicidad– que hemos construido y estimado un modelo AR(1). Es decir, asumamos que el proceso subyacente para la media condicional está dada por:\n\\[\\begin{equation}\n    X_t = a_0 + a_1 X_{t-1} + U_t\n    \\tag{10.1}\n\\end{equation}\\]Donde \\(|a_1|< 1\\) para garantizar la convergencia del proceso en el largo plazo, en el cual:\n\\[\\begin{eqnarray*}\n    \\mathbb{E}[X_t] & = & \\frac{a_0 }{1 - a_1} = \\mu \\\\\n    Var[X_t] & = & \\frac{\\sigma^2}{1 - a_1^2}\n\\end{eqnarray*}\\]Ahora, supongamos que este tipo de modelos pueden ser extendidos y generalizados un modelo ARMA(p, q), que incluya otras variables exogénas. Denotemos como \\(\\mathbf{Z}_t\\) al conjunto que incluye los componentes AR, MA y variables exogénas que pueden explicar \\(X_t\\) de forma que el proceso estará dado por:\n\\[\\begin{equation}\n    X_t = \\mathbf{Z}_t \\boldsymbol{\\beta} + U_t\n        \\tag{10.2}\n\\end{equation}\\]Donde \\(U_t\\) es un proceso estacionario que representa el error asociado un proceso ARMA(p, q) y donde siguen diendo válidos los supuestos:\n\\[\\begin{eqnarray*}\n    \\mathbb{E}[U_t] & = & 0 \\\\\n    Var[U_t^2] & = & \\sigma^2\n\\end{eqnarray*}\\]obstante, en este caso podemos suponer que existe autocorrelación en el término de error que puede ser capturada por un porceso similar uno de medias móviles (MA) dado por:\n\\[\\begin{equation}\n    U_t^2 = \\gamma_0 + \\gamma_1 U_{t-1}^2 + \\gamma_2 U_{t-2}^2 + \\ldots + \\gamma_q U_{t-q}^2 + \\nu_t\n        \\tag{10.3}\n\\end{equation}\\]Donde \\(\\nu_t\\) es un ruido blanco y \\(U_{t-} = X_{t-} - \\mathbf{Z}_{t-} \\boldsymbol{\\beta}\\), $= 1, 2 ,$. Si bien los procesos son estacionarios por los supuestos antes enunciados, la varianza condicional estará dada por:\n\\[\\begin{eqnarray*}\n    \\sigma^2_{t | t-1} & = & Var[ U_t | \\Omega_{t-1} ] \\\\\n    & = & \\mathbb{E}[ U^2_t | \\Omega_{t-1} ]\n\\end{eqnarray*}\\]Donde \\(\\Omega_{t-1} = \\{U_{t-1}, U_{t-2}, \\ldots \\}\\) es el conjunto de toda la información pasada de \\(U_t\\) y observada hasta el momento \\(t-1\\), por lo que:\n\\[\\begin{equation*}\n    U_t | \\Omega_{t-1} \\sim \\mathbb{D}(0, \\sigma^2_{t | t-1})\n\\end{equation*}\\]Así, de forma similar un proceso MA(q) podemos decir que la varianza condicional tendrá efectos ARCH de orden \\(q\\) (ARCH(q)) cuando:\n\\[\\begin{equation}\n    \\sigma^2_{t | t-1} = \\gamma_0 + \\gamma_1 U_{t-1}^2 + \\gamma_2 U_{t-2}^2 + \\ldots + \\gamma_q U_{t-q}^2\n        \\tag{10.4}\n\\end{equation}\\]Donde \\(\\mathbb{E}[\\nu_t] = 0\\) y \\(\\gamma_0\\) y \\(\\gamma_i \\geq 0\\), para \\(= 1, 2, \\ldots, q-1\\) y \\(\\gamma_q > 0\\). Estas condiciones son necesarias para garantizar que la varianza sea positiva. En general, la varianza condicional se expresa de la forma \\(\\sigma^2_{t | t-1}\\), obstante, para facilitar la notación, nos referiremos en cada caso esta simplemente como \\(\\sigma^2_{t}\\).Podemos generalizar está situación si asumimos la varianza condicional como dependiente de lo valores de la varianza rezagados, es decir, como si fuera un proceso AR de orden \\(p\\) para la varianza y juntandolo con la ecuación (10.4). Bollerslev (1986) y Taylor (1986) generalizaron el problema de heterocedásticidad condicional. El modelo se conoce como GARCH(p, q), el cual se especifica como:\n\\[\\begin{equation}\n    \\sigma^2_t = \\gamma_0 + \\gamma_1 U_{t-1}^2 + \\gamma_2 U_{t-2}^2 + \\ldots + \\gamma_q U_{t-q}^2 + \\beta_1 \\sigma^2_{t-1} + \\beta_2 \\sigma^2_{t-2} + \\ldots + \\beta_p \\sigma^2_{t-p}\n    \\tag{10.5}\n\\end{equation}\\]Donde las condiciones de negatividad son que \\(\\gamma_0 > 0\\), \\(\\gamma_i \\geq 0\\), \\(= 1, 2, \\ldots, q-1\\), \\(\\beta_j \\geq 0\\), \\(j = 1, 2, \\ldots, p-1\\), \\(\\gamma_q > 0\\) y \\(\\beta_p > 0\\). Además, otra condición de convergencia es que:\n\\[\\begin{equation*}\n    \\gamma_1 + \\ldots + \\gamma_q + \\beta_1 + \\ldots + \\beta_p < 1\n\\end{equation*}\\]Usando el operador rezago \\(L\\) en la ecuación (10.5) podemos obtener:\n\\[\\begin{equation}\n    \\sigma^2_t = \\gamma_0 + \\alpha(L) U_t^2 + \\beta(L) \\sigma^2_t\n        \\tag{10.6}\n\\end{equation}\\]De donde podemos establecer:\n\\[\\begin{equation}\n    \\sigma^2_t = \\frac{\\gamma_0}{1 - \\beta(L)} + \\frac{\\alpha(L)}{1 - \\beta(L)} U_t^2         \\tag{10.7}\n\\end{equation}\\]Por lo que la ecuación (10.5) del GARCH(p, q) representa un ARCH(\\(\\infty\\)):\n\\[\\begin{equation}\n    \\sigma^2_t = \\frac{a_0}{1 - b_1 - b_2 - \\ldots - b_p} + \\sum_{= 1}^\\infty U_{t-}^2\n            \\tag{10.8}\n\\end{equation}\\]","code":""},{"path":"heterocedasticidad-condicionada.html","id":"ejemplo-2","chapter":"10 Heterocedasticidad condicionada","heading":"10.2 Ejemplo","text":"\nFigure 10.2: Serie de tiempo de los precios en los últimos 20 años\n\nFigure 5.5: Serie de tiempo de los residuos al cuadrado \\({^2}\\)\n","code":"\n#install.packages(\"pacman\")\n#pacman nos permite cargar varias librerias en una sola línea\nlibrary(pacman)\npacman::p_load(tidyverse,BatchGetSymbols,ggplot2,lubridate,readxl,forecast,stats,stargazer,knitr,tseries, aTSA, TSA, rugarch)\n#Primero determinamos el lapso de tiempo\npd<-as.Date(\"2002/9/30\") #primer fecha\npd\n#> [1] \"2002-09-30\"\n#> [1] \"2021-09-18\"\nld<- as.Date(\"2021/09/30\")#última fecha\nld\n#> [1] \"2021-09-30\"\n#Intervalos de tiempo\nint<-\"monthly\"\n\n#Datos a elegir\ndt<-c(\"AMZN\")\n\n#Descargando los valores\ndata1<- BatchGetSymbols(tickers = dt,\n                       first.date = pd,\n                       last.date = ld,\n                       freq.data = int,\n                       do.cache = FALSE,\n                       thresh.bad.data = 0)\n\n#Generando data frame con los valores\ndata_precio_amzn<-data1$df.tickers\ncolnames(data_precio_amzn)\n#>  [1] \"ticker\"              \"ref.date\"           \n#>  [3] \"volume\"              \"price.open\"         \n#>  [5] \"price.high\"          \"price.low\"          \n#>  [7] \"price.close\"         \"price.adjusted\"     \n#>  [9] \"ret.adjusted.prices\" \"ret.closing.prices\"\n#original\nprice_amazn_ts<-ts(data_precio_amzn$price.open, frequency = 12, start=c(2002,09))\n#logartimo\nlprice_amazn_ts<-ts(log(data_precio_amzn$price.open), frequency = 12,start=c(2002,09))\n#diferencias logaritmicas(cambio porcential)\ndlprice_amazn_ts<-ts(log(data_precio_amzn$price.open)-lag(log(data_precio_amzn$price.open),1), frequency = 12, start=c(2002,10))\ndlprice_amazn_ts<-na.omit(dlprice_amazn_ts)\nauto.arima(price_amazn_ts)\n#> Series: price_amazn_ts \n#> ARIMA(0,2,1) \n#> \n#> Coefficients:\n#>           ma1\n#>       -0.9716\n#> s.e.   0.0140\n#> \n#> sigma^2 = 17.95:  log likelihood = -650.79\n#> AIC=1305.58   AICc=1305.63   BIC=1312.43\narima021 <- arima(price_amazn_ts,\n                  order=c(0,2,1),\n                  method = \"ML\")\nplot(arima021)\nresarima021 <- resid(arima021)^2\nplot(resarima021)\nauto.arima(resarima021, trace = T)\n#> \n#>  Fitting models using approximations to speed things up...\n#> \n#>  ARIMA(2,1,2)(1,0,1)[12] with drift         : Inf\n#>  ARIMA(0,1,0)            with drift         : 2580.559\n#>  ARIMA(1,1,0)(1,0,0)[12] with drift         : 2483.806\n#>  ARIMA(0,1,1)(0,0,1)[12] with drift         : 2459.32\n#>  ARIMA(0,1,0)                               : 2578.524\n#>  ARIMA(0,1,1)            with drift         : 2457.799\n#>  ARIMA(0,1,1)(1,0,0)[12] with drift         : 2471.589\n#>  ARIMA(0,1,1)(1,0,1)[12] with drift         : Inf\n#>  ARIMA(1,1,1)            with drift         : 2462.02\n#>  ARIMA(0,1,2)            with drift         : 2468.224\n#>  ARIMA(1,1,0)            with drift         : 2469.957\n#>  ARIMA(1,1,2)            with drift         : 2463.999\n#>  ARIMA(0,1,1)                               : 2457.284\n#>  ARIMA(0,1,1)(1,0,0)[12]                    : 2471.293\n#>  ARIMA(0,1,1)(0,0,1)[12]                    : 2458.924\n#>  ARIMA(0,1,1)(1,0,1)[12]                    : 2472.638\n#>  ARIMA(1,1,1)                               : 2459.962\n#>  ARIMA(0,1,2)                               : 2466.155\n#>  ARIMA(1,1,0)                               : 2467.907\n#>  ARIMA(1,1,2)                               : 2461.911\n#> \n#>  Now re-fitting the best model(s) without approximations...\n#> \n#>  ARIMA(0,1,1)                               : 2467.364\n#> \n#>  Best model: ARIMA(0,1,1)\n#> Series: resarima021 \n#> ARIMA(0,1,1) \n#> \n#> Coefficients:\n#>           ma1\n#>       -0.8877\n#> s.e.   0.0418\n#> \n#> sigma^2 = 2875:  log likelihood = -1231.66\n#> AIC=2467.31   AICc=2467.36   BIC=2474.17\ngarch.amzn <- ugarchspec(mean.model = list(armaOrder=c(0,2,1)),\n                         variance.model = list(garchOrder=c(0,1,1)))\ngarch.amzn011 <- ugarchfit(spec = garch.amzn, \n                           data = price_amazn_ts)\ngarch.amzn011\n#> \n#> *---------------------------------*\n#> *          GARCH Model Fit        *\n#> *---------------------------------*\n#> \n#> Conditional Variance Dynamics    \n#> -----------------------------------\n#> GARCH Model  : sGARCH(0,1)\n#> Mean Model   : ARFIMA(0,0,2)\n#> Distribution : norm \n#> \n#> Optimal Parameters\n#> ------------------------------------\n#>        Estimate  Std. Error   t value Pr(>|t|)\n#> mu     24.95198    3.157546    7.9023    0e+00\n#> ma1     1.30976    0.030288   43.2438    0e+00\n#> ma2     0.92045    0.027681   33.2521    0e+00\n#> omega   0.85535    0.187792    4.5547    5e-06\n#> beta1   0.99900    0.000742 1345.5614    0e+00\n#> \n#> Robust Standard Errors:\n#>        Estimate  Std. Error   t value Pr(>|t|)\n#> mu     24.95198    4.118254    6.0589 0.000000\n#> ma1     1.30976    0.025642   51.0793 0.000000\n#> ma2     0.92045    0.026220   35.1049 0.000000\n#> omega   0.85535    0.331671    2.5789 0.009912\n#> beta1   0.99900    0.000605 1651.1260 0.000000\n#> \n#> LogLikelihood : -944.8252 \n#> \n#> Information Criteria\n#> ------------------------------------\n#>                    \n#> Akaike       8.2954\n#> Bayes        8.3704\n#> Shibata      8.2945\n#> Hannan-Quinn 8.3257\n#> \n#> Weighted Ljung-Box Test on Standardized Residuals\n#> ------------------------------------\n#>                         statistic p-value\n#> Lag[1]                      121.1       0\n#> Lag[2*(p+q)+(p+q)-1][5]     419.8       0\n#> Lag[4*(p+q)+(p+q)-1][9]     673.2       0\n#> d.o.f=2\n#> H0 : No serial correlation\n#> \n#> Weighted Ljung-Box Test on Standardized Squared Residuals\n#> ------------------------------------\n#>                         statistic p-value\n#> Lag[1]                      75.93       0\n#> Lag[2*(p+q)+(p+q)-1][2]    140.42       0\n#> Lag[4*(p+q)+(p+q)-1][5]    289.77       0\n#> d.o.f=1\n#> \n#> Weighted ARCH LM Tests\n#> ------------------------------------\n#>             Statistic Shape Scale P-Value\n#> ARCH Lag[2]     126.7 0.500 2.000       0\n#> ARCH Lag[4]     239.1 1.397 1.611       0\n#> ARCH Lag[6]     322.8 2.222 1.500       0\n#> \n#> Nyblom stability test\n#> ------------------------------------\n#> Joint Statistic:  75.7755\n#> Individual Statistics:              \n#> mu    12.39823\n#> ma1    0.01388\n#> ma2    0.61855\n#> omega  4.26665\n#> beta1  3.42184\n#> \n#> Asymptotic Critical Values (10% 5% 1%)\n#> Joint Statistic:          1.28 1.47 1.88\n#> Individual Statistic:     0.35 0.47 0.75\n#> \n#> Sign Bias Test\n#> ------------------------------------\n#>                      t-value      prob sig\n#> Sign Bias            1.17559 2.410e-01    \n#> Negative Sign Bias   0.08769 9.302e-01    \n#> Positive Sign Bias   8.59226 1.470e-15 ***\n#> Joint Effect       175.55530 8.040e-38 ***\n#> \n#> \n#> Adjusted Pearson Goodness-of-Fit Test:\n#> ------------------------------------\n#>   group statistic p-value(g-1)\n#> 1    20     305.9    1.214e-53\n#> 2    30     315.1    8.413e-50\n#> 3    40     331.2    5.579e-48\n#> 4    50     337.6    9.995e-45\n#> \n#> \n#> Elapsed time : 0.1092081\nsummary(garch.amzn011)\n#>    Length     Class      Mode \n#>         1 uGARCHfit        S4\nf.garch.amzn011 <- ugarchforecast(garch.amzn011, n.ahead=12)\nf.garch.amzn011\n#> \n#> *------------------------------------*\n#> *       GARCH Model Forecast         *\n#> *------------------------------------*\n#> Model: sGARCH\n#> Horizon: 12\n#> Roll Steps: 0\n#> Out of Sample: 0\n#> \n#> 0-roll forecast [T0=Sep 2021]:\n#>      Series Sigma\n#> T+1  113.91 19.11\n#> T+2   60.07 19.12\n#> T+3   24.95 19.13\n#> T+4   24.95 19.14\n#> T+5   24.95 19.16\n#> T+6   24.95 19.17\n#> T+7   24.95 19.18\n#> T+8   24.95 19.20\n#> T+9   24.95 19.21\n#> T+10  24.95 19.22\n#> T+11  24.95 19.23\n#> T+12  24.95 19.25"},{"path":"heterocedasticidad-condicionada.html","id":"ejemplo-3","chapter":"10 Heterocedasticidad condicionada","heading":"10.3 Ejemplo","text":"Realice el mismo procedimiento pero con la serie de tiempo de diferencias logaritmicas.","code":""},{"path":"markov-switching-regime-model.html","id":"markov-switching-regime-model","chapter":"11 Markov Switching Regime Model","heading":"11 Markov Switching Regime Model","text":"","code":""},{"path":"markov-switching-regime-model.html","id":"motivación","chapter":"11 Markov Switching Regime Model","heading":"11.1 Motivación","text":"Conforme vamos avanzando en las lecciones hemos construido modelos que consideran –concretamente– más y más particularidades de las series de tiempo. Realmente, lo más complicado de las series de tiempo es la presencia de la volatilidad. Cuando vimos \\(GARCH\\) pudimos ver una manera de modelarla; sin embargo, en esta lección consideraremos distintas dinámicas de volatilidad y como – puede haber comportamientos distintos en momentos de expansión y contracción económica. Veremos en concreto qué tan posible es ver estos cambios de una dinámica otra –cambios de régimen– y adaptarnos estos cambios para hacer mejores ajustes y predicciones.Supongamos que tenemos una serie de rendimientos \\(r_t\\) que tiene dos estados de Markov con diferentes primas de riesgo y diferentes dinámicas GARCH:\n\\[\\begin{equation}\n    \\tag{11.1}\nr_t = \\left\\lbrace\n\\begin{array}{ll}\n\\beta_1\\sqrt{h_t}+\\sqrt{h_t}\\epsilon_t, \\quad h_t=\\alpha_{10}+\\alpha_{11}h_{t-1}+\\alpha_{12}^2_{t-1}\\quad\\mathrm{ si } \\quad s_t=1\\\\\n\\beta_2\\sqrt{h_t}+\\sqrt{h_t}\\epsilon_t,\\quad h_t=\\alpha_{20}+\\alpha_{21}h_{t-1}+\\alpha_{22}^2_{t-1}\\quad\\mathrm{ si } \\quad s_t=2\n\\end{array}\n\\right.\n\\end{equation}\\]Donde \\(a_t=\\sqrt{h_t}\\epsilon_t\\) son los residuos y \\({\\epsilon_t}\\) es una secuencia Gausiana de ruido blanco con media \\(0\\) y varianza \\(1\\). La probabilidad de transicionar de un régimen otro esta descrita por:\n\\[\\begin{equation}\n    P(s_t=2|s_{t-1}=1)=e_1,\\quad P(s_t=1|s_{t-1}=2)=e_2\n\\end{equation}\\]Donde \\(e_i\\) va de 0 1 y un valor muy pequeño de \\(e_i\\) significa que la serie de tiempo tiene una tendencia muy alta de quedarse en el régimen con una duración de \\(\\frac{1}{e_i}\\). Asumiendo que \\(\\beta_1<\\beta_2\\) por lo que el régimen 2 tiene una prima por riesgo mayor. Un caso particular e interesante es si \\(\\alpha_{1j}=\\alpha_{2j}\\) para todas las jotas entonces tenemos un simple GARCH. Por lo que si \\(\\beta_i\\sqrt{h_t}\\) es remplazado por \\(\\beta_i\\), entonces el modelo (11.1) es un Modelo de cambio de régimen de Markov: \\(GARCH-M\\).Por simplicidad, asumiremos que la volatilidad \\(h_1\\) tiene el mismo valor que la varianza muestral de \\(r_t\\). El modelo tradicional la volatilidad debe ser calculada como un parámetro cualquiera. Sin embargo, al fijar \\(h_1\\) simplificamos las cosas y con una muestra grande debería importar.Así pues, los modelos de \\(GARCH-M\\) son \\(\\beta=(\\beta_1,\\beta_2)^{'}\\), \\(\\alpha=(\\alpha_{i0},alpha_{i1},alpha_{i2})^{'}\\), vector de transición \\(e=(e_1,e_2)^{'}\\), el vector de estado \\(S=(s_1,s_2,\\dots,s_n)^{'}\\) y el vector de volatilidad es \\(H=(h_2,\\dots,h_n)^{'}\\). Dado que hay dependencia del retorno en la volatilidad, entonces también sabemos que esta serialmente correlacionado, por lo que se puede predecir el retorno. Sin embargo, la predicción es complicada debido que hay muchas combinaciones entre todos las configuraciones. Por ello, usaremos el método de Gibbs solo las siguientes:\n\\[f(\\beta|R,S,H,\\alpha_1,\\alpha_2), \\quad f(\\alpha_1|R,S,H,\\alpha_{j\\neq }),\\]\n\\[P(S|R,h_1,\\alpha_1,\\alpha_2),\\quad f(e_i|S), \\quad\\mathrm{ para } =1,2\\]\nDonde R es la colección de todos los retornos observados y las distribuciones son:\n\\[\\beta_i\\sim N(\\beta_{io},\\sigma^2_{io}), \\quad e_i\\sim Beta(\\gamma{i1},\\gamma{i2})\\] Cabe mencionar que la distribucion anterior de \\(\\alpha_{ij}\\) es uniforme sobre un intervalo bien especificadoLa distribución posterior de \\(\\beta_i\\) depende solamente del régimen en el que está.\n\\[\\begin{equation}\nr_t = \\left\\lbrace\n\\begin{array}{ll}\n\\frac{r_t}{\\sqrt{h_t}} \\quad\\mathrm{ si } s_t=\\\\\n0\\quad\\mathrm{ c.o.c }\n\\end{array}\n\\right.\n\\end{equation}\\]\nPor lo que tenemos:\n\\[\\begin{equation}\n    r_{}=\\beta_i+\\epsilon_t,\\quad \\mathrm{si }s_t=\n\\end{equation}\\]En consecuencia, la informacion en \\(\\beta_i\\) está contenida en la media muestral de \\(r_{}\\). \\(\\bar{r_i}=(\\sum_{s_t=}r_{}/n_i\\) donde la suma son los puntos en el régimen y \\(n_i\\) es la cantidad de numero es ese punto. Por lo mimso, la distribucion posterior condicional de \\(\\beta_i\\) es normal con media en \\(\\beta_i^*\\) y varianza \\(\\sigma^{2*}_i\\), donde,\\[\\begin{equation}\n    \\frac{1}{\\sigma^{2*}_i}=n_i+\\frac{1}{\\sigma^{2*}_{io}}, \\quad \\beta_i^*=\\sigma^{2*}_{}(n_i\\bar{r_i}+\\beta{io}/\\sigma^{2}_{io})\n\\end{equation}\\]Los parametros \\(\\alpha_ij\\) pueden ser definidos usando el metodo Griddy Gibbs. Dado \\(h_i,S,\\alpha_{v\\neq }\\)y \\(\\alpha_{iv}\\) tenemos la distribucion condicional posterior se ve:\\[ f(\\alpha_{ij}|.) \\alpha - \\frac{1}{2} \\left[ ln(h_t) + \\frac{(r_t-\\beta_i \\sqrt{ht})^{2}}{h_t}\\right],  \\quad \\mathrm{ para } \\quad s_t=\\]Y se evaluan estos puntos en un intervalo bien definido porque \\(0\\geq\\alpha_{11}<\\alpha_{12}\\)\\La distribucion condicional posterior de \\(e_i\\) solo toma en cuenta \\(S\\). Donde \\(l_t\\) es el numero de cambios del regimen 1 2 y \\(l_2\\) al revés, tenemos que \\(n_i\\) es el numero de puntos en el estado .Su distribución es \\(Beta(\\gamma_{i1}+l_i,\\gamma_{i2}+n_i-l_i)\\).Finalmente, los elementos de S de pueden definir uno por uno. Donde \\(S_{-j}\\) es el vector al quitar \\(s_j\\) de S, \\(s_j\\) tiene dos posibilidades (\\(s_j=1\\) o \\(s_j=2\\)) y si distribución condicional posterior es:\n\\[\\begin{equation}\n        P(s_j|.)\\alpha\\prod_{t=j}^nf(a_t|H)P(s_j|S_{-j})    \n        \\end{equation}\\]La probabilidad esta dada por:\n\\[P(s:j=|s_{-k}=P(s_{j=}|s_{j-1},s_{j+1}),\\quad =1,2\\]\nAdicionalmente, asumiento que \\(s_j=\\), uno puede computar h_t para \\(t\\geq j\\). La funcion de verosimilitud, denodtada por \\(L(S_j)\\) está dada por:\\[\\begin{equation}\n    L(s_j=)\\equiv\\prod_{t=j}^nf(a_t|H)\\alpha exp(f_{ji}), \\quad f_{ji}=\\sum_{t=j}^n\n-\\frac{1}{2}\\left[ln(h_t)+\\frac{a_t^2}{h_t}\\right]\n\\end{equation}\\]Donde \\(a_t\\) puede ser \\(a_t=r_t-\\beta_1\\sqrt{h_t}\\) o \\(a_t=r_t-\\beta_2\\sqrt{h_t}\\), por lo que la probabilidad de estar en el régimen \\(s_j=1\\) es:\\[\\begin{equation}\n    P(s_j=1|.)=\\frac{P(s_j=1|s_{j-1},s_{j+1})L(s_j=1)}{P(s_j=1|s_{j-1},s_{j+1})L(s_j=1)+P(s_j=2|s_{j-1},s_{j+1})L(s_j=2)}\n\\end{equation}\\]Por lo que el régimen \\(s_j\\) puede ser dibujado fácilmente como una distribución uniforme con intervalo \\([0,1]\\).","code":""},{"path":"markov-switching-regime-model.html","id":"ejemplo-4","chapter":"11 Markov Switching Regime Model","heading":"11.2 Ejemplo","text":"","code":"\n#install.packages(\"pacman\")\n#pacman nos permite cargar varias librerias en una sola línea\nlibrary(pacman)\npacman::p_load(tidyverse,BatchGetSymbols,ggplot2,lubridate,readxl,forecast,stats,stargazer,knitr,tseries,aTSA, TSA, rugarch, MSwM, MSGARCH, fGarch, ggpubr, knitr, MSGARCH,paletteer,MetBrewer)\nlibrary(MSGARCH)\n#Primero determinamos el lapso de tiempo\npd<-as.Date(\"2002/9/30\") #primer fecha\npd\n#> [1] \"2002-09-30\"\n#> [1] \"2021-09-18\"\nld<- as.Date(\"2021/09/30\")#última fecha\nld\n#> [1] \"2021-09-30\"\n#Intervalos de tiempo\nint<-\"monthly\"\n\n#Datos a elegir\ndt<-c(\"AMZN\")\n\n#Descargando los valores\ndata1<- BatchGetSymbols(tickers = dt,\n                       first.date = pd,\n                       last.date = ld,\n                       freq.data = int,\n                       do.cache = FALSE,\n                       thresh.bad.data = 0)\n\n#Generando data frame con los valores\ndata_precio_amzn<-data1$df.tickers\ncolnames(data_precio_amzn)\n#>  [1] \"ticker\"              \"ref.date\"           \n#>  [3] \"volume\"              \"price.open\"         \n#>  [5] \"price.high\"          \"price.low\"          \n#>  [7] \"price.close\"         \"price.adjusted\"     \n#>  [9] \"ret.adjusted.prices\" \"ret.closing.prices\"\n#original\nprice_amazn_ts<-ts(data_precio_amzn$price.open, frequency = 12, start=c(2002,09))\n#logartimo\nlprice_amazn_ts<-ts(log(data_precio_amzn$price.open), frequency = 12,start=c(2002,09))\n#diferencias logaritmicas(cambio porcential)\ndlprice_amazn_ts<-ts(log(data_precio_amzn$price.open)-lag(log(data_precio_amzn$price.open),1), frequency = 12, start=c(2002,10))\ndlprice_amazn_ts<-na.omit(dlprice_amazn_ts)\n# GARCH creado el capitulo pasado\ngarch.amzn <- ugarchspec(mean.model = list(armaOrder=c(0,2,1)),\n                         variance.model = list(garchOrder=c(0,1,1)))\ngarch.amzn011 <- ugarchfit(spec = garch.amzn, \n                           data = price_amazn_ts)\ngarch.amzn011\n#> \n#> *---------------------------------*\n#> *          GARCH Model Fit        *\n#> *---------------------------------*\n#> \n#> Conditional Variance Dynamics    \n#> -----------------------------------\n#> GARCH Model  : sGARCH(0,1)\n#> Mean Model   : ARFIMA(0,0,2)\n#> Distribution : norm \n#> \n#> Optimal Parameters\n#> ------------------------------------\n#>        Estimate  Std. Error   t value Pr(>|t|)\n#> mu     24.95198    3.157546    7.9023    0e+00\n#> ma1     1.30976    0.030288   43.2438    0e+00\n#> ma2     0.92045    0.027681   33.2521    0e+00\n#> omega   0.85535    0.187792    4.5547    5e-06\n#> beta1   0.99900    0.000742 1345.5614    0e+00\n#> \n#> Robust Standard Errors:\n#>        Estimate  Std. Error   t value Pr(>|t|)\n#> mu     24.95198    4.118254    6.0589 0.000000\n#> ma1     1.30976    0.025642   51.0793 0.000000\n#> ma2     0.92045    0.026220   35.1049 0.000000\n#> omega   0.85535    0.331671    2.5789 0.009912\n#> beta1   0.99900    0.000605 1651.1260 0.000000\n#> \n#> LogLikelihood : -944.8252 \n#> \n#> Information Criteria\n#> ------------------------------------\n#>                    \n#> Akaike       8.2954\n#> Bayes        8.3704\n#> Shibata      8.2945\n#> Hannan-Quinn 8.3257\n#> \n#> Weighted Ljung-Box Test on Standardized Residuals\n#> ------------------------------------\n#>                         statistic p-value\n#> Lag[1]                      121.1       0\n#> Lag[2*(p+q)+(p+q)-1][5]     419.8       0\n#> Lag[4*(p+q)+(p+q)-1][9]     673.2       0\n#> d.o.f=2\n#> H0 : No serial correlation\n#> \n#> Weighted Ljung-Box Test on Standardized Squared Residuals\n#> ------------------------------------\n#>                         statistic p-value\n#> Lag[1]                      75.93       0\n#> Lag[2*(p+q)+(p+q)-1][2]    140.42       0\n#> Lag[4*(p+q)+(p+q)-1][5]    289.77       0\n#> d.o.f=1\n#> \n#> Weighted ARCH LM Tests\n#> ------------------------------------\n#>             Statistic Shape Scale P-Value\n#> ARCH Lag[2]     126.7 0.500 2.000       0\n#> ARCH Lag[4]     239.1 1.397 1.611       0\n#> ARCH Lag[6]     322.8 2.222 1.500       0\n#> \n#> Nyblom stability test\n#> ------------------------------------\n#> Joint Statistic:  75.7755\n#> Individual Statistics:              \n#> mu    12.39823\n#> ma1    0.01388\n#> ma2    0.61855\n#> omega  4.26665\n#> beta1  3.42184\n#> \n#> Asymptotic Critical Values (10% 5% 1%)\n#> Joint Statistic:          1.28 1.47 1.88\n#> Individual Statistic:     0.35 0.47 0.75\n#> \n#> Sign Bias Test\n#> ------------------------------------\n#>                      t-value      prob sig\n#> Sign Bias            1.17559 2.410e-01    \n#> Negative Sign Bias   0.08769 9.302e-01    \n#> Positive Sign Bias   8.59226 1.470e-15 ***\n#> Joint Effect       175.55530 8.040e-38 ***\n#> \n#> \n#> Adjusted Pearson Goodness-of-Fit Test:\n#> ------------------------------------\n#>   group statistic p-value(g-1)\n#> 1    20     305.9    1.214e-53\n#> 2    30     315.1    8.413e-50\n#> 3    40     331.2    5.579e-48\n#> 4    50     337.6    9.995e-45\n#> \n#> \n#> Elapsed time : 0.1398311"},{"path":"markov-switching-regime-model.html","id":"cambio-de-régimen","chapter":"11 Markov Switching Regime Model","heading":"11.2.1 Cambio de Régimen","text":"Asumiendo que es de GARCH(1,1) y que ha ARIMA, podemos calcular la cantidad de regímenes con el paquete MSGARCH.\nFigure 11.1: Simulacion de las trancisiones del primer y segundo régimen\n","code":"\n#Definimos las caracteristicas\nspec <- CreateSpec(\n      variance.spec = list(model = c(\"sGARCH\")),\n      distribution.spec = list(distribution = c(\"norm\")),\n      switch.spec = list(do.mix = FALSE, K = 2),\n      constraint.spec = list(fixed = list(), regime.const = NULL),\n      prior = list(mean = list(), sd = list())\n)\nsummary(spec)\n#> Specification type: Markov-switching\n#> Specification name: sGARCH_norm sGARCH_norm\n#> Number of parameters in each variance model: 3 3\n#> Number of parameters in each distribution: 0 0\n#> ------------------------------------------\n#> Fixed parameters:\n#> None\n#> ------------------------------------------\n#> Across regime constrained parameters:\n#> None\n#> ------------------------------------------\n#Corremos el calculo de los regímenes\n#maximum likelihood\nfit.ml <- FitML(spec, dlprice_amazn_ts, ctr = list())\nfit.ml\n#> Specification type: Markov-switching\n#> Specification name: sGARCH_norm sGARCH_norm\n#> Number of parameters in each variance model: 3 3\n#> Number of parameters in each distribution: 0 0\n#> ------------------------------------------\n#> Fixed parameters:\n#> None\n#> ------------------------------------------\n#> Across regime constrained parameters:\n#> None\n#> ------------------------------------------\n#> Fitted parameters:\n#>          Estimate Std. Error  t value  Pr(>|t|)\n#> alpha0_1   0.0018     0.0018   0.9788 1.638e-01\n#> alpha1_1   0.0457     0.0935   0.4885 3.126e-01\n#> beta_1     0.6847     0.2768   2.4734 6.693e-03\n#> alpha0_2   0.0000     0.0020   0.0053 4.979e-01\n#> alpha1_2   0.0000     0.0043   0.0043 4.983e-01\n#> beta_2     0.9994     0.1040   9.6103    <1e-16\n#> P_1_1      0.9954     0.0069 144.2269    <1e-16\n#> P_2_1      0.0056     0.0060   0.9408 1.734e-01\n#> ------------------------------------------\n#> Transition matrix:\n#>       t+1|k=1 t+1|k=2\n#> t|k=1  0.9954  0.0046\n#> t|k=2  0.0056  0.9944\n#> ------------------------------------------\n#> Stable probabilities:\n#> State 1 State 2 \n#>  0.5521  0.4479 \n#> ------------------------------------------\n#> LL: 194.849\n#> AIC: -373.6981\n#> BIC: -346.2633\n#> ------------------------------------------\n\n#simulación bayesiana de monte carlo\nset.seed(1234)\nfit.mcmc <- FitMCMC(spec, dlprice_amazn_ts, ctr = list())\nfit.mcmc\n#> Specification type: Markov-switching\n#> Specification name: sGARCH_norm sGARCH_norm\n#> Number of parameters in each variance model: 3 3\n#> Number of parameters in each distribution: 0 0\n#> ------------------------------------------\n#> Fixed parameters:\n#> None\n#> ------------------------------------------\n#> Across regime constrained parameters:\n#> None\n#> ------------------------------------------\n#> Posterior sample (size: 1000)\n#>            Mean     SD     SE   TSSE    RNE\n#> alpha0_1 0.0041 0.0022 0.0001 0.0002 0.0875\n#> alpha1_1 0.1429 0.0785 0.0025 0.0039 0.4022\n#> beta_1   0.4299 0.2350 0.0074 0.0277 0.0719\n#> alpha0_2 0.1164 0.2122 0.0067 0.0178 0.1426\n#> alpha1_2 0.2857 0.2169 0.0069 0.0122 0.3181\n#> beta_2   0.4039 0.1613 0.0051 0.0095 0.2856\n#> P_1_1    0.9577 0.0315 0.0010 0.0017 0.3345\n#> P_2_1    0.7307 0.2221 0.0070 0.0124 0.3196\n#> ------------------------------------------\n#> Posterior mean transition matrix:\n#>       t+1|k=1 t+1|k=2\n#> t|k=1  0.9577  0.0423\n#> t|k=2  0.7307  0.2693\n#> ------------------------------------------\n#> Posterior mean stable probabilities:\n#> State 1 State 2 \n#>  0.9453  0.0547 \n#> ------------------------------------------\n#> Acceptance rate MCMC sampler: 28.2%\n#> nmcmc: 10000\n#> nburn: 5000\n#> nthin: 10\n#> ------------------------------------------\n#> DIC: -366.1017\n#> ------------------------------------------\n\nP.matrix <- matrix(c(0.9577,0.7307,0.0423,0.2693),2,2)\nt.end <- 100\n\ns0 <- 1 \nst <- function(i) sample(1:2,1,prob = P.matrix[i,])\n\ns <- st(s0)\nfor(t in 2:t.end) {\n  s <- c(s,st(s[t-1]))\n}\nplot(s, pch = 20,cex = 0.5)"},{"path":"markov-switching-regime-model.html","id":"simulación-de-monte-carlo","chapter":"11 Markov Switching Regime Model","heading":"11.2.2 Simulación de Monte Carlo","text":"Utilizando el modelo GARCH utilizado en el capítulo pasado podemos haccer una simulacion de los coeficientes. Esto para ver si, dado una simulacion Monte Carlo, son consistentes lo que calculamos.\nTable 11.1: Medias estándar de los coeficientes\n\nTable 11.2: Desviaciones estándar de los coeficientes\nVeamos los histogramas de los coeficientes\nFigure 11.2: Histograma de \\(\\mu\\)\n\nFigure 11.3: Histograma de \\(MA_1\\)\n\nFigure 11.4: Histograma de \\(MA_2\\)\n\nFigure 11.5: Histograma de \\(\\Omega\\)\n\nFigure 11.6: Histograma de \\(eta_1\\)\nEvidentemente esto demuestra que si convergen los valores estimados.","code":"\n#-------------------------\n# SIMULACIÓN\n#-------------------------\ndist <- ugarchdistribution(garch.amzn011, n.sim = 100, m.sim = 5000, n.start = 10, rseed = 1234)\ndist.garch.amzn011 <- as.data.frame(dist)\n\n#-------------------------\n# Coeficientes iterados\n#-------------------------\nhead(dist.garch.amzn011)\n#>          mu      ma1       ma2        omega     beta1\n#> 1 -3.100545 1.291266 0.9193533 1.141042e-04 0.9989541\n#> 2 40.541769 1.273353 0.8643736 4.861132e+00 0.9952600\n#> 3 11.815010 1.300380 1.0229470 1.883227e-05 0.9987412\n#> 4  4.943115 1.354630 1.0343739 6.946167e+00 0.9912582\n#> 5 41.911480 1.267513 0.8490696 4.929588e-05 0.9964037\n#> 6 24.796880 1.224390 0.7188740 1.122114e-05 0.9957013\nkable(as.data.frame(cbind(mu=mean(dist.garch.amzn011$mu,na.rm=TRUE),ma1=mean(dist.garch.amzn011$ma1,na.rm=TRUE),ma2=mean(dist.garch.amzn011$ma2,na.rm=TRUE),omega=mean(dist.garch.amzn011$omega,na.rm=TRUE),beat1=mean(dist.garch.amzn011$beta1,na.rm=TRUE))), \"html\", caption = \"Medias estándar de los coeficientes\")\nkable(as.data.frame(cbind(mu=sd(dist.garch.amzn011$mu,na.rm=TRUE),ma1=sd(dist.garch.amzn011$ma1,na.rm=TRUE),ma2=sd(dist.garch.amzn011$ma2,na.rm=TRUE),omega=sd(dist.garch.amzn011$omega,na.rm=TRUE),beat1=sd(dist.garch.amzn011$beta1,na.rm=TRUE))),\"html\", caption = \"Desviaciones estándar de los coeficientes\")\n#-------------\n# PALETTE\n#-------------\npalette <- met.brewer(name=\"Monet\", n=3)#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2\n#> 3.4.0.\n#> ℹ Please use `linewidth` instead.\n#> Warning: The dot-dot notation (`..count..`) was deprecated in\n#> ggplot2 3.4.0.\n#> ℹ Please use `after_stat(count)` instead.\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> Warning: Removed 2 rows containing non-finite values\n#> (`stat_bin()`).#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> Warning: Removed 2 rows containing non-finite values\n#> (`stat_bin()`).#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> Warning: Removed 2 rows containing non-finite values\n#> (`stat_bin()`).#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> Warning: Removed 2 rows containing non-finite values\n#> (`stat_bin()`).#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> Warning: Removed 2 rows containing non-finite values\n#> (`stat_bin()`)."},{"path":"setar-star-linear-test.html","id":"setar-star-linear-test","chapter":"12 SETAR-STAR-LINEAR-TEST","heading":"12 SETAR-STAR-LINEAR-TEST","text":"","code":""},{"path":"setar-star-linear-test.html","id":"setar-y-star","chapter":"12 SETAR-STAR-LINEAR-TEST","heading":"12.1 SETAR Y STAR","text":"Como lo estudiamos en el capitulo anterior, existen muchos modelos de cambio de régimen.\nOtro de los modelos más utilizados es el Self-Exciting Threshold Autorregressive model o (SETAR) por sus siglas. Este modelo se denota de la siguiente manera, veamos AR(1):\\[\\begin{equation}\ny_t = \\left\\lbrace\n\\begin{array}{ll}\n\\phi_{0,1}+\\phi_{1,1}y_{t-1}+\\epsilon_t\\mathrm{ si }  y_{t-1}\\leq c\\\\\n\\phi_{0,2}+\\phi_{1,2}y_{t-1}+\\epsilon_t\\mathrm{ si }  y_{t-1}>c\n\\end{array}\n\\right.\n\\end{equation}\\]Donde c representa un limite el cual determina el régimen en el que nos encontramos y \\(\\epsilon\\) es una serie de ruido blanco condicional la historia que esta denotada por \\(\\Omega_{t-1}=(y_{t-1},y_{t-2}, \\dots, y_{1-(p-1)}, y_{t-p})\\) y su valor esperado \\(E[\\epsilon_t|\\Omega_{t-1}]=0\\) y \\(E[\\epsilon_t^2|\\Omega_{t-1}]=\\sigma^2\\).Queda claro que el cambio se hace de manera discreta; sin embargo, se puede construir un modelo continuo donde se construye un indicador contiunuo \\(G(y_t-1;\\gamma,c)\\) que cambia suavizadamente de 0 1 cuando \\(y_{t-1}\\) aumenta. Se ve así y se le conoce como Smooth Transition Auto-Regressive Model (STAR):\\[\\begin{equation}\n    y_t=(\\phi_{0,1}+\\phi_{1,1}y_{t-1})(1-G(y_t-1;\\gamma,c)+ y_t=(\\phi_{0,2}+\\phi_{1,2}y_{t-1})(G(y_t-1;\\gamma,c))\n\\end{equation}\\]En práctica realmente siempre se utilizan modelos de AR(1), por lo mismo de órdenes más altos vemos:\n\\[\\begin{equation}\ny_t = \\left\\lbrace\n\\begin{array}{ll}\n\\phi_{0,1}+\\phi_{1,1}y_{t-1}+\\dots+\\phi_{p_{1},1}y_{t-p_1}+\\epsilon_t\\mathrm{ si }  y_{t-1}\\leq c\\\\\n\\phi_{0,2}+\\phi_{1,2}y_{t-1}+\\dots+\\phi_{p_{2},2}y_{t-p_2}+\\epsilon_t\\mathrm{ si }  y_{t-1}>c\n\\end{array}\n\\right.\n\\end{equation}\\]SETAR(p) se ve así:\\[\\begin{equation}\n   y_t=(\\phi_{0,1}+\\phi_{1,1}y_{t-1}+\\dots+\\phi_{p_{1},1}y_{t-p_1})(1-G(y_t-1;\\gamma,c)+ y_t=(\\phi_{0,2}+\\phi_{1,2}y_{t-1}+\\dots+\\phi_{p_{2},2}y_{t-p_2})(G(y_t-1;\\gamma,c))\n\\end{equation}\\]","code":""},{"path":"setar-star-linear-test.html","id":"setar-ejemplo","chapter":"12 SETAR-STAR-LINEAR-TEST","heading":"12.1.1 SETAR EJEMPLO","text":"","code":"\n#---------------------\n# Dependencias\n#---------------------\n\n#install.packages(\"pacman\")\n#pacman nos permite cargar varias librerias en una sola línea\nlibrary(pacman)\npacman::p_load(tidyverse,BatchGetSymbols,ggplot2,lubridate,readxl,forecast,stats,stargazer,knitr,tseries,aTSA, TSA, rugarch, MSwM, MSGARCH, fGarch, ggpubr, knitr, MSGARCH, paletteer, MetBrewer, tsDyn, knitr)\n#Primero determinamos el lapso de tiempo\npd<-as.Date(\"2002/9/30\") #primer fecha\npd\n#> [1] \"2002-09-30\"\n#> [1] \"2021-09-18\"\nld<- as.Date(\"2021/09/30\")#última fecha\nld\n#> [1] \"2021-09-30\"\n#Intervalos de tiempo\nint<-\"monthly\"\n\n#Datos a elegir\ndt<-c(\"AMZN\")\n\n#Descargando los valores\ndata1<- BatchGetSymbols(tickers = dt,\n                       first.date = pd,\n                       last.date = ld,\n                       freq.data = int,\n                       do.cache = FALSE,\n                       thresh.bad.data = 0)\n\n#Generando data frame con los valores\ndata_precio_amzn<-data1$df.tickers\ncolnames(data_precio_amzn)\n#>  [1] \"ticker\"              \"ref.date\"           \n#>  [3] \"volume\"              \"price.open\"         \n#>  [5] \"price.high\"          \"price.low\"          \n#>  [7] \"price.close\"         \"price.adjusted\"     \n#>  [9] \"ret.adjusted.prices\" \"ret.closing.prices\"\n#original\nprice_amazn_ts<-ts(data_precio_amzn$price.open, frequency = 12, start=c(2002,09))\n#logartimo\nlprice_amazn_ts<-ts(log(data_precio_amzn$price.open), frequency = 12,start=c(2002,09))\n#diferencias logaritmicas(cambio porcential)\ndlprice_amazn_ts<-ts(log(data_precio_amzn$price.open)-lag(log(data_precio_amzn$price.open),1), frequency = 12, start=c(2002,10))\ndlprice_amazn_ts<-na.omit(dlprice_amazn_ts)\nselectSETAR(price_amazn_ts, m=1)\n#> Using maximum autoregressive order for low regime: mL = 1 \n#> Using maximum autoregressive order for high regime: mH = 1 \n#> Searching on 158 possible threshold values within regimes with sufficient ( 15% ) number of observations\n#> Searching on  158  combinations of thresholds ( 158 ), thDelay ( 1 ), mL ( 1 ) and MM ( 1 )#> Results of the grid search for 1 threshold\n#>    thDelay mL mH      th pooled-AIC\n#> 1        0  1  1 15.6845   826.1945\n#> 2        0  1  1 15.6295   828.5923\n#> 3        0  1  1 15.2065   829.0579\n#> 4        0  1  1 15.3120   830.3599\n#> 5        0  1  1 13.9500   832.8452\n#> 6        0  1  1 15.6290   833.4052\n#> 7        0  1  1 14.2365   833.6059\n#> 8        0  1  1 13.4480   833.8387\n#> 9        0  1  1 15.1540   834.0295\n#> 10       0  1  1 21.5200   834.2624\nsetar <- setar(price_amazn_ts, m=2, mL=1, MH=1, thDelay=0)\n#> Warning: Possible unit root in the low regime. Roots are:\n#> 0.9745\nsummary(setar)\n#> \n#> Non linear autoregressive model\n#> \n#> SETAR model ( 2 regimes)\n#> Coefficients:\n#> Low regime:\n#>      const.L       phiL.1 \n#> -0.002008592  1.026173699 \n#> \n#> High regime:\n#>   const.H    phiH.1 \n#> 4.9224719 0.9787845 \n#> \n#> Threshold:\n#> -Variable: Z(t) = + (1) X(t)+ (0)X(t-1)\n#> -Value: 49.93\n#> Proportion of points in low regime: 79.74%    High regime: 20.26% \n#> \n#> Residuals:\n#>         Min          1Q      Median          3Q         Max \n#> -22.7005989  -0.7170736  -0.0014706   0.4991115  19.1291281 \n#> \n#> Fit:\n#> residuals variance = 16.9,  AIC = 657, MAPE = 8.001%\n#> \n#> Coefficient(s):\n#> \n#>           Estimate  Std. Error  t value Pr(>|t|)    \n#> const.L -0.0020086   0.4190008  -0.0048  0.99618    \n#> phiL.1   1.0261737   0.0245249  41.8421  < 2e-16 ***\n#> const.H  4.9224719   1.9252131   2.5568  0.01122 *  \n#> phiH.1   0.9787845   0.0166682  58.7215  < 2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Threshold\n#> Variable: Z(t) = + (1) X(t) + (0) X(t-1)\n#> \n#> Value: 49.93"},{"path":"setar-star-linear-test.html","id":"star-ejemplo","chapter":"12 SETAR-STAR-LINEAR-TEST","heading":"12.1.2 STAR Ejemplo","text":"","code":"\nstar <- star(price_amazn_ts, m=2, noRegimes=2, d = 1, trace=TRUE)\n#> Using default threshold variable: thDelay=0\n#> Testing linearity...   p-Value =  3.966884e-10 \n#> The series is nonlinear. Incremental building procedure:\n#> Building a 2 regime STAR.\n#> Using default threshold variable: thDelay=0\n#> Performing grid search for starting values...\n#> Starting values fixed: gamma =  100 , th =  88.33759 ; SSE =  3795.842 \n#> Grid search selected lower/upper bound gamma (was:  1 100 ]). \n#>                    Might try to widen bound with arg: 'starting.control=list(gammaInt=c(1,200))'\n#> Optimization algorithm converged\n#> Optimized values fixed for regime 2  : gamma =  100 , th =  88.33716 ; SSE =  3795.842 \n#> Finished building a MRSTAR with 2 regimes\nsummary(star)\n#> \n#> Non linear autoregressive model\n#> \n#> Multiple regime STAR model\n#> \n#> Regime  1 :\n#>     Linear parameters: -0.179361, 0.8515913, 0.2003287 \n#> \n#> Regime  2 :\n#>     Linear parameters: 0.9058675, 0.1825174, -0.2279061 \n#>     Non-linear parameters:\n#> 100.0000006, 88.3371573\n#> \n#> Residuals:\n#>          Min           1Q       Median           3Q \n#> -21.30360652  -0.79297830   0.08150576   0.50008703 \n#>          Max \n#>  19.07138218 \n#> \n#> Fit:\n#> residuals variance = 16.58,  AIC = 659, MAPE = 8.584%"},{"path":"setar-star-linear-test.html","id":"pruebas-de-detección-lineal.","chapter":"12 SETAR-STAR-LINEAR-TEST","heading":"12.2 Pruebas de detección lineal.","text":"","code":""},{"path":"setar-star-linear-test.html","id":"test-setar","chapter":"12 SETAR-STAR-LINEAR-TEST","heading":"12.2.1 Test SETAR","text":"Concretamente, utilizamos los estimados del modelo SETAR para definir F que comprueba las restricciones de la hipótesis nula.\n\\[\\begin{equation}\n        F(\\hat{c})=n\\left(\\frac{\\tilde{\\sigma^2}-\\hat{\\sigma^2}}{\\hat{\\sigma^2}} \\right)\n    \\end{equation}\\]\nDonde \\(\\tilde{\\sigma^2}\\) es un estimado de la varianza residual. \\(\\tilde{\\sigma^2}=\\sum a_{t=1}^n\\tilde{\\varepsilon^2_t}\\) donde \\(\\tilde{\\varepsilon^2_t}=y_t-\\hat{\\phi^{'}}x_t\\) y \\(\\hat{\\sigma^2}=\\sum a_{t=1}^n\\hat{\\varepsilon^2_t}(c)\\).","code":""},{"path":"setar-star-linear-test.html","id":"ejemplo-5","chapter":"12 SETAR-STAR-LINEAR-TEST","heading":"12.2.1.1 Ejemplo","text":"Por tanto sabemos que es lineal.","code":"\n#1vs2 para indicar ARvsSETAR\nsetarTest(\n      price_amazn_ts,\n      m=1,\n      thDelay = 0,\n      trim = 0.1,\n      nboot = 100,\n      seed = 1234\n)\n#> Test of linearity against setar(2) and setar(3)\n#> \n#>          Test Pval\n#> 1vs2 7.364584 0.57\n#> 1vs3 8.599106 0.97"},{"path":"setar-star-linear-test.html","id":"test-star","chapter":"12 SETAR-STAR-LINEAR-TEST","heading":"12.2.2 Test STAR","text":"Aprovechando este test, resulta relevante tomar en cuenta variables problematicas que pueden colapsar un STAR un simple AR. L hipótesis nula se puede expresar \\(H_0:\\gamma=0\\). Tambien si \\(\\gamma=0\\) entonces:\n\\[\\begin{equation}\n        G(y_{t-1};\\gamma,c)=\\frac{1}{1+exp(-\\gamma[t_{t-1}-c])}\n    \\end{equation}\\]\n\\(G(y_{t-1};\\gamma,c)=0.5\\) para todo \\(x_t\\) y los parámetros son iguales \\((\\phi_1+\\phi_2)/2\\). Los parámetros \\(\\gamma\\) y \\(c\\) pueden ser problemáticos y difíciles de identificar. Para resolver este problema se utiliza el Multiplicado de Lagrange (LM) que tiene una distribuscion \\(\\chi^2\\).\nEl modelo STAR se puede reescribir:\n\\[\\begin{equation}\n            \\tag{12.1}\n    y_t=\\frac{1}{2}(\\phi_1+\\phi_2)^{'}x_t+(\\phi_2-\\phi_1)^{'}x_tG^{*}(y_{t-1};\\gamma,c)+\\epsilon_t\n\\end{equation}\\]\nDonde \\(G^{*}(y_{t-1};\\gamma,c)=G(y_{t-1};\\gamma,c)-\\frac{1}{2}\\). La hipotesis nula hace que \\(G^{*}(y_{t-1};\\gamma,c)=0\\). Si hacemos una aproximación de Taylor de \\(G^{*}(y_{t-1};\\gamma,c)\\):\n\\[\\begin{eqnarray}\n    T_1(t_{t-1};c)&\\approx& G^{*}(y_{t-1};0,c)+\\gamma\\frac{\\partial G^{*}(y_{t-1};\\gamma,c)}{\\partial\\gamma}\\bigg|_{\\gamma=0}\\\\\n    &=&\\frac{1}{4}\\gamma(y_{t-1}-c)\n\\end{eqnarray}\\]Metiendo \\(T_1\\) en \\(G_1^*\\) obtenemos una regresión auxiliar.\n\\[\\begin{equation}\n    \\tag{12.2}\n    y_t=\\beta_{0,0}+\\beta_0^{'}\\tilde{x_t}+\\beta_1^{'}\\tilde{x_t}y_{t-1}+\\eta\n    \\end{equation}\\]\nDonde \\(tilde{x_t}=(y_{t-1},\\dots,y_{t-p})^{'}\\) y \\(\\beta_j=(\\beta_{1,j},\\dots,\\beta_{p,j})^{'}\\). La relación entre (12.1) y (12.2) se ve:\\[\\begin{eqnarray}\n    \\beta_{0,0}&=&(\\phi_{0,1}+\\phi_{0,2})/2-\\frac{1}{4}\\gamma c\\gamma c(\\phi_{0,2}-\\phi_{0,1})\\\\\n    \\beta_{1,0}&=&(\\phi_{1,1}+\\phi_{1,2})/2-\\frac{1}{4}\\gamma (c(\\phi_{1,2}-\\phi_{1,1})-(\\phi_{0,2}-\\phi_{0,1}))\\\\\n    \\beta_{,0}&=&(\\phi_{,1}+\\phi_{,2})/2-\\frac{1}{4}\\gamma c\\gamma c(\\phi_{,2}-\\phi_{,1}),\\quad =2,\\dots,p\\\\\n    \\beta_{,1}&=&\\gamma c\\frac{1}{4}\\gamma c(\\phi_{,2}-\\phi_{,1}),\\quad =1,\\dots,p\n\\end{eqnarray}\\]\nEn las ecuaciones de arriba podemos ver que si \\(\\gamma=0\\) en (12.1), entonces \\(\\beta_{,1}=0\\) en (12.2). Lo cual se refiere nuestra variable de prueba en la .\\(H_0^{'}:\\gamma=0\\) y \\(H_0^{''}:\\beta_1=0\\) Esto se distribuye como \\(\\chi^2\\) con \\(p\\) grados de libertad.Finalmente hay que hacer una especificación en la que se permite que el intercepto sea diferente, pero los coeficientes . Es decir, \\(\\phi_{0,1}\\neq\\phi_{0,2}\\), pero \\(\\phi_{,1}=\\phi_{,2}\\) donde \\(=1,\\dots,p\\). Por su parte \\(G(y_{t-1};\\gamma,c)\\) es:\n\\[\\begin{eqnarray}\n        T_3(y_{t-1};\\gamma,c)&\\approx&\\gamma\\frac{\\partial G^{*}(y_{t-1};\\gamma,c))}{\\partial\\gamma}\\bigg|_{\\gamma=0}+\\frac{1}{6}\\gamma^3\\frac{\\partial^3 G^{*}(y_{t-1};\\gamma,c))}{\\partial\\gamma^3}\\bigg|_{\\gamma=0}\\\\\n        &=&\\frac{1}{4}\\gamma(y_{t-1}-c)+\\frac{1}{48}\\gamma^3(y_{t-1}-c)^3\n    \\end{eqnarray}\\]\nEl modelo auxiliar se ve de la siguiente manera cuando evaluamos en 0.\n\\[\\begin{equation}\n        \\tag{12.2}\n        y_t=\\beta_{0,0}+\\beta_0^{'}\\tilde{x_t}+\\beta_1^{'}\\tilde{x_t}y_{t-1}+\\beta_2^{'}\\tilde{x_t}y_{t-2}+\\beta_3^{'}\\tilde{x_t}y_{t-3}+\\eta_t.\n    \\end{equation}\\]\nDe esta manera las pruebas son \\(H_0^{'}:\\gamma=0\\) y \\(H_0^{''}:\\beta_1=\\beta_2=\\beta_3=0\\) con distribución \\(\\chi^2\\) y \\(3p\\) grados de libertad.Finalmente hay que hacer una especificación en la que se permite que el intercepto sea diferente, pero los coeficientes . Es decir, \\(\\phi_{0,1}\\neq\\phi_{0,2}\\), pero \\(\\phi_{,1}=\\phi_{,2}\\) donde \\(=1,\\dots,p\\). Por su parte \\(G(y_{t-1};\\gamma,c)\\) es:\n\\[\\begin{eqnarray}\n        T_3(y_{t-1};\\gamma,c)&\\approx&\\gamma\\frac{\\partial G^{*}(y_{t-1};\\gamma,c))}{\\partial\\gamma}\\bigg|_{\\gamma=0}+\\frac{1}{6}\\gamma^3\\frac{\\partial^3 G^{*}(y_{t-1};\\gamma,c))}{\\partial\\gamma^3}\\bigg|_{\\gamma=0}\\\\\n        &=&\\frac{1}{4}\\gamma(y_{t-1}-c)+\\frac{1}{48}\\gamma^3(y_{t-1}-c)^3\n    \\end{eqnarray}\\]\nEl modelo auxiliar se ve de la siguiente manera cuando evaluamos en 0.\n\\[\\begin{equation}\n    \\tag{12.2}\n    y_t=\\beta_{0,0}+\\beta_0^{'}\\tilde{x_t}+\\beta_1^{'}\\tilde{x_t}y_{t-1}+\\beta_2^{'}\\tilde{x_t}y_{t-2}+\\beta_3^{'}\\tilde{x_t}y_{t-3}+\\eta_t.\n    \\end{equation}\\]\nDe esta manera las pruebas son \\(H_0^{'}:\\gamma=0\\) y \\(H_0^{''}:\\beta_1=\\beta_2=\\beta_3=0\\) con distribución \\(\\chi^2\\) y \\(3p\\) grados de libertad.El F test se puede computar\n- Estimar el modelo de la hipótesis nula regresando \\(y_t\\) en \\(x_t\\) y computar los residuales \\(\\tilde{\\epsilon}\\) y el SSR (suma de residos al cuadrado: \\(SSR_0=\\sum_{t=1}^n\\tilde{\\epsilon}^2\\)\n- Estimamos la regression auxiliar de \\(\\tilde{\\epsilon}\\) en \\(x_t\\) y \\(\\tilde{x_t}y_{t-1}^j\\) donde \\(j=1,2,3\\) y obtener la suma de los errores al cuadrado \\(SSR_1\\).\n- Finalmente:\n\\[\\begin{equation}\n        LM=\\frac{(SSR_0-SSR_1)/3p}{SSR/(n-4p-1}\n    \\end{equation}\\]","code":""},{"path":"setar-star-linear-test.html","id":"test-markov","chapter":"12 SETAR-STAR-LINEAR-TEST","heading":"12.2.3 Test MARKOV","text":"Ahora veamos el modelo de cambio de régimen de Markov. El test necesita de \\(h_t(\\theta)\\) que esta definido por la derivada del logaritmo de la densidad condicional definida como \\(f(y_t|\\Omega_{t-1};\\theta)\\).\\[\\begin{eqnarray}\n    f(y_t|\\Omega_{t-1};\\theta)&=&f(y_t,st=1|\\Omega_{t-1};\\theta)+f(y_t,st=2|\\Omega_{t-1};\\theta)\\\\\n    \\tag{12.3}\n    &=& \\sum_{j=1}^2f(y_t,st=j|\\Omega_{t-1};\\theta)\\cdot P(y_t|\\Omega_{t-1};\\theta)\n\\end{eqnarray}\\]\n\\[\\begin{equation}\n    h_t(\\theta)\\equiv \\frac{\\partial ln[f(y_t|\\Omega_{t-1};\\theta)]}{\\partial\\theta}\n\\end{equation}\\]\nPara dos regímenes:\n{\n\\[\\begin{eqnarray}\n    \\frac{\\partial ln[f(y_t|\\Omega_{t-1};\\theta)]}{\\partial\\theta}=\\frac{1}{\\sigma^2}(y_t-\\phi^{'}_jx_t)x_t\\cdot P(st=j|\\Omega)+\\frac{1}{\\sigma^2}(y_{\\tau}-\\phi^{'}_jx_{\\tau})x_{\\tau}\\cdot (P(st=j|\\Omega_t;\\theta)-P(st=j|\\Omega_{t-1};\\theta)\n\\end{eqnarray}\\]Por construcción, el resultado evaluado por ML estima \\(\\hat{\\theta}\\) tiene media cero. \\(\\sum_{t-1}^nh_t(\\hat{\\theta})\\).Por lo mismo, se construye las estadísticas LM. Para dos regímenes agregamos variables \\(z_t\\) que se han omitido y queremos probar contra la alternativa:\n\\[\\begin{equation}\n        y_t=\\phi_{0,s_t}+\\phi_{1,s_t}y_{t-1}+\\dots+\\phi_{p,s_t}y_{t-p}+\\delta^{'}z_t+\\epsilon_t.\n    \\end{equation}\\]\nLa ecuación evaluada respecto \\(\\delta\\) –que es un vector de variable omitidas– construye la hipótesis \\(H_0:\\delta=0\\) es igual :\\[\\begin{equation}\n    \\frac{\\partial ln[f(y_t|\\Omega_{t-1};\\theta)]}{\\partial\\theta}\\bigg|_{\\delta=0}=\\sum_{j=1}^2(y_t-\\phi_j^{'}x_t)z_t\\cdot P(s_t=j|\\Omega_n;\\hat{\\theta})\n\\end{equation}\\]\nDonde estima el vector \\(\\theta^{'}=(\\phi_1{'},\\phi_2,p_{11},p_{22},\\delta)\\) debajo la hipótesis nula. Por lo mismo LM se obtiene con:\n\\[\\begin{equation}\n    n\\left(\\frac{1}{n}\\sum_{t=1}^nh_t(\\hat{\\theta)}\\right)^{'}\\left(\\frac{1}{n}\\sum_{t=1}^nh_t(\\hat{\\theta})h_t(\\hat{\\theta}^{'})\\right)\\left(\\frac{1}{n}\\sum_{t=1}^nh_t(\\hat{\\theta})\\right)\n\\end{equation}\\]\nse distribuye asintóticamente \\(\\chi^2\\) con el número de variables en \\(z_t\\).","code":""},{"path":"setar-star-linear-test.html","id":"test-garch","chapter":"12 SETAR-STAR-LINEAR-TEST","heading":"12.2.4 Test GARCH","text":"El test está basado en el modelo de ARCH(q). La varianza condicional es constante si los parámetros de los residuos al cuadrado (\\(\\epsilon_{t-1}^2,=1,\\dots,q\\)) son iguales cero. Por tanto, la hipótesis nula de la heterocedasticidad condicional es \\(H_0:\\alpha_1=\\dots=\\alpha_q\\). El LM correspondiente se computa como \\(nR^2\\) donde n es la cantidad de datos y \\(R^2\\) se obtiene de de la regresión de los residuos al cuadrado respecto los \\(q\\) lags constantes:\\[\\begin{equation}\n\\hat{\\epsilon_t^2}=\\omega+\\alpha_1\\hat{\\epsilon_{t-1}^2}+\\dots+\\alpha_p\\hat{\\epsilon_{t-p}^2}+u_t\n\\end{equation}\\]Por lo que donde los residuales \\(\\hat{\\epsilon_t^2}\\) se obtienen del del modelo de media condicional de la serie de tiempo observada. Se distribuye \\(\\chi^2(q)\\). Este test es equivalente hacer uno para el GARCH(p,q)En el capítulo anterior al hacer las simulaciones de Monte Carlo demostramos como nuestros estimadores son consistentes y, por consiguiente, se hace cierto test demostrando que hay necesidad de cambiar de tener más de un régimen. Esto se puede ver en la Figura 12.1.Con la matriz:\nTable 11.1: Matriz de transicion GARCH\n\nFigure 12.1: Cambios de régimen en el modelo GARCH\n","code":"\nkable(P.matrix <- matrix(c(0.9577,0.7307,0.0423,0.2693),2,2), caption = \"Matriz de transicion GARCH\", \"html\")"}]
